{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBpu4udRMj1t"
      },
      "source": [
        "# BTP - 1\n",
        "\n",
        "DAM and RTM Forecasting using LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-05-07T03:27:14.624732Z",
          "iopub.status.busy": "2025-05-07T03:27:14.624306Z",
          "iopub.status.idle": "2025-05-07T03:27:14.630353Z",
          "shell.execute_reply": "2025-05-07T03:27:14.628857Z",
          "shell.execute_reply.started": "2025-05-07T03:27:14.624701Z"
        },
        "id": "et5B2--QRfbD",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGXMx1fgdrdz"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xFh4HkFkZqe"
      },
      "source": [
        "Below function is applied to a csv file having the following columns:\n",
        "1. Date\n",
        "2. Hour: 00:00, 00:15, 00:30, 00:45 are considered as 1\n",
        "3. MCP (Rs/MWh)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-05-07T03:27:29.282192Z",
          "iopub.status.busy": "2025-05-07T03:27:29.281781Z",
          "iopub.status.idle": "2025-05-07T03:27:29.289602Z",
          "shell.execute_reply": "2025-05-07T03:27:29.288123Z",
          "shell.execute_reply.started": "2025-05-07T03:27:29.282162Z"
        },
        "id": "ci2Bea5ORfbG",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def convert_to_datetime(df):\n",
        "    ndf = df.copy(deep = True)\n",
        "    ndf['Date'] = pd.to_datetime(df['Date'], format = '%d-%m-%Y')\n",
        "    ndf['Hour'] = df['Hour'].apply(lambda x: x-1)\n",
        "\n",
        "    # Combine into datetime\n",
        "    ndf['datetime'] = ndf['Date'] + pd.to_timedelta(ndf['Hour'], unit='h')\n",
        "\n",
        "    # Add 15-minute increments (4 rows per hour â†’ 00:00, 00:15, 00:30, 00:45)\n",
        "    ndf['datetime'] = ndf.groupby(['Date', 'Hour'])['datetime'].transform(\n",
        "    lambda x: x + pd.to_timedelta((x.groupby(x).cumcount()) * 15, unit='m'))\n",
        "\n",
        "    ndf.drop(columns= ['Hour', 'Date'])\n",
        "    ndf = ndf[['datetime', 'MCP (Rs/MWh)']]\n",
        "    return ndf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-05-07T03:27:33.702606Z",
          "iopub.status.busy": "2025-05-07T03:27:33.702204Z",
          "iopub.status.idle": "2025-05-07T03:27:33.744123Z",
          "shell.execute_reply": "2025-05-07T03:27:33.742828Z",
          "shell.execute_reply.started": "2025-05-07T03:27:33.702573Z"
        },
        "id": "ZIHt5FnFRfbH",
        "outputId": "8e4bb8f2-6ac3-4c90-f7dd-6113d8d950e6",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 50496 entries, 0 to 50495\n",
            "Data columns (total 3 columns):\n",
            " #   Column        Non-Null Count  Dtype  \n",
            "---  ------        --------------  -----  \n",
            " 0   Date          50496 non-null  object \n",
            " 1   Hour          50496 non-null  int64  \n",
            " 2   MCP (Rs/MWh)  50496 non-null  float64\n",
            "dtypes: float64(1), int64(1), object(1)\n",
            "memory usage: 1.2+ MB\n"
          ]
        }
      ],
      "source": [
        "DAM = pd.read_csv('/content/DAM_Price.csv')\n",
        "DAM.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-05-07T03:27:38.672424Z",
          "iopub.status.busy": "2025-05-07T03:27:38.671964Z",
          "iopub.status.idle": "2025-05-07T03:27:38.708050Z",
          "shell.execute_reply": "2025-05-07T03:27:38.706892Z",
          "shell.execute_reply.started": "2025-05-07T03:27:38.672387Z"
        },
        "id": "T0sk-wuBRfbI",
        "outputId": "6e54a31c-c85b-4e7e-c247-101c6950e8a6",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 50400 entries, 0 to 50399\n",
            "Data columns (total 3 columns):\n",
            " #   Column        Non-Null Count  Dtype  \n",
            "---  ------        --------------  -----  \n",
            " 0   Date          50400 non-null  object \n",
            " 1   Hour          50400 non-null  int64  \n",
            " 2   MCP (Rs/MWh)  50400 non-null  float64\n",
            "dtypes: float64(1), int64(1), object(1)\n",
            "memory usage: 1.2+ MB\n"
          ]
        }
      ],
      "source": [
        "RTM = pd.read_csv('/content/RTM_Price.csv')\n",
        "RTM.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYVUB8WdklRR"
      },
      "source": [
        "No null values exist. Atleast 525 days data is present for each. **DAM** has extra one days prediction as it sold day before the given day. So extra 96 entries."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kv1sf86ElFWq"
      },
      "source": [
        "No IQR or Z-score is used to remove outliers as its removal we lead to us losing the continuity of time series data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "execution": {
          "iopub.execute_input": "2025-05-07T03:27:42.551619Z",
          "iopub.status.busy": "2025-05-07T03:27:42.551202Z",
          "iopub.status.idle": "2025-05-07T03:27:55.181162Z",
          "shell.execute_reply": "2025-05-07T03:27:55.180042Z",
          "shell.execute_reply.started": "2025-05-07T03:27:42.551588Z"
        },
        "id": "9VDAZTzGRfbJ",
        "outputId": "40cf9ca9-f90f-4583-8f12-e7f4d8fbccd7",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"dam1\",\n  \"rows\": 50496,\n  \"fields\": [\n    {\n      \"column\": \"datetime\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"2023-09-01 00:00:00\",\n        \"max\": \"2025-02-07 23:45:00\",\n        \"num_unique_values\": 50496,\n        \"samples\": [\n          \"2024-10-03 14:30:00\",\n          \"2023-11-16 15:00:00\",\n          \"2024-10-06 13:30:00\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"MCP (Rs/MWh)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2677.0725408056073,\n        \"min\": 299.76,\n        \"max\": 10000.0,\n        \"num_unique_values\": 31517,\n        \"samples\": [\n          3281.43,\n          2171.66,\n          4168.09\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "dam1"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-a8254296-fe72-41c5-83c5-2ff7a5d6b31e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>datetime</th>\n",
              "      <th>MCP (Rs/MWh)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2023-09-01 00:00:00</td>\n",
              "      <td>10000.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2023-09-01 00:15:00</td>\n",
              "      <td>10000.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2023-09-01 00:30:00</td>\n",
              "      <td>10000.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2023-09-01 00:45:00</td>\n",
              "      <td>10000.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2023-09-01 01:00:00</td>\n",
              "      <td>10000.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a8254296-fe72-41c5-83c5-2ff7a5d6b31e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a8254296-fe72-41c5-83c5-2ff7a5d6b31e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a8254296-fe72-41c5-83c5-2ff7a5d6b31e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-f6a8e211-3485-4a4a-a2fc-7ff86f465330\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f6a8e211-3485-4a4a-a2fc-7ff86f465330')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-f6a8e211-3485-4a4a-a2fc-7ff86f465330 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "             datetime  MCP (Rs/MWh)\n",
              "0 2023-09-01 00:00:00       10000.0\n",
              "1 2023-09-01 00:15:00       10000.0\n",
              "2 2023-09-01 00:30:00       10000.0\n",
              "3 2023-09-01 00:45:00       10000.0\n",
              "4 2023-09-01 01:00:00       10000.0"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dam1 = convert_to_datetime(DAM)\n",
        "dam1.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "execution": {
          "iopub.execute_input": "2025-05-07T03:28:02.475750Z",
          "iopub.status.busy": "2025-05-07T03:28:02.475272Z",
          "iopub.status.idle": "2025-05-07T03:28:15.082479Z",
          "shell.execute_reply": "2025-05-07T03:28:15.081588Z",
          "shell.execute_reply.started": "2025-05-07T03:28:02.475716Z"
        },
        "id": "ZE7jU6TfRfbJ",
        "outputId": "3954f131-982b-49b4-9a2a-64fec2b112a0",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"rtm1\",\n  \"rows\": 50400,\n  \"fields\": [\n    {\n      \"column\": \"datetime\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"2023-09-01 00:00:00\",\n        \"max\": \"2025-02-06 23:45:00\",\n        \"num_unique_values\": 50400,\n        \"samples\": [\n          \"2024-12-13 13:30:00\",\n          \"2024-09-16 03:30:00\",\n          \"2023-12-08 05:30:00\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"MCP (Rs/MWh)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2549.6666883401826,\n        \"min\": 0.0,\n        \"max\": 10000.0,\n        \"num_unique_values\": 32431,\n        \"samples\": [\n          4065.58,\n          2850.68,\n          2830.87\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "rtm1"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-4d8515d1-409d-4ac2-9500-d4edc57fc6ad\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>datetime</th>\n",
              "      <th>MCP (Rs/MWh)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2023-09-01 00:00:00</td>\n",
              "      <td>10000.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2023-09-01 00:15:00</td>\n",
              "      <td>10000.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2023-09-01 00:30:00</td>\n",
              "      <td>10000.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2023-09-01 00:45:00</td>\n",
              "      <td>10000.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2023-09-01 01:00:00</td>\n",
              "      <td>10000.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4d8515d1-409d-4ac2-9500-d4edc57fc6ad')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-4d8515d1-409d-4ac2-9500-d4edc57fc6ad button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-4d8515d1-409d-4ac2-9500-d4edc57fc6ad');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-c548f76e-4adc-469a-ab6b-9d170c0c1f44\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c548f76e-4adc-469a-ab6b-9d170c0c1f44')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-c548f76e-4adc-469a-ab6b-9d170c0c1f44 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "             datetime  MCP (Rs/MWh)\n",
              "0 2023-09-01 00:00:00       10000.0\n",
              "1 2023-09-01 00:15:00       10000.0\n",
              "2 2023-09-01 00:30:00       10000.0\n",
              "3 2023-09-01 00:45:00       10000.0\n",
              "4 2023-09-01 01:00:00       10000.0"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rtm1 = convert_to_datetime(RTM)\n",
        "rtm1.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-05-07T03:28:22.347133Z",
          "iopub.status.busy": "2025-05-07T03:28:22.346731Z",
          "iopub.status.idle": "2025-05-07T03:28:22.352396Z",
          "shell.execute_reply": "2025-05-07T03:28:22.350958Z",
          "shell.execute_reply.started": "2025-05-07T03:28:22.347103Z"
        },
        "id": "dxta7oYuRfbK",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# from pandas.plotting import autocorrelation_plot\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# autocorrelation_plot(dam_check)\n",
        "# plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDdZcNJRlVHa"
      },
      "source": [
        "# Spliiting Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-05-07T03:28:25.413106Z",
          "iopub.status.busy": "2025-05-07T03:28:25.412769Z",
          "iopub.status.idle": "2025-05-07T03:28:25.417862Z",
          "shell.execute_reply": "2025-05-07T03:28:25.416633Z",
          "shell.execute_reply.started": "2025-05-07T03:28:25.413081Z"
        },
        "id": "a6ybLTGiRfbL",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "total_len = len(dam1)\n",
        "train_end = int(0.7 * total_len)\n",
        "val_end = int(0.85 * total_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-05-07T03:28:27.921113Z",
          "iopub.status.busy": "2025-05-07T03:28:27.920651Z",
          "iopub.status.idle": "2025-05-07T03:28:27.927951Z",
          "shell.execute_reply": "2025-05-07T03:28:27.926587Z",
          "shell.execute_reply.started": "2025-05-07T03:28:27.921073Z"
        },
        "id": "PyxJwd49RfbM",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "dam1_train = dam1.iloc[:train_end]\n",
        "dam1_valid = dam1.iloc[train_end:val_end]\n",
        "dam1_test = dam1.iloc[val_end:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-05-07T03:28:31.198390Z",
          "iopub.status.busy": "2025-05-07T03:28:31.198031Z",
          "iopub.status.idle": "2025-05-07T03:28:31.203909Z",
          "shell.execute_reply": "2025-05-07T03:28:31.202397Z",
          "shell.execute_reply.started": "2025-05-07T03:28:31.198362Z"
        },
        "id": "n_UnKaqARfbN",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "rtm1_train = rtm1.iloc[:train_end]\n",
        "rtm1_valid = rtm1.iloc[train_end:val_end]\n",
        "rtm1_test = rtm1.iloc[val_end:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-05-07T03:28:34.561743Z",
          "iopub.status.busy": "2025-05-07T03:28:34.561350Z",
          "iopub.status.idle": "2025-05-07T03:28:34.566872Z",
          "shell.execute_reply": "2025-05-07T03:28:34.565739Z",
          "shell.execute_reply.started": "2025-05-07T03:28:34.561716Z"
        },
        "id": "jNS_A_fxRfbN",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "dam_t_mcp = dam1_train['MCP (Rs/MWh)']\n",
        "rtm_t_mcp = rtm1_train['MCP (Rs/MWh)']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsv7UQuKdbsX"
      },
      "source": [
        "# Spectral Density of Data\n",
        "Using Periodogram we get the spectral power density of the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-05-07T03:28:37.854568Z",
          "iopub.status.busy": "2025-05-07T03:28:37.854172Z",
          "iopub.status.idle": "2025-05-07T03:28:37.861580Z",
          "shell.execute_reply": "2025-05-07T03:28:37.859958Z",
          "shell.execute_reply.started": "2025-05-07T03:28:37.854540Z"
        },
        "id": "zN0ezNPfRfbO",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from scipy.signal import periodogram\n",
        "\n",
        "def SpectralPower(series, top_n = 10):\n",
        "    # Periodogram: Spectral power density\n",
        "    frequencies, power = periodogram(series.dropna(), fs=1.0)\n",
        "\n",
        "    # Sort frequencies by power in descending order\n",
        "    sorted_indices = np.argsort(power)[::-1]  # Get indices that would sort power in descending order\n",
        "    sorted_frequencies = frequencies[sorted_indices]\n",
        "    sorted_power = power[sorted_indices]\n",
        "\n",
        "    # Top N periods having maximum power\n",
        "    top_frequencies = sorted_frequencies[:top_n]\n",
        "    top_powers = sorted_power[:top_n]\n",
        "    top_periods = 1 / top_frequencies  # Convert frequencies to periods\n",
        "\n",
        "    for i, (freq, pow, period) in enumerate(zip(top_frequencies, top_powers, top_periods), 1):\n",
        "        print(f\"{i}. Frequency: {freq:.6f} (power: {pow:.3f}) â†’ Period: {period:.1f} timesteps\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-05-07T03:28:41.771676Z",
          "iopub.status.busy": "2025-05-07T03:28:41.771173Z",
          "iopub.status.idle": "2025-05-07T03:28:41.791506Z",
          "shell.execute_reply": "2025-05-07T03:28:41.790228Z",
          "shell.execute_reply.started": "2025-05-07T03:28:41.771635Z"
        },
        "id": "B8j0wL5aRfbO",
        "outputId": "cfac91c9-1c2a-47ed-f5f7-f1a8017ebaf0",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1. Frequency: 0.010411 (power: 40128173978.864) â†’ Period: 96.1 timesteps\n",
            "2. Frequency: 0.020822 (power: 29630946992.222) â†’ Period: 48.0 timesteps\n",
            "3. Frequency: 0.010439 (power: 19918299209.690) â†’ Period: 95.8 timesteps\n",
            "4. Frequency: 0.010383 (power: 19330737368.016) â†’ Period: 96.3 timesteps\n",
            "5. Frequency: 0.000085 (power: 13182876794.380) â†’ Period: 11782.3 timesteps\n",
            "6. Frequency: 0.010468 (power: 10377418071.950) â†’ Period: 95.5 timesteps\n",
            "7. Frequency: 0.000113 (power: 6189827571.453) â†’ Period: 8836.8 timesteps\n",
            "8. Frequency: 0.031261 (power: 5205747102.592) â†’ Period: 32.0 timesteps\n",
            "9. Frequency: 0.020879 (power: 4790573657.411) â†’ Period: 47.9 timesteps\n",
            "10. Frequency: 0.000538 (power: 4085173248.550) â†’ Period: 1860.4 timesteps\n"
          ]
        }
      ],
      "source": [
        "SpectralPower(dam_t_mcp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-05-07T03:28:45.335978Z",
          "iopub.status.busy": "2025-05-07T03:28:45.335640Z",
          "iopub.status.idle": "2025-05-07T03:28:45.352964Z",
          "shell.execute_reply": "2025-05-07T03:28:45.351843Z",
          "shell.execute_reply.started": "2025-05-07T03:28:45.335951Z"
        },
        "id": "eQ5sr27VRfbO",
        "outputId": "9cbd48d6-acb5-4156-fb99-a48a120eeb14",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1. Frequency: 0.010411 (power: 27133204066.506) â†’ Period: 96.1 timesteps\n",
            "2. Frequency: 0.020822 (power: 21494073145.662) â†’ Period: 48.0 timesteps\n",
            "3. Frequency: 0.010439 (power: 15665110391.039) â†’ Period: 95.8 timesteps\n",
            "4. Frequency: 0.010383 (power: 13430619429.507) â†’ Period: 96.3 timesteps\n",
            "5. Frequency: 0.000085 (power: 10317458504.231) â†’ Period: 11782.3 timesteps\n",
            "6. Frequency: 0.010468 (power: 7651354099.901) â†’ Period: 95.5 timesteps\n",
            "7. Frequency: 0.000113 (power: 7150281174.400) â†’ Period: 8836.8 timesteps\n",
            "8. Frequency: 0.001499 (power: 4769675738.714) â†’ Period: 666.9 timesteps\n",
            "9. Frequency: 0.031261 (power: 4439684091.911) â†’ Period: 32.0 timesteps\n",
            "10. Frequency: 0.000538 (power: 4233599237.475) â†’ Period: 1860.4 timesteps\n"
          ]
        }
      ],
      "source": [
        "SpectralPower(rtm_t_mcp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QbfUoylRfbP"
      },
      "source": [
        "48, 96 (2*48) timesteps or 12, 24 hours have the highest spectral density or power among all periods. The dominant period (in our case, 96 time steps preferrred as they will also take care of 48 steps part) is used as the window size in an LSTM (or any time-series model) because it directly encodes the strongest seasonal pattern in the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZbuwDseifRN"
      },
      "source": [
        "# LSTM model\n",
        "![A LSTM Cell](https://media.geeksforgeeks.org/wp-content/uploads/20250404172141987003/gate_of_lstm.webp)\n",
        "\n",
        "![LSTM cells in series](https://dezyre.gumlet.io/images/blog/lstm-model/LSTM_cells_are_chained_together,_with_the_input_sequence_and_output_sequence_shown.png?w=376&dpr=2.6)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-05-07T03:28:56.182585Z",
          "iopub.status.busy": "2025-05-07T03:28:56.182110Z",
          "iopub.status.idle": "2025-05-07T03:28:56.190296Z",
          "shell.execute_reply": "2025-05-07T03:28:56.188972Z",
          "shell.execute_reply.started": "2025-05-07T03:28:56.182550Z"
        },
        "id": "XPsuHQnGRfbP",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "# -------- LSTM with Residuals (something like ResNet) --------\n",
        "class LSTMResidualModel(nn.Module):\n",
        "    def __init__(self, input_size=1, hidden_size=64, num_layers=2, dropout=0.3, target_len = 96):\n",
        "        super(LSTMResidualModel, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers,\n",
        "                            dropout=dropout, batch_first=True)\n",
        "        self.linear = nn.Linear(hidden_size, target_len)\n",
        "        self.residual = nn.Linear(input_size, target_len)  # 1: MCP input\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.lstm(x)       # lstm outputs: output y, (hidden state, cell state)\n",
        "        last_hidden = out[:, -1, :] # to get the last hidden state\n",
        "        prediction = self.linear(last_hidden)\n",
        "\n",
        "        # Residual connection from last input time step\n",
        "        residual = self.residual(x[:, -1, :])\n",
        "        return prediction + residual"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IiMhgdFDKOJZ"
      },
      "source": [
        "## DAM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-05-07T05:08:03.918966Z",
          "iopub.status.busy": "2025-05-07T05:08:03.918587Z",
          "iopub.status.idle": "2025-05-07T05:08:03.935029Z",
          "shell.execute_reply": "2025-05-07T05:08:03.932805Z",
          "shell.execute_reply.started": "2025-05-07T05:08:03.918937Z"
        },
        "id": "hn-JIk3SRfbP",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "class PriceDataset(Dataset):\n",
        "    def __init__(self, df, seq_len=96, fit_scaler=False, scaler=None, target_len = 96):\n",
        "        \"\"\"\n",
        "            df: DataFrame containing at least a 'MCP (Rs/MWh)' column.\n",
        "            seq_len: number of past time steps used for input.\n",
        "            fit_scaler: if True, fit the MinMaxScaler to the current df.\n",
        "            scaler: optional externally fitted scaler to apply. They are applied to reduce overfitting\n",
        "        \"\"\"\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.seq_len = seq_len\n",
        "        self.scaler = scaler or MinMaxScaler()\n",
        "        self.target_len = target_len\n",
        "\n",
        "        if fit_scaler:     # if True then MinMaxScaler() will be used\n",
        "            self.scaled_mcp = self.scaler.fit_transform(self.df[['MCP (Rs/MWh)']])\n",
        "        else:\n",
        "            if self.scaler is None:\n",
        "                raise ValueError(\"Must provide a fitted scaler if fit_scaler is False.\")\n",
        "            self.scaled_mcp = self.scaler.transform(self.df[['MCP (Rs/MWh)']]) # inputed scaler is used\n",
        "\n",
        "        self.scaled_mcp = self.scaled_mcp.astype(np.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.scaled_mcp) - 2*self.seq_len - self.target_len\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        # last 96 time steps\n",
        "        x = self.scaled_mcp[idx:idx + self.seq_len]\n",
        "\n",
        "        # next day's 96 steps\n",
        "        y = self.scaled_mcp[idx + 2*self.seq_len : idx + 2*self.seq_len + self.target_len]\n",
        "\n",
        "        return torch.tensor(x, dtype = torch.float32), torch.tensor(y, dtype=torch.float32)\n",
        "\n",
        "    def inverse_transform(self, scaled_array): # MinMaxScaled to normal values\n",
        "        if isinstance(scaled_array, torch.Tensor):\n",
        "            scaled_array = scaled_array.detach().cpu().numpy()\n",
        "        scaled_array = np.array(scaled_array).reshape(-1, 1)\n",
        "        return self.scaler.inverse_transform(scaled_array).flatten()\n",
        "\n",
        "    def get_scaler(self): # Same scaler has to be used while validating and testing etc.\n",
        "        return self.scaler\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-05-07T03:28:59.555866Z",
          "iopub.status.busy": "2025-05-07T03:28:59.555261Z",
          "iopub.status.idle": "2025-05-07T03:28:59.563209Z",
          "shell.execute_reply": "2025-05-07T03:28:59.561743Z",
          "shell.execute_reply.started": "2025-05-07T03:28:59.555801Z"
        },
        "id": "8A5aEGIGRfbQ",
        "outputId": "18595319-b8c7-4913-fe86-4fbb0d27367c",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-05-07T05:18:37.508575Z",
          "iopub.status.busy": "2025-05-07T05:18:37.508073Z",
          "iopub.status.idle": "2025-05-07T05:18:37.518872Z",
          "shell.execute_reply": "2025-05-07T05:18:37.517585Z",
          "shell.execute_reply.started": "2025-05-07T05:18:37.508540Z"
        },
        "id": "dcDTpCCVRfbQ",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_loader, val_loader, model_instance_name=\"model\",\n",
        "                num_epochs=100, lr=1e-3, device='cuda', target_len = 96):\n",
        "\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Adam optimiser with learning rate lr\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    # mean square error\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    best_val_loss = float('inf')      #initialisation of val_loss, inf is infinity\n",
        "\n",
        "    best_model_path = f\"best_lstm_{model_instance_name}.pt\"\n",
        "\n",
        "    best_model_state = None  # to store best weights\n",
        "\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "\n",
        "        # training mode\n",
        "        model.train()\n",
        "        train_losses = []\n",
        "\n",
        "        for x_batch, y_batch in train_loader:\n",
        "\n",
        "            # x_batch and y_batch is better of in GPU if present as batches can be computed parrllely\n",
        "            x_batch = x_batch.to(device)\n",
        "            y_batch = y_batch.to(device)\n",
        "\n",
        "            # to clear previous gradients as gradients are accumulative in PyTorch\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            #predictions\n",
        "            preds = model(x_batch).squeeze()\n",
        "\n",
        "            # loss calculation\n",
        "            loss = criterion(preds, y_batch.squeeze())\n",
        "\n",
        "            #backward propagation\n",
        "            loss.backward()\n",
        "\n",
        "            # updates the parameters\n",
        "            optimizer.step()\n",
        "\n",
        "            train_losses.append(loss.item())\n",
        "\n",
        "        # evaluation mode: no backpropagation,\n",
        "        model.eval()\n",
        "\n",
        "        val_losses = []\n",
        "\n",
        "        # no\n",
        "        with torch.no_grad():     # disables gradient calculation\n",
        "            for x_val, y_val in val_loader:\n",
        "                x_val = x_val.to(device)\n",
        "                y_val = y_val.to(device)\n",
        "                val_preds = model(x_val).squeeze()\n",
        "                val_loss = criterion(val_preds, y_val.squeeze())\n",
        "                val_losses.append(val_loss.item())\n",
        "\n",
        "        avg_train_loss = sum(train_losses) / (len(train_losses))\n",
        "        avg_val_loss = sum(val_losses) / (len(val_losses))\n",
        "\n",
        "        print(f\"Epoch {epoch}/{num_epochs} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            best_model_state = model.state_dict()            # save better weights\n",
        "            torch.save(best_model_state, best_model_path)    # old ones are replaced with better ones\n",
        "\n",
        "\n",
        "    # return best model weights\n",
        "    print(f\"Saved best model as {best_model_path}\")\n",
        "    return best_model_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-05-07T05:11:40.305580Z",
          "iopub.status.busy": "2025-05-07T05:11:40.305181Z",
          "iopub.status.idle": "2025-05-07T05:11:40.323370Z",
          "shell.execute_reply": "2025-05-07T05:11:40.321930Z",
          "shell.execute_reply.started": "2025-05-07T05:11:40.305541Z"
        },
        "id": "or4XDJDERfbR",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "# datasets\n",
        "dam_train_dataset = PriceDataset(dam1_train, fit_scaler=True)  # MinMaxScaler is fitted\n",
        "\n",
        "# gets the fitted MinMaxScaler model, uses the same scaler while validating and testing\n",
        "dam_scaler = dam_train_dataset.get_scaler()\n",
        "\n",
        "dam_val_dataset = PriceDataset(dam1_valid, scaler=dam_scaler)\n",
        "\n",
        "\"\"\"DataLoaders is used for efficient batching, shuffles and to avoid GPU idle time.\n",
        "It is provided by PyTorch. Classes of Dataset (from Pytorch) can only be used to load the data here\"\"\"\n",
        "dam_train_loader = DataLoader(dam_train_dataset, batch_size=64, shuffle=False)\n",
        "dam_val_loader = DataLoader(dam_val_dataset, batch_size=64)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrQdT_pcl1YY"
      },
      "source": [
        "Scaler is fitted and extracted only from the train dataset not the whole as it might lead to information extraction of validation and testing dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8srGy_fnZNvF",
        "outputId": "38e3b40d-0886-4dff-e309-d579e9bcce3d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100 | Train Loss: 0.1246 | Val Loss: 0.0603\n",
            "Epoch 2/100 | Train Loss: 0.0777 | Val Loss: 0.0507\n",
            "Epoch 3/100 | Train Loss: 0.0614 | Val Loss: 0.0445\n",
            "Epoch 4/100 | Train Loss: 0.0569 | Val Loss: 0.0402\n",
            "Epoch 5/100 | Train Loss: 0.0515 | Val Loss: 0.0352\n",
            "Epoch 6/100 | Train Loss: 0.0482 | Val Loss: 0.0339\n",
            "Epoch 7/100 | Train Loss: 0.0452 | Val Loss: 0.0321\n",
            "Epoch 8/100 | Train Loss: 0.0429 | Val Loss: 0.0317\n",
            "Epoch 9/100 | Train Loss: 0.0402 | Val Loss: 0.0323\n",
            "Epoch 10/100 | Train Loss: 0.0386 | Val Loss: 0.0329\n",
            "Epoch 11/100 | Train Loss: 0.0369 | Val Loss: 0.0302\n",
            "Epoch 12/100 | Train Loss: 0.0351 | Val Loss: 0.0286\n",
            "Epoch 13/100 | Train Loss: 0.0338 | Val Loss: 0.0275\n",
            "Epoch 14/100 | Train Loss: 0.0330 | Val Loss: 0.0266\n",
            "Epoch 15/100 | Train Loss: 0.0323 | Val Loss: 0.0259\n",
            "Epoch 16/100 | Train Loss: 0.0315 | Val Loss: 0.0255\n",
            "Epoch 17/100 | Train Loss: 0.0306 | Val Loss: 0.0253\n",
            "Epoch 18/100 | Train Loss: 0.0300 | Val Loss: 0.0250\n",
            "Epoch 19/100 | Train Loss: 0.0297 | Val Loss: 0.0247\n",
            "Epoch 20/100 | Train Loss: 0.0293 | Val Loss: 0.0244\n",
            "Epoch 21/100 | Train Loss: 0.0289 | Val Loss: 0.0242\n",
            "Epoch 22/100 | Train Loss: 0.0285 | Val Loss: 0.0240\n",
            "Epoch 23/100 | Train Loss: 0.0283 | Val Loss: 0.0239\n",
            "Epoch 24/100 | Train Loss: 0.0281 | Val Loss: 0.0238\n",
            "Epoch 25/100 | Train Loss: 0.0281 | Val Loss: 0.0237\n",
            "Epoch 26/100 | Train Loss: 0.0278 | Val Loss: 0.0235\n",
            "Epoch 27/100 | Train Loss: 0.0275 | Val Loss: 0.0234\n",
            "Epoch 28/100 | Train Loss: 0.0273 | Val Loss: 0.0232\n",
            "Epoch 29/100 | Train Loss: 0.0271 | Val Loss: 0.0231\n",
            "Epoch 30/100 | Train Loss: 0.0269 | Val Loss: 0.0230\n",
            "Epoch 31/100 | Train Loss: 0.0267 | Val Loss: 0.0228\n",
            "Epoch 32/100 | Train Loss: 0.0266 | Val Loss: 0.0228\n",
            "Epoch 33/100 | Train Loss: 0.0265 | Val Loss: 0.0227\n",
            "Epoch 34/100 | Train Loss: 0.0264 | Val Loss: 0.0226\n",
            "Epoch 35/100 | Train Loss: 0.0263 | Val Loss: 0.0226\n",
            "Epoch 36/100 | Train Loss: 0.0262 | Val Loss: 0.0226\n",
            "Epoch 37/100 | Train Loss: 0.0261 | Val Loss: 0.0225\n",
            "Epoch 38/100 | Train Loss: 0.0265 | Val Loss: 0.0225\n",
            "Epoch 39/100 | Train Loss: 0.0261 | Val Loss: 0.0226\n",
            "Epoch 40/100 | Train Loss: 0.0260 | Val Loss: 0.0226\n",
            "Epoch 41/100 | Train Loss: 0.0260 | Val Loss: 0.0225\n",
            "Epoch 42/100 | Train Loss: 0.0259 | Val Loss: 0.0223\n",
            "Epoch 43/100 | Train Loss: 0.0258 | Val Loss: 0.0221\n",
            "Epoch 44/100 | Train Loss: 0.0257 | Val Loss: 0.0221\n",
            "Epoch 45/100 | Train Loss: 0.0257 | Val Loss: 0.0220\n",
            "Epoch 46/100 | Train Loss: 0.0256 | Val Loss: 0.0220\n",
            "Epoch 47/100 | Train Loss: 0.0255 | Val Loss: 0.0220\n",
            "Epoch 48/100 | Train Loss: 0.0253 | Val Loss: 0.0220\n",
            "Epoch 49/100 | Train Loss: 0.0251 | Val Loss: 0.0221\n",
            "Epoch 50/100 | Train Loss: 0.0250 | Val Loss: 0.0222\n",
            "Epoch 51/100 | Train Loss: 0.0250 | Val Loss: 0.0218\n",
            "Epoch 52/100 | Train Loss: 0.0248 | Val Loss: 0.0224\n",
            "Epoch 53/100 | Train Loss: 0.0247 | Val Loss: 0.0222\n",
            "Epoch 54/100 | Train Loss: 0.0245 | Val Loss: 0.0227\n",
            "Epoch 55/100 | Train Loss: 0.0243 | Val Loss: 0.0213\n",
            "Epoch 56/100 | Train Loss: 0.0243 | Val Loss: 0.0213\n",
            "Epoch 57/100 | Train Loss: 0.0242 | Val Loss: 0.0207\n",
            "Epoch 58/100 | Train Loss: 0.0244 | Val Loss: 0.0205\n",
            "Epoch 59/100 | Train Loss: 0.0239 | Val Loss: 0.0208\n",
            "Epoch 60/100 | Train Loss: 0.0238 | Val Loss: 0.0205\n",
            "Epoch 61/100 | Train Loss: 0.0238 | Val Loss: 0.0201\n",
            "Epoch 62/100 | Train Loss: 0.0236 | Val Loss: 0.0200\n",
            "Epoch 63/100 | Train Loss: 0.0236 | Val Loss: 0.0198\n",
            "Epoch 64/100 | Train Loss: 0.0235 | Val Loss: 0.0202\n",
            "Epoch 65/100 | Train Loss: 0.0236 | Val Loss: 0.0203\n",
            "Epoch 66/100 | Train Loss: 0.0238 | Val Loss: 0.0192\n",
            "Epoch 67/100 | Train Loss: 0.0234 | Val Loss: 0.0196\n",
            "Epoch 68/100 | Train Loss: 0.0234 | Val Loss: 0.0197\n",
            "Epoch 69/100 | Train Loss: 0.0234 | Val Loss: 0.0196\n",
            "Epoch 70/100 | Train Loss: 0.0237 | Val Loss: 0.0192\n",
            "Epoch 71/100 | Train Loss: 0.0233 | Val Loss: 0.0195\n",
            "Epoch 72/100 | Train Loss: 0.0232 | Val Loss: 0.0194\n",
            "Epoch 73/100 | Train Loss: 0.0232 | Val Loss: 0.0196\n",
            "Epoch 74/100 | Train Loss: 0.0232 | Val Loss: 0.0194\n",
            "Epoch 75/100 | Train Loss: 0.0232 | Val Loss: 0.0195\n",
            "Epoch 76/100 | Train Loss: 0.0232 | Val Loss: 0.0194\n",
            "Epoch 77/100 | Train Loss: 0.0231 | Val Loss: 0.0195\n",
            "Epoch 78/100 | Train Loss: 0.0232 | Val Loss: 0.0194\n",
            "Epoch 79/100 | Train Loss: 0.0231 | Val Loss: 0.0193\n",
            "Epoch 80/100 | Train Loss: 0.0228 | Val Loss: 0.0193\n",
            "Epoch 81/100 | Train Loss: 0.0228 | Val Loss: 0.0195\n",
            "Epoch 82/100 | Train Loss: 0.0228 | Val Loss: 0.0194\n",
            "Epoch 83/100 | Train Loss: 0.0227 | Val Loss: 0.0192\n",
            "Epoch 84/100 | Train Loss: 0.0227 | Val Loss: 0.0196\n",
            "Epoch 85/100 | Train Loss: 0.0225 | Val Loss: 0.0194\n",
            "Epoch 86/100 | Train Loss: 0.0224 | Val Loss: 0.0192\n",
            "Epoch 87/100 | Train Loss: 0.0229 | Val Loss: 0.0187\n",
            "Epoch 88/100 | Train Loss: 0.0231 | Val Loss: 0.0183\n",
            "Epoch 89/100 | Train Loss: 0.0226 | Val Loss: 0.0188\n",
            "Epoch 90/100 | Train Loss: 0.0224 | Val Loss: 0.0190\n",
            "Epoch 91/100 | Train Loss: 0.0223 | Val Loss: 0.0190\n",
            "Epoch 92/100 | Train Loss: 0.0224 | Val Loss: 0.0195\n",
            "Epoch 93/100 | Train Loss: 0.0225 | Val Loss: 0.0198\n",
            "Epoch 94/100 | Train Loss: 0.0221 | Val Loss: 0.0198\n",
            "Epoch 95/100 | Train Loss: 0.0221 | Val Loss: 0.0200\n",
            "Epoch 96/100 | Train Loss: 0.0220 | Val Loss: 0.0200\n",
            "Epoch 97/100 | Train Loss: 0.0219 | Val Loss: 0.0199\n",
            "Epoch 98/100 | Train Loss: 0.0220 | Val Loss: 0.0200\n",
            "Epoch 99/100 | Train Loss: 0.0219 | Val Loss: 0.0198\n",
            "Epoch 100/100 | Train Loss: 0.0220 | Val Loss: 0.0197\n",
            "Saved best model as best_lstm_dam_lstm1_weights.pt\n"
          ]
        }
      ],
      "source": [
        "dam_model_1 = LSTMResidualModel(target_len = 96, num_layers = 1)\n",
        "dam_lstm1_weights = train_model(dam_model_1, dam_train_loader, dam_val_loader, device = device, model_instance_name = \"dam_lstm1_weights\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PBeNlCJjaUjw",
        "outputId": "52578676-c2a7-491c-c264-098fc25e5b8f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100 | Train Loss: 0.0218 | Val Loss: 0.0198\n",
            "Epoch 2/100 | Train Loss: 0.0218 | Val Loss: 0.0202\n",
            "Epoch 3/100 | Train Loss: 0.0217 | Val Loss: 0.0202\n",
            "Epoch 4/100 | Train Loss: 0.0217 | Val Loss: 0.0203\n",
            "Epoch 5/100 | Train Loss: 0.0216 | Val Loss: 0.0203\n",
            "Epoch 6/100 | Train Loss: 0.0215 | Val Loss: 0.0203\n",
            "Epoch 7/100 | Train Loss: 0.0214 | Val Loss: 0.0206\n",
            "Epoch 8/100 | Train Loss: 0.0214 | Val Loss: 0.0205\n",
            "Epoch 9/100 | Train Loss: 0.0214 | Val Loss: 0.0203\n",
            "Epoch 10/100 | Train Loss: 0.0212 | Val Loss: 0.0202\n",
            "Epoch 11/100 | Train Loss: 0.0213 | Val Loss: 0.0200\n",
            "Epoch 12/100 | Train Loss: 0.0212 | Val Loss: 0.0199\n",
            "Epoch 13/100 | Train Loss: 0.0212 | Val Loss: 0.0196\n",
            "Epoch 14/100 | Train Loss: 0.0212 | Val Loss: 0.0194\n",
            "Epoch 15/100 | Train Loss: 0.0214 | Val Loss: 0.0198\n",
            "Epoch 16/100 | Train Loss: 0.0215 | Val Loss: 0.0186\n",
            "Epoch 17/100 | Train Loss: 0.0215 | Val Loss: 0.0194\n",
            "Epoch 18/100 | Train Loss: 0.0212 | Val Loss: 0.0195\n",
            "Epoch 19/100 | Train Loss: 0.0209 | Val Loss: 0.0197\n",
            "Epoch 20/100 | Train Loss: 0.0209 | Val Loss: 0.0198\n",
            "Epoch 21/100 | Train Loss: 0.0209 | Val Loss: 0.0197\n",
            "Epoch 22/100 | Train Loss: 0.0210 | Val Loss: 0.0194\n",
            "Epoch 23/100 | Train Loss: 0.0210 | Val Loss: 0.0193\n",
            "Epoch 24/100 | Train Loss: 0.0207 | Val Loss: 0.0192\n",
            "Epoch 25/100 | Train Loss: 0.0206 | Val Loss: 0.0193\n",
            "Epoch 26/100 | Train Loss: 0.0206 | Val Loss: 0.0194\n",
            "Epoch 27/100 | Train Loss: 0.0210 | Val Loss: 0.0197\n",
            "Epoch 28/100 | Train Loss: 0.0206 | Val Loss: 0.0197\n",
            "Epoch 29/100 | Train Loss: 0.0205 | Val Loss: 0.0192\n",
            "Epoch 30/100 | Train Loss: 0.0205 | Val Loss: 0.0202\n",
            "Epoch 31/100 | Train Loss: 0.0204 | Val Loss: 0.0198\n",
            "Epoch 32/100 | Train Loss: 0.0205 | Val Loss: 0.0203\n",
            "Epoch 33/100 | Train Loss: 0.0204 | Val Loss: 0.0202\n",
            "Epoch 34/100 | Train Loss: 0.0205 | Val Loss: 0.0204\n",
            "Epoch 35/100 | Train Loss: 0.0203 | Val Loss: 0.0202\n",
            "Epoch 36/100 | Train Loss: 0.0204 | Val Loss: 0.0198\n",
            "Epoch 37/100 | Train Loss: 0.0206 | Val Loss: 0.0192\n",
            "Epoch 38/100 | Train Loss: 0.0202 | Val Loss: 0.0199\n",
            "Epoch 39/100 | Train Loss: 0.0202 | Val Loss: 0.0193\n",
            "Epoch 40/100 | Train Loss: 0.0202 | Val Loss: 0.0198\n",
            "Epoch 41/100 | Train Loss: 0.0201 | Val Loss: 0.0197\n",
            "Epoch 42/100 | Train Loss: 0.0201 | Val Loss: 0.0197\n",
            "Epoch 43/100 | Train Loss: 0.0202 | Val Loss: 0.0201\n",
            "Epoch 44/100 | Train Loss: 0.0199 | Val Loss: 0.0198\n",
            "Epoch 45/100 | Train Loss: 0.0201 | Val Loss: 0.0195\n",
            "Epoch 46/100 | Train Loss: 0.0198 | Val Loss: 0.0196\n",
            "Epoch 47/100 | Train Loss: 0.0198 | Val Loss: 0.0198\n",
            "Epoch 48/100 | Train Loss: 0.0197 | Val Loss: 0.0201\n",
            "Epoch 49/100 | Train Loss: 0.0198 | Val Loss: 0.0202\n",
            "Epoch 50/100 | Train Loss: 0.0198 | Val Loss: 0.0216\n",
            "Epoch 51/100 | Train Loss: 0.0203 | Val Loss: 0.0198\n",
            "Epoch 52/100 | Train Loss: 0.0196 | Val Loss: 0.0190\n",
            "Epoch 53/100 | Train Loss: 0.0195 | Val Loss: 0.0189\n",
            "Epoch 54/100 | Train Loss: 0.0194 | Val Loss: 0.0201\n",
            "Epoch 55/100 | Train Loss: 0.0193 | Val Loss: 0.0194\n",
            "Epoch 56/100 | Train Loss: 0.0193 | Val Loss: 0.0192\n",
            "Epoch 57/100 | Train Loss: 0.0192 | Val Loss: 0.0204\n",
            "Epoch 58/100 | Train Loss: 0.0194 | Val Loss: 0.0200\n",
            "Epoch 59/100 | Train Loss: 0.0195 | Val Loss: 0.0196\n",
            "Epoch 60/100 | Train Loss: 0.0191 | Val Loss: 0.0194\n",
            "Epoch 61/100 | Train Loss: 0.0191 | Val Loss: 0.0198\n",
            "Epoch 62/100 | Train Loss: 0.0189 | Val Loss: 0.0197\n",
            "Epoch 63/100 | Train Loss: 0.0191 | Val Loss: 0.0202\n",
            "Epoch 64/100 | Train Loss: 0.0189 | Val Loss: 0.0206\n",
            "Epoch 65/100 | Train Loss: 0.0189 | Val Loss: 0.0210\n",
            "Epoch 66/100 | Train Loss: 0.0194 | Val Loss: 0.0196\n",
            "Epoch 67/100 | Train Loss: 0.0189 | Val Loss: 0.0196\n",
            "Epoch 68/100 | Train Loss: 0.0187 | Val Loss: 0.0197\n",
            "Epoch 69/100 | Train Loss: 0.0187 | Val Loss: 0.0199\n",
            "Epoch 70/100 | Train Loss: 0.0187 | Val Loss: 0.0222\n",
            "Epoch 71/100 | Train Loss: 0.0191 | Val Loss: 0.0213\n",
            "Epoch 72/100 | Train Loss: 0.0187 | Val Loss: 0.0205\n",
            "Epoch 73/100 | Train Loss: 0.0189 | Val Loss: 0.0205\n",
            "Epoch 74/100 | Train Loss: 0.0187 | Val Loss: 0.0216\n",
            "Epoch 75/100 | Train Loss: 0.0186 | Val Loss: 0.0207\n",
            "Epoch 76/100 | Train Loss: 0.0185 | Val Loss: 0.0210\n",
            "Epoch 77/100 | Train Loss: 0.0185 | Val Loss: 0.0214\n",
            "Epoch 78/100 | Train Loss: 0.0188 | Val Loss: 0.0196\n",
            "Epoch 79/100 | Train Loss: 0.0190 | Val Loss: 0.0221\n",
            "Epoch 80/100 | Train Loss: 0.0189 | Val Loss: 0.0192\n",
            "Epoch 81/100 | Train Loss: 0.0186 | Val Loss: 0.0209\n",
            "Epoch 82/100 | Train Loss: 0.0187 | Val Loss: 0.0199\n",
            "Epoch 83/100 | Train Loss: 0.0187 | Val Loss: 0.0195\n",
            "Epoch 84/100 | Train Loss: 0.0184 | Val Loss: 0.0213\n",
            "Epoch 85/100 | Train Loss: 0.0188 | Val Loss: 0.0199\n",
            "Epoch 86/100 | Train Loss: 0.0183 | Val Loss: 0.0196\n",
            "Epoch 87/100 | Train Loss: 0.0183 | Val Loss: 0.0206\n",
            "Epoch 88/100 | Train Loss: 0.0184 | Val Loss: 0.0201\n",
            "Epoch 89/100 | Train Loss: 0.0180 | Val Loss: 0.0201\n",
            "Epoch 90/100 | Train Loss: 0.0180 | Val Loss: 0.0210\n",
            "Epoch 91/100 | Train Loss: 0.0181 | Val Loss: 0.0205\n",
            "Epoch 92/100 | Train Loss: 0.0178 | Val Loss: 0.0201\n",
            "Epoch 93/100 | Train Loss: 0.0180 | Val Loss: 0.0202\n",
            "Epoch 94/100 | Train Loss: 0.0180 | Val Loss: 0.0214\n",
            "Epoch 95/100 | Train Loss: 0.0181 | Val Loss: 0.0205\n",
            "Epoch 96/100 | Train Loss: 0.0179 | Val Loss: 0.0208\n",
            "Epoch 97/100 | Train Loss: 0.0177 | Val Loss: 0.0203\n",
            "Epoch 98/100 | Train Loss: 0.0182 | Val Loss: 0.0202\n",
            "Epoch 99/100 | Train Loss: 0.0179 | Val Loss: 0.0206\n",
            "Epoch 100/100 | Train Loss: 0.0179 | Val Loss: 0.0212\n",
            "Saved best model as best_lstm_dam_lstm1_200_weights.pt\n"
          ]
        }
      ],
      "source": [
        "dam_lstm1_200_weights = train_model(dam_model_1, dam_train_loader, dam_val_loader, device = device, model_instance_name = \"dam_lstm1_200_weights\", num_epochs = 100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q5vaCbhcA5f_",
        "outputId": "80a39a09-ae53-46a0-a13d-052846b180d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100 | Train Loss: 0.1033 | Val Loss: 0.0536\n",
            "Epoch 2/100 | Train Loss: 0.0603 | Val Loss: 0.0419\n",
            "Epoch 3/100 | Train Loss: 0.0572 | Val Loss: 0.0414\n",
            "Epoch 4/100 | Train Loss: 0.0521 | Val Loss: 0.0394\n",
            "Epoch 5/100 | Train Loss: 0.0506 | Val Loss: 0.0346\n",
            "Epoch 6/100 | Train Loss: 0.0437 | Val Loss: 0.0337\n",
            "Epoch 7/100 | Train Loss: 0.0388 | Val Loss: 0.0326\n",
            "Epoch 8/100 | Train Loss: 0.0362 | Val Loss: 0.0320\n",
            "Epoch 9/100 | Train Loss: 0.0341 | Val Loss: 0.0310\n",
            "Epoch 10/100 | Train Loss: 0.0333 | Val Loss: 0.0306\n",
            "Epoch 11/100 | Train Loss: 0.0320 | Val Loss: 0.0300\n",
            "Epoch 12/100 | Train Loss: 0.0313 | Val Loss: 0.0298\n",
            "Epoch 13/100 | Train Loss: 0.0309 | Val Loss: 0.0287\n",
            "Epoch 14/100 | Train Loss: 0.0327 | Val Loss: 0.0264\n",
            "Epoch 15/100 | Train Loss: 0.0299 | Val Loss: 0.0260\n",
            "Epoch 16/100 | Train Loss: 0.0292 | Val Loss: 0.0255\n",
            "Epoch 17/100 | Train Loss: 0.0289 | Val Loss: 0.0251\n",
            "Epoch 18/100 | Train Loss: 0.0287 | Val Loss: 0.0248\n",
            "Epoch 19/100 | Train Loss: 0.0281 | Val Loss: 0.0241\n",
            "Epoch 20/100 | Train Loss: 0.0278 | Val Loss: 0.0237\n",
            "Epoch 21/100 | Train Loss: 0.0276 | Val Loss: 0.0236\n",
            "Epoch 22/100 | Train Loss: 0.0272 | Val Loss: 0.0239\n",
            "Epoch 23/100 | Train Loss: 0.0269 | Val Loss: 0.0234\n",
            "Epoch 24/100 | Train Loss: 0.0267 | Val Loss: 0.0240\n",
            "Epoch 25/100 | Train Loss: 0.0265 | Val Loss: 0.0240\n",
            "Epoch 26/100 | Train Loss: 0.0271 | Val Loss: 0.0222\n",
            "Epoch 27/100 | Train Loss: 0.0262 | Val Loss: 0.0221\n",
            "Epoch 28/100 | Train Loss: 0.0260 | Val Loss: 0.0223\n",
            "Epoch 29/100 | Train Loss: 0.0259 | Val Loss: 0.0222\n",
            "Epoch 30/100 | Train Loss: 0.0257 | Val Loss: 0.0224\n",
            "Epoch 31/100 | Train Loss: 0.0257 | Val Loss: 0.0221\n",
            "Epoch 32/100 | Train Loss: 0.0256 | Val Loss: 0.0223\n",
            "Epoch 33/100 | Train Loss: 0.0254 | Val Loss: 0.0219\n",
            "Epoch 34/100 | Train Loss: 0.0253 | Val Loss: 0.0223\n",
            "Epoch 35/100 | Train Loss: 0.0251 | Val Loss: 0.0223\n",
            "Epoch 36/100 | Train Loss: 0.0249 | Val Loss: 0.0220\n",
            "Epoch 37/100 | Train Loss: 0.0247 | Val Loss: 0.0214\n",
            "Epoch 38/100 | Train Loss: 0.0246 | Val Loss: 0.0211\n",
            "Epoch 39/100 | Train Loss: 0.0243 | Val Loss: 0.0209\n",
            "Epoch 40/100 | Train Loss: 0.0245 | Val Loss: 0.0212\n",
            "Epoch 41/100 | Train Loss: 0.0243 | Val Loss: 0.0206\n",
            "Epoch 42/100 | Train Loss: 0.0243 | Val Loss: 0.0207\n",
            "Epoch 43/100 | Train Loss: 0.0239 | Val Loss: 0.0206\n",
            "Epoch 44/100 | Train Loss: 0.0240 | Val Loss: 0.0206\n",
            "Epoch 45/100 | Train Loss: 0.0237 | Val Loss: 0.0204\n",
            "Epoch 46/100 | Train Loss: 0.0237 | Val Loss: 0.0202\n",
            "Epoch 47/100 | Train Loss: 0.0234 | Val Loss: 0.0201\n",
            "Epoch 48/100 | Train Loss: 0.0235 | Val Loss: 0.0204\n",
            "Epoch 49/100 | Train Loss: 0.0233 | Val Loss: 0.0198\n",
            "Epoch 50/100 | Train Loss: 0.0233 | Val Loss: 0.0193\n",
            "Epoch 51/100 | Train Loss: 0.0231 | Val Loss: 0.0195\n",
            "Epoch 52/100 | Train Loss: 0.0235 | Val Loss: 0.0195\n",
            "Epoch 53/100 | Train Loss: 0.0230 | Val Loss: 0.0200\n",
            "Epoch 54/100 | Train Loss: 0.0229 | Val Loss: 0.0196\n",
            "Epoch 55/100 | Train Loss: 0.0228 | Val Loss: 0.0198\n",
            "Epoch 56/100 | Train Loss: 0.0227 | Val Loss: 0.0198\n",
            "Epoch 57/100 | Train Loss: 0.0224 | Val Loss: 0.0202\n",
            "Epoch 58/100 | Train Loss: 0.0224 | Val Loss: 0.0197\n",
            "Epoch 59/100 | Train Loss: 0.0224 | Val Loss: 0.0192\n",
            "Epoch 60/100 | Train Loss: 0.0225 | Val Loss: 0.0194\n",
            "Epoch 61/100 | Train Loss: 0.0229 | Val Loss: 0.0195\n",
            "Epoch 62/100 | Train Loss: 0.0229 | Val Loss: 0.0192\n",
            "Epoch 63/100 | Train Loss: 0.0221 | Val Loss: 0.0193\n",
            "Epoch 64/100 | Train Loss: 0.0222 | Val Loss: 0.0193\n",
            "Epoch 65/100 | Train Loss: 0.0222 | Val Loss: 0.0192\n",
            "Epoch 66/100 | Train Loss: 0.0221 | Val Loss: 0.0191\n",
            "Epoch 67/100 | Train Loss: 0.0223 | Val Loss: 0.0199\n",
            "Epoch 68/100 | Train Loss: 0.0219 | Val Loss: 0.0196\n",
            "Epoch 69/100 | Train Loss: 0.0217 | Val Loss: 0.0196\n",
            "Epoch 70/100 | Train Loss: 0.0215 | Val Loss: 0.0196\n",
            "Epoch 71/100 | Train Loss: 0.0216 | Val Loss: 0.0195\n",
            "Epoch 72/100 | Train Loss: 0.0217 | Val Loss: 0.0196\n",
            "Epoch 73/100 | Train Loss: 0.0219 | Val Loss: 0.0201\n",
            "Epoch 74/100 | Train Loss: 0.0216 | Val Loss: 0.0194\n",
            "Epoch 75/100 | Train Loss: 0.0215 | Val Loss: 0.0202\n",
            "Epoch 76/100 | Train Loss: 0.0213 | Val Loss: 0.0199\n",
            "Epoch 77/100 | Train Loss: 0.0213 | Val Loss: 0.0200\n",
            "Epoch 78/100 | Train Loss: 0.0212 | Val Loss: 0.0198\n",
            "Epoch 79/100 | Train Loss: 0.0212 | Val Loss: 0.0197\n",
            "Epoch 80/100 | Train Loss: 0.0215 | Val Loss: 0.0193\n",
            "Epoch 81/100 | Train Loss: 0.0210 | Val Loss: 0.0198\n",
            "Epoch 82/100 | Train Loss: 0.0209 | Val Loss: 0.0201\n",
            "Epoch 83/100 | Train Loss: 0.0213 | Val Loss: 0.0197\n",
            "Epoch 84/100 | Train Loss: 0.0213 | Val Loss: 0.0191\n",
            "Epoch 85/100 | Train Loss: 0.0211 | Val Loss: 0.0196\n",
            "Epoch 86/100 | Train Loss: 0.0211 | Val Loss: 0.0198\n",
            "Epoch 87/100 | Train Loss: 0.0206 | Val Loss: 0.0200\n",
            "Epoch 88/100 | Train Loss: 0.0206 | Val Loss: 0.0201\n",
            "Epoch 89/100 | Train Loss: 0.0207 | Val Loss: 0.0196\n",
            "Epoch 90/100 | Train Loss: 0.0205 | Val Loss: 0.0199\n",
            "Epoch 91/100 | Train Loss: 0.0205 | Val Loss: 0.0190\n",
            "Epoch 92/100 | Train Loss: 0.0202 | Val Loss: 0.0196\n",
            "Epoch 93/100 | Train Loss: 0.0203 | Val Loss: 0.0196\n",
            "Epoch 94/100 | Train Loss: 0.0204 | Val Loss: 0.0196\n",
            "Epoch 95/100 | Train Loss: 0.0201 | Val Loss: 0.0196\n",
            "Epoch 96/100 | Train Loss: 0.0200 | Val Loss: 0.0194\n",
            "Epoch 97/100 | Train Loss: 0.0202 | Val Loss: 0.0186\n",
            "Epoch 98/100 | Train Loss: 0.0205 | Val Loss: 0.0195\n",
            "Epoch 99/100 | Train Loss: 0.0200 | Val Loss: 0.0198\n",
            "Epoch 100/100 | Train Loss: 0.0198 | Val Loss: 0.0190\n",
            "Saved best model as best_lstm_dam_lstm2_weights.pt\n"
          ]
        }
      ],
      "source": [
        "dam_model_2 = LSTMResidualModel(target_len = 96, num_layers = 2)\n",
        "dam_lstm2_weights = train_model(dam_model_2, dam_train_loader, dam_val_loader, device = device, model_instance_name = \"dam_lstm2_weights\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94hPne26bm4u",
        "outputId": "f0cd7ada-c86e-453f-9ebe-9fe9ed154308"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100 | Train Loss: 0.0198 | Val Loss: 0.0193\n",
            "Epoch 2/100 | Train Loss: 0.0195 | Val Loss: 0.0190\n",
            "Epoch 3/100 | Train Loss: 0.0195 | Val Loss: 0.0195\n",
            "Epoch 4/100 | Train Loss: 0.0197 | Val Loss: 0.0199\n",
            "Epoch 5/100 | Train Loss: 0.0198 | Val Loss: 0.0201\n",
            "Epoch 6/100 | Train Loss: 0.0194 | Val Loss: 0.0205\n",
            "Epoch 7/100 | Train Loss: 0.0195 | Val Loss: 0.0208\n",
            "Epoch 8/100 | Train Loss: 0.0191 | Val Loss: 0.0209\n",
            "Epoch 9/100 | Train Loss: 0.0190 | Val Loss: 0.0204\n",
            "Epoch 10/100 | Train Loss: 0.0189 | Val Loss: 0.0201\n",
            "Epoch 11/100 | Train Loss: 0.0190 | Val Loss: 0.0211\n",
            "Epoch 12/100 | Train Loss: 0.0189 | Val Loss: 0.0213\n",
            "Epoch 13/100 | Train Loss: 0.0186 | Val Loss: 0.0206\n",
            "Epoch 14/100 | Train Loss: 0.0186 | Val Loss: 0.0198\n",
            "Epoch 15/100 | Train Loss: 0.0187 | Val Loss: 0.0204\n",
            "Epoch 16/100 | Train Loss: 0.0186 | Val Loss: 0.0212\n",
            "Epoch 17/100 | Train Loss: 0.0185 | Val Loss: 0.0204\n",
            "Epoch 18/100 | Train Loss: 0.0183 | Val Loss: 0.0203\n",
            "Epoch 19/100 | Train Loss: 0.0190 | Val Loss: 0.0219\n",
            "Epoch 20/100 | Train Loss: 0.0188 | Val Loss: 0.0200\n",
            "Epoch 21/100 | Train Loss: 0.0180 | Val Loss: 0.0209\n",
            "Epoch 22/100 | Train Loss: 0.0179 | Val Loss: 0.0198\n",
            "Epoch 23/100 | Train Loss: 0.0178 | Val Loss: 0.0206\n",
            "Epoch 24/100 | Train Loss: 0.0177 | Val Loss: 0.0205\n",
            "Epoch 25/100 | Train Loss: 0.0176 | Val Loss: 0.0201\n",
            "Epoch 26/100 | Train Loss: 0.0177 | Val Loss: 0.0211\n",
            "Epoch 27/100 | Train Loss: 0.0175 | Val Loss: 0.0201\n",
            "Epoch 28/100 | Train Loss: 0.0174 | Val Loss: 0.0219\n",
            "Epoch 29/100 | Train Loss: 0.0174 | Val Loss: 0.0213\n",
            "Epoch 30/100 | Train Loss: 0.0172 | Val Loss: 0.0214\n",
            "Epoch 31/100 | Train Loss: 0.0174 | Val Loss: 0.0215\n",
            "Epoch 32/100 | Train Loss: 0.0169 | Val Loss: 0.0210\n",
            "Epoch 33/100 | Train Loss: 0.0168 | Val Loss: 0.0213\n",
            "Epoch 34/100 | Train Loss: 0.0168 | Val Loss: 0.0216\n",
            "Epoch 35/100 | Train Loss: 0.0169 | Val Loss: 0.0210\n",
            "Epoch 36/100 | Train Loss: 0.0167 | Val Loss: 0.0222\n",
            "Epoch 37/100 | Train Loss: 0.0167 | Val Loss: 0.0216\n",
            "Epoch 38/100 | Train Loss: 0.0166 | Val Loss: 0.0211\n",
            "Epoch 39/100 | Train Loss: 0.0164 | Val Loss: 0.0227\n",
            "Epoch 40/100 | Train Loss: 0.0164 | Val Loss: 0.0225\n",
            "Epoch 41/100 | Train Loss: 0.0160 | Val Loss: 0.0223\n",
            "Epoch 42/100 | Train Loss: 0.0160 | Val Loss: 0.0214\n",
            "Epoch 43/100 | Train Loss: 0.0160 | Val Loss: 0.0212\n",
            "Epoch 44/100 | Train Loss: 0.0168 | Val Loss: 0.0216\n",
            "Epoch 45/100 | Train Loss: 0.0164 | Val Loss: 0.0215\n",
            "Epoch 46/100 | Train Loss: 0.0162 | Val Loss: 0.0215\n",
            "Epoch 47/100 | Train Loss: 0.0157 | Val Loss: 0.0232\n",
            "Epoch 48/100 | Train Loss: 0.0157 | Val Loss: 0.0212\n",
            "Epoch 49/100 | Train Loss: 0.0160 | Val Loss: 0.0237\n",
            "Epoch 50/100 | Train Loss: 0.0159 | Val Loss: 0.0211\n",
            "Epoch 51/100 | Train Loss: 0.0156 | Val Loss: 0.0229\n",
            "Epoch 52/100 | Train Loss: 0.0152 | Val Loss: 0.0218\n",
            "Epoch 53/100 | Train Loss: 0.0159 | Val Loss: 0.0223\n",
            "Epoch 54/100 | Train Loss: 0.0160 | Val Loss: 0.0221\n",
            "Epoch 55/100 | Train Loss: 0.0152 | Val Loss: 0.0226\n",
            "Epoch 56/100 | Train Loss: 0.0149 | Val Loss: 0.0228\n",
            "Epoch 57/100 | Train Loss: 0.0150 | Val Loss: 0.0233\n",
            "Epoch 58/100 | Train Loss: 0.0154 | Val Loss: 0.0235\n",
            "Epoch 59/100 | Train Loss: 0.0152 | Val Loss: 0.0229\n",
            "Epoch 60/100 | Train Loss: 0.0148 | Val Loss: 0.0223\n",
            "Epoch 61/100 | Train Loss: 0.0146 | Val Loss: 0.0237\n",
            "Epoch 62/100 | Train Loss: 0.0146 | Val Loss: 0.0229\n",
            "Epoch 63/100 | Train Loss: 0.0144 | Val Loss: 0.0236\n",
            "Epoch 64/100 | Train Loss: 0.0144 | Val Loss: 0.0221\n",
            "Epoch 65/100 | Train Loss: 0.0146 | Val Loss: 0.0228\n",
            "Epoch 66/100 | Train Loss: 0.0151 | Val Loss: 0.0215\n",
            "Epoch 67/100 | Train Loss: 0.0145 | Val Loss: 0.0227\n",
            "Epoch 68/100 | Train Loss: 0.0143 | Val Loss: 0.0206\n",
            "Epoch 69/100 | Train Loss: 0.0141 | Val Loss: 0.0228\n",
            "Epoch 70/100 | Train Loss: 0.0139 | Val Loss: 0.0229\n",
            "Epoch 71/100 | Train Loss: 0.0141 | Val Loss: 0.0226\n",
            "Epoch 72/100 | Train Loss: 0.0140 | Val Loss: 0.0221\n",
            "Epoch 73/100 | Train Loss: 0.0142 | Val Loss: 0.0223\n",
            "Epoch 74/100 | Train Loss: 0.0140 | Val Loss: 0.0223\n",
            "Epoch 75/100 | Train Loss: 0.0137 | Val Loss: 0.0218\n",
            "Epoch 76/100 | Train Loss: 0.0136 | Val Loss: 0.0228\n",
            "Epoch 77/100 | Train Loss: 0.0138 | Val Loss: 0.0209\n",
            "Epoch 78/100 | Train Loss: 0.0136 | Val Loss: 0.0238\n",
            "Epoch 79/100 | Train Loss: 0.0135 | Val Loss: 0.0220\n",
            "Epoch 80/100 | Train Loss: 0.0134 | Val Loss: 0.0240\n",
            "Epoch 81/100 | Train Loss: 0.0138 | Val Loss: 0.0215\n",
            "Epoch 82/100 | Train Loss: 0.0132 | Val Loss: 0.0223\n",
            "Epoch 83/100 | Train Loss: 0.0130 | Val Loss: 0.0235\n",
            "Epoch 84/100 | Train Loss: 0.0128 | Val Loss: 0.0231\n",
            "Epoch 85/100 | Train Loss: 0.0135 | Val Loss: 0.0224\n",
            "Epoch 86/100 | Train Loss: 0.0138 | Val Loss: 0.0265\n",
            "Epoch 87/100 | Train Loss: 0.0131 | Val Loss: 0.0228\n",
            "Epoch 88/100 | Train Loss: 0.0127 | Val Loss: 0.0219\n",
            "Epoch 89/100 | Train Loss: 0.0132 | Val Loss: 0.0235\n",
            "Epoch 90/100 | Train Loss: 0.0133 | Val Loss: 0.0225\n",
            "Epoch 91/100 | Train Loss: 0.0128 | Val Loss: 0.0221\n",
            "Epoch 92/100 | Train Loss: 0.0129 | Val Loss: 0.0231\n",
            "Epoch 93/100 | Train Loss: 0.0127 | Val Loss: 0.0228\n",
            "Epoch 94/100 | Train Loss: 0.0122 | Val Loss: 0.0219\n",
            "Epoch 95/100 | Train Loss: 0.0125 | Val Loss: 0.0229\n",
            "Epoch 96/100 | Train Loss: 0.0124 | Val Loss: 0.0228\n",
            "Epoch 97/100 | Train Loss: 0.0126 | Val Loss: 0.0228\n",
            "Epoch 98/100 | Train Loss: 0.0124 | Val Loss: 0.0224\n",
            "Epoch 99/100 | Train Loss: 0.0121 | Val Loss: 0.0226\n",
            "Epoch 100/100 | Train Loss: 0.0120 | Val Loss: 0.0239\n",
            "Saved best model as best_lstm_dam_lstm2_150_weights.pt\n"
          ]
        }
      ],
      "source": [
        "dam_lstm2_150_weights = train_model(dam_model_2, dam_train_loader, dam_val_loader, device = device, model_instance_name = \"dam_lstm2_150_weights\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zn2TRGGJTXKz",
        "outputId": "8c1111fd-690f-4e0c-f7d0-72893c89cb79"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100 | Train Loss: 0.0913 | Val Loss: 0.0751\n",
            "Epoch 2/100 | Train Loss: 0.0740 | Val Loss: 0.0713\n",
            "Epoch 3/100 | Train Loss: 0.0733 | Val Loss: 0.0708\n",
            "Epoch 4/100 | Train Loss: 0.0732 | Val Loss: 0.0707\n",
            "Epoch 5/100 | Train Loss: 0.0732 | Val Loss: 0.0707\n",
            "Epoch 6/100 | Train Loss: 0.0732 | Val Loss: 0.0707\n",
            "Epoch 7/100 | Train Loss: 0.0732 | Val Loss: 0.0706\n",
            "Epoch 8/100 | Train Loss: 0.0732 | Val Loss: 0.0706\n",
            "Epoch 9/100 | Train Loss: 0.0732 | Val Loss: 0.0706\n",
            "Epoch 10/100 | Train Loss: 0.0732 | Val Loss: 0.0706\n",
            "Epoch 11/100 | Train Loss: 0.0732 | Val Loss: 0.0706\n",
            "Epoch 12/100 | Train Loss: 0.0732 | Val Loss: 0.0706\n",
            "Epoch 13/100 | Train Loss: 0.0732 | Val Loss: 0.0706\n",
            "Epoch 14/100 | Train Loss: 0.0732 | Val Loss: 0.0706\n",
            "Epoch 15/100 | Train Loss: 0.0732 | Val Loss: 0.0706\n",
            "Epoch 16/100 | Train Loss: 0.0732 | Val Loss: 0.0706\n",
            "Epoch 17/100 | Train Loss: 0.0732 | Val Loss: 0.0706\n",
            "Epoch 18/100 | Train Loss: 0.0732 | Val Loss: 0.0706\n",
            "Epoch 19/100 | Train Loss: 0.0732 | Val Loss: 0.0706\n",
            "Epoch 20/100 | Train Loss: 0.0732 | Val Loss: 0.0706\n",
            "Epoch 21/100 | Train Loss: 0.0732 | Val Loss: 0.0706\n",
            "Epoch 22/100 | Train Loss: 0.0732 | Val Loss: 0.0706\n",
            "Epoch 23/100 | Train Loss: 0.0732 | Val Loss: 0.0706\n",
            "Epoch 24/100 | Train Loss: 0.0732 | Val Loss: 0.0706\n",
            "Epoch 25/100 | Train Loss: 0.0732 | Val Loss: 0.0706\n",
            "Epoch 26/100 | Train Loss: 0.0732 | Val Loss: 0.0706\n",
            "Epoch 27/100 | Train Loss: 0.0731 | Val Loss: 0.0707\n",
            "Epoch 28/100 | Train Loss: 0.0732 | Val Loss: 0.0707\n",
            "Epoch 29/100 | Train Loss: 0.0730 | Val Loss: 0.0707\n",
            "Epoch 30/100 | Train Loss: 0.0729 | Val Loss: 0.0669\n",
            "Epoch 31/100 | Train Loss: 0.0724 | Val Loss: 0.0670\n",
            "Epoch 32/100 | Train Loss: 0.0724 | Val Loss: 0.0671\n",
            "Epoch 33/100 | Train Loss: 0.0724 | Val Loss: 0.0671\n",
            "Epoch 34/100 | Train Loss: 0.0724 | Val Loss: 0.0671\n",
            "Epoch 35/100 | Train Loss: 0.0724 | Val Loss: 0.0672\n",
            "Epoch 36/100 | Train Loss: 0.0724 | Val Loss: 0.0672\n",
            "Epoch 37/100 | Train Loss: 0.0724 | Val Loss: 0.0672\n",
            "Epoch 38/100 | Train Loss: 0.0724 | Val Loss: 0.0672\n",
            "Epoch 39/100 | Train Loss: 0.0724 | Val Loss: 0.0672\n",
            "Epoch 40/100 | Train Loss: 0.0724 | Val Loss: 0.0672\n",
            "Epoch 41/100 | Train Loss: 0.0724 | Val Loss: 0.0672\n",
            "Epoch 42/100 | Train Loss: 0.0724 | Val Loss: 0.0672\n",
            "Epoch 43/100 | Train Loss: 0.0724 | Val Loss: 0.0672\n",
            "Epoch 44/100 | Train Loss: 0.0724 | Val Loss: 0.0672\n",
            "Epoch 45/100 | Train Loss: 0.0724 | Val Loss: 0.0672\n",
            "Epoch 46/100 | Train Loss: 0.0724 | Val Loss: 0.0672\n",
            "Epoch 47/100 | Train Loss: 0.0724 | Val Loss: 0.0672\n",
            "Epoch 48/100 | Train Loss: 0.0724 | Val Loss: 0.0672\n",
            "Epoch 49/100 | Train Loss: 0.0724 | Val Loss: 0.0672\n",
            "Epoch 50/100 | Train Loss: 0.0724 | Val Loss: 0.0672\n",
            "Epoch 51/100 | Train Loss: 0.0724 | Val Loss: 0.0672\n",
            "Epoch 52/100 | Train Loss: 0.0724 | Val Loss: 0.0672\n",
            "Epoch 53/100 | Train Loss: 0.0724 | Val Loss: 0.0672\n",
            "Epoch 54/100 | Train Loss: 0.0724 | Val Loss: 0.0672\n",
            "Epoch 55/100 | Train Loss: 0.0724 | Val Loss: 0.0672\n",
            "Epoch 56/100 | Train Loss: 0.0724 | Val Loss: 0.0672\n",
            "Epoch 57/100 | Train Loss: 0.0724 | Val Loss: 0.0672\n",
            "Epoch 58/100 | Train Loss: 0.0724 | Val Loss: 0.0672\n",
            "Epoch 59/100 | Train Loss: 0.0724 | Val Loss: 0.0672\n",
            "Epoch 60/100 | Train Loss: 0.0724 | Val Loss: 0.0672\n",
            "Epoch 61/100 | Train Loss: 0.0724 | Val Loss: 0.0672\n",
            "Epoch 62/100 | Train Loss: 0.0724 | Val Loss: 0.0672\n",
            "Epoch 63/100 | Train Loss: 0.0724 | Val Loss: 0.0672\n",
            "Epoch 64/100 | Train Loss: 0.0724 | Val Loss: 0.0672\n",
            "Epoch 65/100 | Train Loss: 0.0724 | Val Loss: 0.0672\n",
            "Epoch 66/100 | Train Loss: 0.0724 | Val Loss: 0.0672\n",
            "Epoch 67/100 | Train Loss: 0.0724 | Val Loss: 0.0672\n",
            "Epoch 68/100 | Train Loss: 0.0724 | Val Loss: 0.0672\n",
            "Epoch 69/100 | Train Loss: 0.0724 | Val Loss: 0.0672\n",
            "Epoch 70/100 | Train Loss: 0.0724 | Val Loss: 0.0672\n",
            "Epoch 71/100 | Train Loss: 0.0724 | Val Loss: 0.0672\n",
            "Epoch 72/100 | Train Loss: 0.0724 | Val Loss: 0.0672\n",
            "Epoch 73/100 | Train Loss: 0.0724 | Val Loss: 0.0672\n",
            "Epoch 74/100 | Train Loss: 0.0724 | Val Loss: 0.0672\n",
            "Epoch 75/100 | Train Loss: 0.0724 | Val Loss: 0.0672\n",
            "Epoch 76/100 | Train Loss: 0.0724 | Val Loss: 0.0672\n",
            "Epoch 77/100 | Train Loss: 0.0724 | Val Loss: 0.0672\n",
            "Epoch 78/100 | Train Loss: 0.0724 | Val Loss: 0.0672\n",
            "Epoch 79/100 | Train Loss: 0.0724 | Val Loss: 0.0672\n",
            "Epoch 80/100 | Train Loss: 0.0724 | Val Loss: 0.0672\n",
            "Epoch 81/100 | Train Loss: 0.0724 | Val Loss: 0.0672\n",
            "Epoch 82/100 | Train Loss: 0.0724 | Val Loss: 0.0672\n",
            "Epoch 83/100 | Train Loss: 0.0724 | Val Loss: 0.0672\n",
            "Epoch 84/100 | Train Loss: 0.0724 | Val Loss: 0.0672\n",
            "Epoch 85/100 | Train Loss: 0.0724 | Val Loss: 0.0672\n",
            "Epoch 86/100 | Train Loss: 0.0724 | Val Loss: 0.0672\n",
            "Epoch 87/100 | Train Loss: 0.0724 | Val Loss: 0.0672\n",
            "Epoch 88/100 | Train Loss: 0.0724 | Val Loss: 0.0672\n",
            "Epoch 89/100 | Train Loss: 0.0724 | Val Loss: 0.0672\n",
            "Epoch 90/100 | Train Loss: 0.0724 | Val Loss: 0.0672\n",
            "Epoch 91/100 | Train Loss: 0.0724 | Val Loss: 0.0672\n",
            "Epoch 92/100 | Train Loss: 0.0724 | Val Loss: 0.0672\n",
            "Epoch 93/100 | Train Loss: 0.0724 | Val Loss: 0.0672\n",
            "Epoch 94/100 | Train Loss: 0.0724 | Val Loss: 0.0672\n",
            "Epoch 95/100 | Train Loss: 0.0724 | Val Loss: 0.0672\n",
            "Epoch 96/100 | Train Loss: 0.0724 | Val Loss: 0.0672\n",
            "Epoch 97/100 | Train Loss: 0.0724 | Val Loss: 0.0672\n",
            "Epoch 98/100 | Train Loss: 0.0724 | Val Loss: 0.0672\n",
            "Epoch 99/100 | Train Loss: 0.0724 | Val Loss: 0.0672\n",
            "Epoch 100/100 | Train Loss: 0.0724 | Val Loss: 0.0672\n",
            "Saved best model as best_lstm_dam_lstm2_01_weights.pt\n"
          ]
        }
      ],
      "source": [
        "# set learning rate to 0.01\n",
        "dam_model_2_01 = LSTMResidualModel(target_len = 96, num_layers = 2)\n",
        "dam_lstm2_01_weights = train_model(dam_model_2_01, dam_train_loader, dam_val_loader, device = device, model_instance_name = \"dam_lstm2_01_weights\", lr = 1e-2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JtDcdobsXHvW",
        "outputId": "4ede888f-e359-4efc-e829-09b82a3be417"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100 | Train Loss: 0.1184 | Val Loss: 0.0510\n",
            "Epoch 2/100 | Train Loss: 0.0652 | Val Loss: 0.0463\n",
            "Epoch 3/100 | Train Loss: 0.0575 | Val Loss: 0.0402\n",
            "Epoch 4/100 | Train Loss: 0.0490 | Val Loss: 0.0372\n",
            "Epoch 5/100 | Train Loss: 0.0442 | Val Loss: 0.0376\n",
            "Epoch 6/100 | Train Loss: 0.0389 | Val Loss: 0.0307\n",
            "Epoch 7/100 | Train Loss: 0.0361 | Val Loss: 0.0293\n",
            "Epoch 8/100 | Train Loss: 0.0345 | Val Loss: 0.0300\n",
            "Epoch 9/100 | Train Loss: 0.0339 | Val Loss: 0.0294\n",
            "Epoch 10/100 | Train Loss: 0.0327 | Val Loss: 0.0291\n",
            "Epoch 11/100 | Train Loss: 0.0324 | Val Loss: 0.0283\n",
            "Epoch 12/100 | Train Loss: 0.0316 | Val Loss: 0.0278\n",
            "Epoch 13/100 | Train Loss: 0.0308 | Val Loss: 0.0273\n",
            "Epoch 14/100 | Train Loss: 0.0303 | Val Loss: 0.0271\n",
            "Epoch 15/100 | Train Loss: 0.0296 | Val Loss: 0.0266\n",
            "Epoch 16/100 | Train Loss: 0.0291 | Val Loss: 0.0262\n",
            "Epoch 17/100 | Train Loss: 0.0286 | Val Loss: 0.0257\n",
            "Epoch 18/100 | Train Loss: 0.0282 | Val Loss: 0.0251\n",
            "Epoch 19/100 | Train Loss: 0.0279 | Val Loss: 0.0246\n",
            "Epoch 20/100 | Train Loss: 0.0277 | Val Loss: 0.0243\n",
            "Epoch 21/100 | Train Loss: 0.0275 | Val Loss: 0.0243\n",
            "Epoch 22/100 | Train Loss: 0.0271 | Val Loss: 0.0244\n",
            "Epoch 23/100 | Train Loss: 0.0270 | Val Loss: 0.0241\n",
            "Epoch 24/100 | Train Loss: 0.0268 | Val Loss: 0.0238\n",
            "Epoch 25/100 | Train Loss: 0.0265 | Val Loss: 0.0236\n",
            "Epoch 26/100 | Train Loss: 0.0263 | Val Loss: 0.0236\n",
            "Epoch 27/100 | Train Loss: 0.0262 | Val Loss: 0.0235\n",
            "Epoch 28/100 | Train Loss: 0.0263 | Val Loss: 0.0230\n",
            "Epoch 29/100 | Train Loss: 0.0259 | Val Loss: 0.0237\n",
            "Epoch 30/100 | Train Loss: 0.0258 | Val Loss: 0.0241\n",
            "Epoch 31/100 | Train Loss: 0.0258 | Val Loss: 0.0246\n",
            "Epoch 32/100 | Train Loss: 0.0255 | Val Loss: 0.0245\n",
            "Epoch 33/100 | Train Loss: 0.0254 | Val Loss: 0.0239\n",
            "Epoch 34/100 | Train Loss: 0.0252 | Val Loss: 0.0237\n",
            "Epoch 35/100 | Train Loss: 0.0251 | Val Loss: 0.0234\n",
            "Epoch 36/100 | Train Loss: 0.0254 | Val Loss: 0.0227\n",
            "Epoch 37/100 | Train Loss: 0.0249 | Val Loss: 0.0223\n",
            "Epoch 38/100 | Train Loss: 0.0249 | Val Loss: 0.0223\n",
            "Epoch 39/100 | Train Loss: 0.0248 | Val Loss: 0.0221\n",
            "Epoch 40/100 | Train Loss: 0.0247 | Val Loss: 0.0216\n",
            "Epoch 41/100 | Train Loss: 0.0246 | Val Loss: 0.0213\n",
            "Epoch 42/100 | Train Loss: 0.0249 | Val Loss: 0.0217\n",
            "Epoch 43/100 | Train Loss: 0.0248 | Val Loss: 0.0203\n",
            "Epoch 44/100 | Train Loss: 0.0242 | Val Loss: 0.0208\n",
            "Epoch 45/100 | Train Loss: 0.0241 | Val Loss: 0.0205\n",
            "Epoch 46/100 | Train Loss: 0.0240 | Val Loss: 0.0207\n",
            "Epoch 47/100 | Train Loss: 0.0238 | Val Loss: 0.0203\n",
            "Epoch 48/100 | Train Loss: 0.0237 | Val Loss: 0.0200\n",
            "Epoch 49/100 | Train Loss: 0.0236 | Val Loss: 0.0199\n",
            "Epoch 50/100 | Train Loss: 0.0234 | Val Loss: 0.0201\n",
            "Epoch 51/100 | Train Loss: 0.0234 | Val Loss: 0.0199\n",
            "Epoch 52/100 | Train Loss: 0.0232 | Val Loss: 0.0200\n",
            "Epoch 53/100 | Train Loss: 0.0234 | Val Loss: 0.0201\n",
            "Epoch 54/100 | Train Loss: 0.0230 | Val Loss: 0.0205\n",
            "Epoch 55/100 | Train Loss: 0.0228 | Val Loss: 0.0204\n",
            "Epoch 56/100 | Train Loss: 0.0227 | Val Loss: 0.0203\n",
            "Epoch 57/100 | Train Loss: 0.0226 | Val Loss: 0.0205\n",
            "Epoch 58/100 | Train Loss: 0.0228 | Val Loss: 0.0206\n",
            "Epoch 59/100 | Train Loss: 0.0224 | Val Loss: 0.0201\n",
            "Epoch 60/100 | Train Loss: 0.0224 | Val Loss: 0.0208\n",
            "Epoch 61/100 | Train Loss: 0.0223 | Val Loss: 0.0208\n",
            "Epoch 62/100 | Train Loss: 0.0222 | Val Loss: 0.0209\n",
            "Epoch 63/100 | Train Loss: 0.0221 | Val Loss: 0.0209\n",
            "Epoch 64/100 | Train Loss: 0.0220 | Val Loss: 0.0210\n",
            "Epoch 65/100 | Train Loss: 0.0219 | Val Loss: 0.0211\n",
            "Epoch 66/100 | Train Loss: 0.0221 | Val Loss: 0.0211\n",
            "Epoch 67/100 | Train Loss: 0.0219 | Val Loss: 0.0209\n",
            "Epoch 68/100 | Train Loss: 0.0218 | Val Loss: 0.0211\n",
            "Epoch 69/100 | Train Loss: 0.0216 | Val Loss: 0.0209\n",
            "Epoch 70/100 | Train Loss: 0.0215 | Val Loss: 0.0206\n",
            "Epoch 71/100 | Train Loss: 0.0214 | Val Loss: 0.0203\n",
            "Epoch 72/100 | Train Loss: 0.0214 | Val Loss: 0.0212\n",
            "Epoch 73/100 | Train Loss: 0.0213 | Val Loss: 0.0216\n",
            "Epoch 74/100 | Train Loss: 0.0212 | Val Loss: 0.0210\n",
            "Epoch 75/100 | Train Loss: 0.0213 | Val Loss: 0.0210\n",
            "Epoch 76/100 | Train Loss: 0.0211 | Val Loss: 0.0212\n",
            "Epoch 77/100 | Train Loss: 0.0211 | Val Loss: 0.0212\n",
            "Epoch 78/100 | Train Loss: 0.0210 | Val Loss: 0.0222\n",
            "Epoch 79/100 | Train Loss: 0.0212 | Val Loss: 0.0208\n",
            "Epoch 80/100 | Train Loss: 0.0209 | Val Loss: 0.0215\n",
            "Epoch 81/100 | Train Loss: 0.0210 | Val Loss: 0.0228\n",
            "Epoch 82/100 | Train Loss: 0.0208 | Val Loss: 0.0210\n",
            "Epoch 83/100 | Train Loss: 0.0209 | Val Loss: 0.0204\n",
            "Epoch 84/100 | Train Loss: 0.0208 | Val Loss: 0.0204\n",
            "Epoch 85/100 | Train Loss: 0.0204 | Val Loss: 0.0203\n",
            "Epoch 86/100 | Train Loss: 0.0204 | Val Loss: 0.0199\n",
            "Epoch 87/100 | Train Loss: 0.0203 | Val Loss: 0.0202\n",
            "Epoch 88/100 | Train Loss: 0.0203 | Val Loss: 0.0199\n",
            "Epoch 89/100 | Train Loss: 0.0207 | Val Loss: 0.0210\n",
            "Epoch 90/100 | Train Loss: 0.0201 | Val Loss: 0.0209\n",
            "Epoch 91/100 | Train Loss: 0.0201 | Val Loss: 0.0218\n",
            "Epoch 92/100 | Train Loss: 0.0201 | Val Loss: 0.0213\n",
            "Epoch 93/100 | Train Loss: 0.0207 | Val Loss: 0.0202\n",
            "Epoch 94/100 | Train Loss: 0.0204 | Val Loss: 0.0203\n",
            "Epoch 95/100 | Train Loss: 0.0202 | Val Loss: 0.0195\n",
            "Epoch 96/100 | Train Loss: 0.0201 | Val Loss: 0.0192\n",
            "Epoch 97/100 | Train Loss: 0.0197 | Val Loss: 0.0207\n",
            "Epoch 98/100 | Train Loss: 0.0201 | Val Loss: 0.0205\n",
            "Epoch 99/100 | Train Loss: 0.0198 | Val Loss: 0.0201\n",
            "Epoch 100/100 | Train Loss: 0.0196 | Val Loss: 0.0199\n",
            "Saved best model as best_lstm_dam_lstm2_05_weights.pt\n"
          ]
        }
      ],
      "source": [
        "dam_model_2_05 = LSTMResidualModel(target_len = 96, num_layers = 2, dropout=0.05)\n",
        "dam_lstm2_05_weights = train_model(dam_model_2_05, dam_train_loader, dam_val_loader, device = device, model_instance_name = \"dam_lstm2_05_weights\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-05-07T05:18:43.912420Z",
          "iopub.status.busy": "2025-05-07T05:18:43.911850Z"
        },
        "id": "eZeNqJM_RfbR",
        "outputId": "fb43476e-2f25-48a8-cde2-c74b3ee6574f",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100 | Train Loss: 0.1191 | Val Loss: 0.0686\n",
            "Epoch 2/100 | Train Loss: 0.0822 | Val Loss: 0.0521\n",
            "Epoch 3/100 | Train Loss: 0.0648 | Val Loss: 0.0459\n",
            "Epoch 4/100 | Train Loss: 0.0601 | Val Loss: 0.0426\n",
            "Epoch 5/100 | Train Loss: 0.0525 | Val Loss: 0.0407\n",
            "Epoch 6/100 | Train Loss: 0.0471 | Val Loss: 0.0338\n",
            "Epoch 7/100 | Train Loss: 0.0441 | Val Loss: 0.0339\n",
            "Epoch 8/100 | Train Loss: 0.0405 | Val Loss: 0.0327\n",
            "Epoch 9/100 | Train Loss: 0.0382 | Val Loss: 0.0326\n",
            "Epoch 10/100 | Train Loss: 0.0366 | Val Loss: 0.0457\n",
            "Epoch 11/100 | Train Loss: 0.0388 | Val Loss: 0.0317\n",
            "Epoch 12/100 | Train Loss: 0.0362 | Val Loss: 0.0292\n",
            "Epoch 13/100 | Train Loss: 0.0332 | Val Loss: 0.0289\n",
            "Epoch 14/100 | Train Loss: 0.0326 | Val Loss: 0.0292\n",
            "Epoch 15/100 | Train Loss: 0.0319 | Val Loss: 0.0283\n",
            "Epoch 16/100 | Train Loss: 0.0311 | Val Loss: 0.0274\n",
            "Epoch 17/100 | Train Loss: 0.0307 | Val Loss: 0.0272\n",
            "Epoch 18/100 | Train Loss: 0.0304 | Val Loss: 0.0258\n",
            "Epoch 19/100 | Train Loss: 0.0301 | Val Loss: 0.0262\n",
            "Epoch 20/100 | Train Loss: 0.0313 | Val Loss: 0.0261\n",
            "Epoch 21/100 | Train Loss: 0.0298 | Val Loss: 0.0268\n",
            "Epoch 22/100 | Train Loss: 0.0289 | Val Loss: 0.0275\n",
            "Epoch 23/100 | Train Loss: 0.0289 | Val Loss: 0.0272\n",
            "Epoch 24/100 | Train Loss: 0.0283 | Val Loss: 0.0272\n",
            "Epoch 25/100 | Train Loss: 0.0280 | Val Loss: 0.0261\n",
            "Epoch 26/100 | Train Loss: 0.0289 | Val Loss: 0.0267\n",
            "Epoch 27/100 | Train Loss: 0.0454 | Val Loss: 0.0268\n",
            "Epoch 28/100 | Train Loss: 0.0359 | Val Loss: 0.0250\n",
            "Epoch 29/100 | Train Loss: 0.0387 | Val Loss: 0.0459\n",
            "Epoch 30/100 | Train Loss: 0.0491 | Val Loss: 0.0330\n",
            "Epoch 31/100 | Train Loss: 0.0423 | Val Loss: 0.0311\n",
            "Epoch 32/100 | Train Loss: 0.0400 | Val Loss: 0.0300\n",
            "Epoch 33/100 | Train Loss: 0.0375 | Val Loss: 0.0290\n",
            "Epoch 34/100 | Train Loss: 0.0344 | Val Loss: 0.0279\n",
            "Epoch 35/100 | Train Loss: 0.0322 | Val Loss: 0.0271\n",
            "Epoch 36/100 | Train Loss: 0.0317 | Val Loss: 0.0269\n",
            "Epoch 37/100 | Train Loss: 0.0307 | Val Loss: 0.0268\n",
            "Epoch 38/100 | Train Loss: 0.0303 | Val Loss: 0.0263\n",
            "Epoch 39/100 | Train Loss: 0.0296 | Val Loss: 0.0261\n",
            "Epoch 40/100 | Train Loss: 0.0291 | Val Loss: 0.0258\n",
            "Epoch 41/100 | Train Loss: 0.0288 | Val Loss: 0.0256\n",
            "Epoch 42/100 | Train Loss: 0.0283 | Val Loss: 0.0257\n",
            "Epoch 43/100 | Train Loss: 0.0284 | Val Loss: 0.0250\n",
            "Epoch 44/100 | Train Loss: 0.0277 | Val Loss: 0.0249\n",
            "Epoch 45/100 | Train Loss: 0.0274 | Val Loss: 0.0247\n",
            "Epoch 46/100 | Train Loss: 0.0274 | Val Loss: 0.0245\n",
            "Epoch 47/100 | Train Loss: 0.0271 | Val Loss: 0.0242\n",
            "Epoch 48/100 | Train Loss: 0.0270 | Val Loss: 0.0242\n",
            "Epoch 49/100 | Train Loss: 0.0270 | Val Loss: 0.0242\n",
            "Epoch 50/100 | Train Loss: 0.0270 | Val Loss: 0.0230\n",
            "Epoch 51/100 | Train Loss: 0.0261 | Val Loss: 0.0227\n",
            "Epoch 52/100 | Train Loss: 0.0257 | Val Loss: 0.0227\n",
            "Epoch 53/100 | Train Loss: 0.0257 | Val Loss: 0.0227\n",
            "Epoch 54/100 | Train Loss: 0.0256 | Val Loss: 0.0230\n",
            "Epoch 55/100 | Train Loss: 0.0256 | Val Loss: 0.0220\n",
            "Epoch 56/100 | Train Loss: 0.0251 | Val Loss: 0.0215\n",
            "Epoch 57/100 | Train Loss: 0.0249 | Val Loss: 0.0219\n",
            "Epoch 58/100 | Train Loss: 0.0252 | Val Loss: 0.0214\n",
            "Epoch 59/100 | Train Loss: 0.0252 | Val Loss: 0.0216\n",
            "Epoch 60/100 | Train Loss: 0.0247 | Val Loss: 0.0209\n",
            "Epoch 61/100 | Train Loss: 0.0243 | Val Loss: 0.0213\n",
            "Epoch 62/100 | Train Loss: 0.0242 | Val Loss: 0.0208\n",
            "Epoch 63/100 | Train Loss: 0.0241 | Val Loss: 0.0210\n",
            "Epoch 64/100 | Train Loss: 0.0240 | Val Loss: 0.0208\n",
            "Epoch 65/100 | Train Loss: 0.0239 | Val Loss: 0.0207\n",
            "Epoch 66/100 | Train Loss: 0.0242 | Val Loss: 0.0209\n",
            "Epoch 67/100 | Train Loss: 0.0239 | Val Loss: 0.0203\n",
            "Epoch 68/100 | Train Loss: 0.0236 | Val Loss: 0.0202\n",
            "Epoch 69/100 | Train Loss: 0.0237 | Val Loss: 0.0206\n",
            "Epoch 70/100 | Train Loss: 0.0237 | Val Loss: 0.0202\n",
            "Epoch 71/100 | Train Loss: 0.0233 | Val Loss: 0.0204\n",
            "Epoch 72/100 | Train Loss: 0.0232 | Val Loss: 0.0208\n",
            "Epoch 73/100 | Train Loss: 0.0231 | Val Loss: 0.0205\n",
            "Epoch 74/100 | Train Loss: 0.0232 | Val Loss: 0.0204\n",
            "Epoch 75/100 | Train Loss: 0.0228 | Val Loss: 0.0202\n",
            "Epoch 76/100 | Train Loss: 0.0231 | Val Loss: 0.0202\n",
            "Epoch 77/100 | Train Loss: 0.0231 | Val Loss: 0.0203\n",
            "Epoch 78/100 | Train Loss: 0.0228 | Val Loss: 0.0204\n",
            "Epoch 79/100 | Train Loss: 0.0226 | Val Loss: 0.0199\n",
            "Epoch 80/100 | Train Loss: 0.0230 | Val Loss: 0.0198\n",
            "Epoch 81/100 | Train Loss: 0.0224 | Val Loss: 0.0208\n",
            "Epoch 82/100 | Train Loss: 0.0227 | Val Loss: 0.0213\n",
            "Epoch 83/100 | Train Loss: 0.0221 | Val Loss: 0.0212\n",
            "Epoch 84/100 | Train Loss: 0.0218 | Val Loss: 0.0209\n",
            "Epoch 85/100 | Train Loss: 0.0219 | Val Loss: 0.0215\n",
            "Epoch 86/100 | Train Loss: 0.0221 | Val Loss: 0.0210\n",
            "Epoch 87/100 | Train Loss: 0.0217 | Val Loss: 0.0212\n",
            "Epoch 88/100 | Train Loss: 0.0215 | Val Loss: 0.0217\n",
            "Epoch 89/100 | Train Loss: 0.0213 | Val Loss: 0.0223\n",
            "Epoch 90/100 | Train Loss: 0.0212 | Val Loss: 0.0218\n",
            "Epoch 91/100 | Train Loss: 0.0211 | Val Loss: 0.0217\n",
            "Epoch 92/100 | Train Loss: 0.0215 | Val Loss: 0.0212\n",
            "Epoch 93/100 | Train Loss: 0.0212 | Val Loss: 0.0211\n",
            "Epoch 94/100 | Train Loss: 0.0211 | Val Loss: 0.0215\n",
            "Epoch 95/100 | Train Loss: 0.0209 | Val Loss: 0.0221\n",
            "Epoch 96/100 | Train Loss: 0.0211 | Val Loss: 0.0222\n",
            "Epoch 97/100 | Train Loss: 0.0207 | Val Loss: 0.0207\n",
            "Epoch 98/100 | Train Loss: 0.0207 | Val Loss: 0.0207\n",
            "Epoch 99/100 | Train Loss: 0.0207 | Val Loss: 0.0214\n",
            "Epoch 100/100 | Train Loss: 0.0208 | Val Loss: 0.0204\n",
            "Saved best model as best_lstm_dam_lstm3_weights.pt\n"
          ]
        }
      ],
      "source": [
        "# num_layers = 3\n",
        "dam_model_3 = LSTMResidualModel(target_len = 96, num_layers = 3)\n",
        "dam_lstm3_weights = train_model(dam_model_3, dam_train_loader, dam_val_loader, device = device, model_instance_name = \"dam_lstm3_weights\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lf6gRMA7SFS_",
        "outputId": "e5c67b33-8670-4c37-8c15-084a2c2fde95"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100 | Train Loss: 0.1213 | Val Loss: 0.0695\n",
            "Epoch 2/100 | Train Loss: 0.0886 | Val Loss: 0.0611\n",
            "Epoch 3/100 | Train Loss: 0.0816 | Val Loss: 0.0565\n",
            "Epoch 4/100 | Train Loss: 0.0772 | Val Loss: 0.0540\n",
            "Epoch 5/100 | Train Loss: 0.0746 | Val Loss: 0.0525\n",
            "Epoch 6/100 | Train Loss: 0.0729 | Val Loss: 0.0512\n",
            "Epoch 7/100 | Train Loss: 0.0719 | Val Loss: 0.0506\n",
            "Epoch 8/100 | Train Loss: 0.0713 | Val Loss: 0.0504\n",
            "Epoch 9/100 | Train Loss: 0.0710 | Val Loss: 0.0502\n",
            "Epoch 10/100 | Train Loss: 0.0708 | Val Loss: 0.0496\n",
            "Epoch 11/100 | Train Loss: 0.0706 | Val Loss: 0.0495\n",
            "Epoch 12/100 | Train Loss: 0.0705 | Val Loss: 0.0495\n",
            "Epoch 13/100 | Train Loss: 0.0705 | Val Loss: 0.0495\n",
            "Epoch 14/100 | Train Loss: 0.0704 | Val Loss: 0.0495\n",
            "Epoch 15/100 | Train Loss: 0.0704 | Val Loss: 0.0495\n",
            "Epoch 16/100 | Train Loss: 0.0704 | Val Loss: 0.0495\n",
            "Epoch 17/100 | Train Loss: 0.0704 | Val Loss: 0.0494\n",
            "Epoch 18/100 | Train Loss: 0.0704 | Val Loss: 0.0492\n",
            "Epoch 19/100 | Train Loss: 0.0703 | Val Loss: 0.0492\n",
            "Epoch 20/100 | Train Loss: 0.0703 | Val Loss: 0.0492\n",
            "Epoch 21/100 | Train Loss: 0.0703 | Val Loss: 0.0492\n",
            "Epoch 22/100 | Train Loss: 0.0703 | Val Loss: 0.0492\n",
            "Epoch 23/100 | Train Loss: 0.0703 | Val Loss: 0.0492\n",
            "Epoch 24/100 | Train Loss: 0.0703 | Val Loss: 0.0492\n",
            "Epoch 25/100 | Train Loss: 0.0703 | Val Loss: 0.0492\n",
            "Epoch 26/100 | Train Loss: 0.0703 | Val Loss: 0.0492\n",
            "Epoch 27/100 | Train Loss: 0.0703 | Val Loss: 0.0492\n",
            "Epoch 28/100 | Train Loss: 0.0703 | Val Loss: 0.0491\n",
            "Epoch 29/100 | Train Loss: 0.0702 | Val Loss: 0.0489\n",
            "Epoch 30/100 | Train Loss: 0.0702 | Val Loss: 0.0489\n",
            "Epoch 31/100 | Train Loss: 0.0701 | Val Loss: 0.0489\n",
            "Epoch 32/100 | Train Loss: 0.0701 | Val Loss: 0.0489\n",
            "Epoch 33/100 | Train Loss: 0.0701 | Val Loss: 0.0489\n",
            "Epoch 34/100 | Train Loss: 0.0701 | Val Loss: 0.0489\n",
            "Epoch 35/100 | Train Loss: 0.0701 | Val Loss: 0.0489\n",
            "Epoch 36/100 | Train Loss: 0.0701 | Val Loss: 0.0489\n",
            "Epoch 37/100 | Train Loss: 0.0701 | Val Loss: 0.0489\n",
            "Epoch 38/100 | Train Loss: 0.0701 | Val Loss: 0.0489\n",
            "Epoch 39/100 | Train Loss: 0.0701 | Val Loss: 0.0489\n",
            "Epoch 40/100 | Train Loss: 0.0701 | Val Loss: 0.0489\n",
            "Epoch 41/100 | Train Loss: 0.0701 | Val Loss: 0.0489\n",
            "Epoch 42/100 | Train Loss: 0.0701 | Val Loss: 0.0489\n",
            "Epoch 43/100 | Train Loss: 0.0701 | Val Loss: 0.0489\n",
            "Epoch 44/100 | Train Loss: 0.0701 | Val Loss: 0.0489\n",
            "Epoch 45/100 | Train Loss: 0.0701 | Val Loss: 0.0489\n",
            "Epoch 46/100 | Train Loss: 0.0701 | Val Loss: 0.0489\n",
            "Epoch 47/100 | Train Loss: 0.0701 | Val Loss: 0.0489\n",
            "Epoch 48/100 | Train Loss: 0.0701 | Val Loss: 0.0489\n",
            "Epoch 49/100 | Train Loss: 0.0701 | Val Loss: 0.0489\n",
            "Epoch 50/100 | Train Loss: 0.0701 | Val Loss: 0.0489\n",
            "Epoch 51/100 | Train Loss: 0.0701 | Val Loss: 0.0489\n",
            "Epoch 52/100 | Train Loss: 0.0701 | Val Loss: 0.0489\n",
            "Epoch 53/100 | Train Loss: 0.0701 | Val Loss: 0.0489\n",
            "Epoch 54/100 | Train Loss: 0.0701 | Val Loss: 0.0489\n",
            "Epoch 55/100 | Train Loss: 0.0701 | Val Loss: 0.0489\n",
            "Epoch 56/100 | Train Loss: 0.0701 | Val Loss: 0.0489\n",
            "Epoch 57/100 | Train Loss: 0.0701 | Val Loss: 0.0489\n",
            "Epoch 58/100 | Train Loss: 0.0701 | Val Loss: 0.0489\n",
            "Epoch 59/100 | Train Loss: 0.0701 | Val Loss: 0.0489\n",
            "Epoch 60/100 | Train Loss: 0.0701 | Val Loss: 0.0489\n",
            "Epoch 61/100 | Train Loss: 0.0701 | Val Loss: 0.0489\n",
            "Epoch 62/100 | Train Loss: 0.0701 | Val Loss: 0.0489\n",
            "Epoch 63/100 | Train Loss: 0.0701 | Val Loss: 0.0489\n",
            "Epoch 64/100 | Train Loss: 0.0701 | Val Loss: 0.0489\n",
            "Epoch 65/100 | Train Loss: 0.0701 | Val Loss: 0.0489\n",
            "Epoch 66/100 | Train Loss: 0.0701 | Val Loss: 0.0489\n",
            "Epoch 67/100 | Train Loss: 0.0701 | Val Loss: 0.0489\n",
            "Epoch 68/100 | Train Loss: 0.0701 | Val Loss: 0.0489\n",
            "Epoch 69/100 | Train Loss: 0.0701 | Val Loss: 0.0489\n",
            "Epoch 70/100 | Train Loss: 0.0701 | Val Loss: 0.0489\n",
            "Epoch 71/100 | Train Loss: 0.0701 | Val Loss: 0.0489\n",
            "Epoch 72/100 | Train Loss: 0.0701 | Val Loss: 0.0489\n",
            "Epoch 73/100 | Train Loss: 0.0701 | Val Loss: 0.0489\n",
            "Epoch 74/100 | Train Loss: 0.0701 | Val Loss: 0.0489\n",
            "Epoch 75/100 | Train Loss: 0.0701 | Val Loss: 0.0489\n",
            "Epoch 76/100 | Train Loss: 0.0701 | Val Loss: 0.0489\n",
            "Epoch 77/100 | Train Loss: 0.0701 | Val Loss: 0.0489\n",
            "Epoch 78/100 | Train Loss: 0.0701 | Val Loss: 0.0489\n",
            "Epoch 79/100 | Train Loss: 0.0701 | Val Loss: 0.0489\n",
            "Epoch 80/100 | Train Loss: 0.0701 | Val Loss: 0.0489\n",
            "Epoch 81/100 | Train Loss: 0.0701 | Val Loss: 0.0489\n",
            "Epoch 82/100 | Train Loss: 0.0701 | Val Loss: 0.0489\n",
            "Epoch 83/100 | Train Loss: 0.0701 | Val Loss: 0.0489\n",
            "Epoch 84/100 | Train Loss: 0.0701 | Val Loss: 0.0489\n",
            "Epoch 85/100 | Train Loss: 0.0701 | Val Loss: 0.0489\n",
            "Epoch 86/100 | Train Loss: 0.0701 | Val Loss: 0.0489\n",
            "Epoch 87/100 | Train Loss: 0.0701 | Val Loss: 0.0489\n",
            "Epoch 88/100 | Train Loss: 0.0701 | Val Loss: 0.0489\n",
            "Epoch 89/100 | Train Loss: 0.0701 | Val Loss: 0.0489\n",
            "Epoch 90/100 | Train Loss: 0.0701 | Val Loss: 0.0489\n",
            "Epoch 91/100 | Train Loss: 0.0701 | Val Loss: 0.0489\n",
            "Epoch 92/100 | Train Loss: 0.0701 | Val Loss: 0.0489\n",
            "Epoch 93/100 | Train Loss: 0.0701 | Val Loss: 0.0489\n",
            "Epoch 94/100 | Train Loss: 0.0701 | Val Loss: 0.0489\n",
            "Epoch 95/100 | Train Loss: 0.0701 | Val Loss: 0.0489\n",
            "Epoch 96/100 | Train Loss: 0.0701 | Val Loss: 0.0489\n",
            "Epoch 97/100 | Train Loss: 0.0701 | Val Loss: 0.0489\n",
            "Epoch 98/100 | Train Loss: 0.0701 | Val Loss: 0.0489\n",
            "Epoch 99/100 | Train Loss: 0.0701 | Val Loss: 0.0489\n",
            "Epoch 100/100 | Train Loss: 0.0701 | Val Loss: 0.0489\n",
            "Saved best model as best_lstm_dam_lstm4_weights.pt\n"
          ]
        }
      ],
      "source": [
        "# num_layers = 4\n",
        "dam_model_4 = LSTMResidualModel(target_len = 96, num_layers = 4)\n",
        "dam_lstm4_weights = train_model(dam_model_4, dam_train_loader, dam_val_loader, device = device, model_instance_name = \"dam_lstm4_weights\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXdjRX-NJvtf"
      },
      "source": [
        "1 LSTM is showing better results than two LSTMs in series."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLOIzNdlRfbS"
      },
      "source": [
        "Validation Loss is higher than training loss might feel counterintutive but this is mainly due to drop connection (with probability = drop rate)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0DAXxXvG4iN"
      },
      "source": [
        "## RTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHHKArQLG790"
      },
      "source": [
        "RTM market is opened atmost 7 timesteps (6 for odd timestep, 7 for even timestep) before the delivery of transacted power. An image from CERC illustrates this:\n",
        "![RTM Schedule](https://etn.news/images/easyblog_articles/54/b2ap3_large_gate-closure.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAAv1WBNIFYu"
      },
      "source": [
        "We are training the model to predict price based on last 24 hours prices from auction start, that is, t - 15 min, t - 20 min so on where t is the time where auction for t + 7 delivery. RTMDataset is designed for this setup."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-05-07T03:27:24.419630Z",
          "iopub.status.idle": "2025-05-07T03:27:24.420003Z",
          "shell.execute_reply": "2025-05-07T03:27:24.419808Z"
        },
        "id": "8k1BIQgwRfbS",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class RTMDataset(Dataset):\n",
        "    def __init__(self, df, seq_len=96, fit_scaler=False, scaler=None):\n",
        "        \"\"\"\n",
        "            df: DataFrame containing at least a 'MCP (Rs/MWh)' column.\n",
        "            seq_len: number of past time steps used for input.\n",
        "            fit_scaler: if True, fit the MinMaxScaler to the current df.\n",
        "            scaler: optional externally fitted scaler to apply. They are applied to reduce overfitting\n",
        "        \"\"\"\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.seq_len = seq_len\n",
        "        self.scaler = scaler or MinMaxScaler()\n",
        "\n",
        "        if fit_scaler:     # if True then MinMaxScaler() will be used\n",
        "            self.scaled_mcp = self.scaler.fit_transform(self.df[['MCP (Rs/MWh)']])\n",
        "        else:\n",
        "            if self.scaler is None:\n",
        "                raise ValueError(\"Must provide a fitted scaler if fit_scaler is False.\")\n",
        "            self.scaled_mcp = self.scaler.transform(self.df[['MCP (Rs/MWh)']]) # inputed scaler is used\n",
        "\n",
        "        self.scaled_mcp = self.scaled_mcp.astype(np.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.scaled_mcp) - self.seq_len - 7\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        # last 96 time steps\n",
        "        x = self.scaled_mcp[idx:idx + self.seq_len]\n",
        "\n",
        "        # RTM transactions are applied after atmost 105 minutes or 7 x 15 minutes\n",
        "        # or 7 timesteps from the begining of auction for a time slot\n",
        "        y = self.scaled_mcp[idx + self.seq_len + 7]\n",
        "\n",
        "        return torch.tensor(x, dtype = torch.float32), torch.tensor(y, dtype=torch.float32)\n",
        "\n",
        "    def inverse_transform(self, scaled_array): # MinMaxScaled to normal values\n",
        "        if isinstance(scaled_array, torch.Tensor):\n",
        "            scaled_array = scaled_array.detach().cpu().numpy()\n",
        "        scaled_array = np.array(scaled_array).reshape(-1, 1)\n",
        "        return self.scaler.inverse_transform(scaled_array).flatten()\n",
        "\n",
        "    def get_scaler(self): # Same scaler has to be used while validating and testing etc.\n",
        "        return self.scaler\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-05-07T03:27:24.420958Z",
          "iopub.status.idle": "2025-05-07T03:27:24.421327Z",
          "shell.execute_reply": "2025-05-07T03:27:24.421189Z"
        },
        "id": "rijWLOIuRfbS",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "rtm_train_dataset = RTMDataset(rtm1_train, fit_scaler=True)  # MinMaxScaler is fitted\n",
        "\n",
        "rtm_scaler = rtm_train_dataset.get_scaler()\n",
        "\n",
        "rtm_val_dataset = RTMDataset(rtm1_valid, scaler=rtm_scaler)\n",
        "\n",
        "rtm_train_loader = DataLoader(rtm_train_dataset, batch_size=64, shuffle=False)\n",
        "rtm_val_loader = DataLoader(rtm_val_dataset, batch_size=64, shuffle = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0hBGtIyG9467",
        "outputId": "bf46a6a7-fd9b-4767-ca29-4c584586ac4d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100 | Train Loss: 0.0950 | Val Loss: 0.0308\n",
            "Epoch 2/100 | Train Loss: 0.0468 | Val Loss: 0.0261\n",
            "Epoch 3/100 | Train Loss: 0.0422 | Val Loss: 0.0247\n",
            "Epoch 4/100 | Train Loss: 0.0400 | Val Loss: 0.0241\n",
            "Epoch 5/100 | Train Loss: 0.0389 | Val Loss: 0.0252\n",
            "Epoch 6/100 | Train Loss: 0.0384 | Val Loss: 0.0246\n",
            "Epoch 7/100 | Train Loss: 0.0380 | Val Loss: 0.0245\n",
            "Epoch 8/100 | Train Loss: 0.0372 | Val Loss: 0.0248\n",
            "Epoch 9/100 | Train Loss: 0.0366 | Val Loss: 0.0248\n",
            "Epoch 10/100 | Train Loss: 0.0368 | Val Loss: 0.0213\n",
            "Epoch 11/100 | Train Loss: 0.0291 | Val Loss: 0.0179\n",
            "Epoch 12/100 | Train Loss: 0.0265 | Val Loss: 0.0175\n",
            "Epoch 13/100 | Train Loss: 0.0262 | Val Loss: 0.0177\n",
            "Epoch 14/100 | Train Loss: 0.0261 | Val Loss: 0.0166\n",
            "Epoch 15/100 | Train Loss: 0.0255 | Val Loss: 0.0157\n",
            "Epoch 16/100 | Train Loss: 0.0247 | Val Loss: 0.0168\n",
            "Epoch 17/100 | Train Loss: 0.0253 | Val Loss: 0.0191\n",
            "Epoch 18/100 | Train Loss: 0.0242 | Val Loss: 0.0160\n",
            "Epoch 19/100 | Train Loss: 0.0244 | Val Loss: 0.0153\n",
            "Epoch 20/100 | Train Loss: 0.0237 | Val Loss: 0.0162\n",
            "Epoch 21/100 | Train Loss: 0.0230 | Val Loss: 0.0147\n",
            "Epoch 22/100 | Train Loss: 0.0242 | Val Loss: 0.0147\n",
            "Epoch 23/100 | Train Loss: 0.0228 | Val Loss: 0.0170\n",
            "Epoch 24/100 | Train Loss: 0.0225 | Val Loss: 0.0149\n",
            "Epoch 25/100 | Train Loss: 0.0227 | Val Loss: 0.0171\n",
            "Epoch 26/100 | Train Loss: 0.0226 | Val Loss: 0.0159\n",
            "Epoch 27/100 | Train Loss: 0.0222 | Val Loss: 0.0149\n",
            "Epoch 28/100 | Train Loss: 0.0220 | Val Loss: 0.0179\n",
            "Epoch 29/100 | Train Loss: 0.0250 | Val Loss: 0.0163\n",
            "Epoch 30/100 | Train Loss: 0.0221 | Val Loss: 0.0156\n",
            "Epoch 31/100 | Train Loss: 0.0219 | Val Loss: 0.0163\n",
            "Epoch 32/100 | Train Loss: 0.0215 | Val Loss: 0.0170\n",
            "Epoch 33/100 | Train Loss: 0.0214 | Val Loss: 0.0151\n",
            "Epoch 34/100 | Train Loss: 0.0215 | Val Loss: 0.0149\n",
            "Epoch 35/100 | Train Loss: 0.0213 | Val Loss: 0.0143\n",
            "Epoch 36/100 | Train Loss: 0.0213 | Val Loss: 0.0144\n",
            "Epoch 37/100 | Train Loss: 0.0213 | Val Loss: 0.0140\n",
            "Epoch 38/100 | Train Loss: 0.0209 | Val Loss: 0.0138\n",
            "Epoch 39/100 | Train Loss: 0.0212 | Val Loss: 0.0143\n",
            "Epoch 40/100 | Train Loss: 0.0205 | Val Loss: 0.0149\n",
            "Epoch 41/100 | Train Loss: 0.0206 | Val Loss: 0.0139\n",
            "Epoch 42/100 | Train Loss: 0.0203 | Val Loss: 0.0147\n",
            "Epoch 43/100 | Train Loss: 0.0202 | Val Loss: 0.0138\n",
            "Epoch 44/100 | Train Loss: 0.0202 | Val Loss: 0.0147\n",
            "Epoch 45/100 | Train Loss: 0.0201 | Val Loss: 0.0142\n",
            "Epoch 46/100 | Train Loss: 0.0202 | Val Loss: 0.0147\n",
            "Epoch 47/100 | Train Loss: 0.0208 | Val Loss: 0.0171\n",
            "Epoch 48/100 | Train Loss: 0.0198 | Val Loss: 0.0137\n",
            "Epoch 49/100 | Train Loss: 0.0200 | Val Loss: 0.0142\n",
            "Epoch 50/100 | Train Loss: 0.0201 | Val Loss: 0.0142\n",
            "Epoch 51/100 | Train Loss: 0.0196 | Val Loss: 0.0134\n",
            "Epoch 52/100 | Train Loss: 0.0200 | Val Loss: 0.0145\n",
            "Epoch 53/100 | Train Loss: 0.0198 | Val Loss: 0.0146\n",
            "Epoch 54/100 | Train Loss: 0.0198 | Val Loss: 0.0153\n",
            "Epoch 55/100 | Train Loss: 0.0199 | Val Loss: 0.0162\n",
            "Epoch 56/100 | Train Loss: 0.0203 | Val Loss: 0.0145\n",
            "Epoch 57/100 | Train Loss: 0.0194 | Val Loss: 0.0142\n",
            "Epoch 58/100 | Train Loss: 0.0195 | Val Loss: 0.0138\n",
            "Epoch 59/100 | Train Loss: 0.0197 | Val Loss: 0.0142\n",
            "Epoch 60/100 | Train Loss: 0.0203 | Val Loss: 0.0148\n",
            "Epoch 61/100 | Train Loss: 0.0199 | Val Loss: 0.0154\n",
            "Epoch 62/100 | Train Loss: 0.0194 | Val Loss: 0.0142\n",
            "Epoch 63/100 | Train Loss: 0.0195 | Val Loss: 0.0144\n",
            "Epoch 64/100 | Train Loss: 0.0197 | Val Loss: 0.0146\n",
            "Epoch 65/100 | Train Loss: 0.0192 | Val Loss: 0.0144\n",
            "Epoch 66/100 | Train Loss: 0.0196 | Val Loss: 0.0143\n",
            "Epoch 67/100 | Train Loss: 0.0191 | Val Loss: 0.0143\n",
            "Epoch 68/100 | Train Loss: 0.0191 | Val Loss: 0.0145\n",
            "Epoch 69/100 | Train Loss: 0.0191 | Val Loss: 0.0149\n",
            "Epoch 70/100 | Train Loss: 0.0195 | Val Loss: 0.0146\n",
            "Epoch 71/100 | Train Loss: 0.0189 | Val Loss: 0.0149\n",
            "Epoch 72/100 | Train Loss: 0.0189 | Val Loss: 0.0143\n",
            "Epoch 73/100 | Train Loss: 0.0192 | Val Loss: 0.0157\n",
            "Epoch 74/100 | Train Loss: 0.0193 | Val Loss: 0.0145\n",
            "Epoch 75/100 | Train Loss: 0.0190 | Val Loss: 0.0141\n",
            "Epoch 76/100 | Train Loss: 0.0191 | Val Loss: 0.0141\n",
            "Epoch 77/100 | Train Loss: 0.0191 | Val Loss: 0.0201\n",
            "Epoch 78/100 | Train Loss: 0.0190 | Val Loss: 0.0139\n",
            "Epoch 79/100 | Train Loss: 0.0187 | Val Loss: 0.0141\n",
            "Epoch 80/100 | Train Loss: 0.0189 | Val Loss: 0.0173\n",
            "Epoch 81/100 | Train Loss: 0.0191 | Val Loss: 0.0152\n",
            "Epoch 82/100 | Train Loss: 0.0190 | Val Loss: 0.0150\n",
            "Epoch 83/100 | Train Loss: 0.0186 | Val Loss: 0.0152\n",
            "Epoch 84/100 | Train Loss: 0.0186 | Val Loss: 0.0151\n",
            "Epoch 85/100 | Train Loss: 0.0185 | Val Loss: 0.0149\n",
            "Epoch 86/100 | Train Loss: 0.0185 | Val Loss: 0.0149\n",
            "Epoch 87/100 | Train Loss: 0.0193 | Val Loss: 0.0151\n",
            "Epoch 88/100 | Train Loss: 0.0190 | Val Loss: 0.0155\n",
            "Epoch 89/100 | Train Loss: 0.0190 | Val Loss: 0.0149\n",
            "Epoch 90/100 | Train Loss: 0.0180 | Val Loss: 0.0143\n",
            "Epoch 91/100 | Train Loss: 0.0183 | Val Loss: 0.0145\n",
            "Epoch 92/100 | Train Loss: 0.0180 | Val Loss: 0.0157\n",
            "Epoch 93/100 | Train Loss: 0.0184 | Val Loss: 0.0143\n",
            "Epoch 94/100 | Train Loss: 0.0184 | Val Loss: 0.0158\n",
            "Epoch 95/100 | Train Loss: 0.0183 | Val Loss: 0.0152\n",
            "Epoch 96/100 | Train Loss: 0.0183 | Val Loss: 0.0162\n",
            "Epoch 97/100 | Train Loss: 0.0377 | Val Loss: 0.0245\n",
            "Epoch 98/100 | Train Loss: 0.0354 | Val Loss: 0.0245\n",
            "Epoch 99/100 | Train Loss: 0.0361 | Val Loss: 0.0253\n",
            "Epoch 100/100 | Train Loss: 0.0307 | Val Loss: 0.0155\n",
            "Saved best model as best_lstm_rtm_lstm1_weights.pt\n"
          ]
        }
      ],
      "source": [
        "rtm_model_1 = LSTMResidualModel(num_layers=1, dropout=0.3, target_len = 1)\n",
        "rtm_lstm1_weights = train_model(rtm_model_1, rtm_train_loader, rtm_val_loader, device = device, model_instance_name = \"rtm_lstm1_weights\", target_len = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i2ParEwpVgAi",
        "outputId": "1189ea1c-5325-4cae-8c0f-6cabb3ee4cd6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100 | Train Loss: 0.1443 | Val Loss: 0.0300\n",
            "Epoch 2/100 | Train Loss: 0.0502 | Val Loss: 0.0265\n",
            "Epoch 3/100 | Train Loss: 0.0435 | Val Loss: 0.0249\n",
            "Epoch 4/100 | Train Loss: 0.0412 | Val Loss: 0.0245\n",
            "Epoch 5/100 | Train Loss: 0.0401 | Val Loss: 0.0242\n",
            "Epoch 6/100 | Train Loss: 0.0391 | Val Loss: 0.0241\n",
            "Epoch 7/100 | Train Loss: 0.0387 | Val Loss: 0.0238\n",
            "Epoch 8/100 | Train Loss: 0.0382 | Val Loss: 0.0239\n",
            "Epoch 9/100 | Train Loss: 0.0378 | Val Loss: 0.0236\n",
            "Epoch 10/100 | Train Loss: 0.0373 | Val Loss: 0.0251\n",
            "Epoch 11/100 | Train Loss: 0.0377 | Val Loss: 0.0246\n",
            "Epoch 12/100 | Train Loss: 0.0371 | Val Loss: 0.0258\n",
            "Epoch 13/100 | Train Loss: 0.0380 | Val Loss: 0.0255\n",
            "Epoch 14/100 | Train Loss: 0.0356 | Val Loss: 0.0247\n",
            "Epoch 15/100 | Train Loss: 0.0370 | Val Loss: 0.0263\n",
            "Epoch 16/100 | Train Loss: 0.0366 | Val Loss: 0.0256\n",
            "Epoch 17/100 | Train Loss: 0.0393 | Val Loss: 0.0252\n",
            "Epoch 18/100 | Train Loss: 0.0364 | Val Loss: 0.0239\n",
            "Epoch 19/100 | Train Loss: 0.0368 | Val Loss: 0.0280\n",
            "Epoch 20/100 | Train Loss: 0.0370 | Val Loss: 0.0262\n",
            "Epoch 21/100 | Train Loss: 0.0365 | Val Loss: 0.0274\n",
            "Epoch 22/100 | Train Loss: 0.0367 | Val Loss: 0.0252\n",
            "Epoch 23/100 | Train Loss: 0.0353 | Val Loss: 0.0258\n",
            "Epoch 24/100 | Train Loss: 0.0358 | Val Loss: 0.0233\n",
            "Epoch 25/100 | Train Loss: 0.0355 | Val Loss: 0.0214\n",
            "Epoch 26/100 | Train Loss: 0.0346 | Val Loss: 0.0204\n",
            "Epoch 27/100 | Train Loss: 0.0356 | Val Loss: 0.0252\n",
            "Epoch 28/100 | Train Loss: 0.0326 | Val Loss: 0.0203\n",
            "Epoch 29/100 | Train Loss: 0.0381 | Val Loss: 0.0232\n",
            "Epoch 30/100 | Train Loss: 0.0372 | Val Loss: 0.0233\n",
            "Epoch 31/100 | Train Loss: 0.0370 | Val Loss: 0.0233\n",
            "Epoch 32/100 | Train Loss: 0.0366 | Val Loss: 0.0239\n",
            "Epoch 33/100 | Train Loss: 0.0356 | Val Loss: 0.0244\n",
            "Epoch 34/100 | Train Loss: 0.0354 | Val Loss: 0.0244\n",
            "Epoch 35/100 | Train Loss: 0.0353 | Val Loss: 0.0243\n",
            "Epoch 36/100 | Train Loss: 0.0351 | Val Loss: 0.0246\n",
            "Epoch 37/100 | Train Loss: 0.0356 | Val Loss: 0.0258\n",
            "Epoch 38/100 | Train Loss: 0.0348 | Val Loss: 0.0238\n",
            "Epoch 39/100 | Train Loss: 0.0336 | Val Loss: 0.0233\n",
            "Epoch 40/100 | Train Loss: 0.0324 | Val Loss: 0.0233\n",
            "Epoch 41/100 | Train Loss: 0.0339 | Val Loss: 0.0195\n",
            "Epoch 42/100 | Train Loss: 0.0269 | Val Loss: 0.0184\n",
            "Epoch 43/100 | Train Loss: 0.0261 | Val Loss: 0.0167\n",
            "Epoch 44/100 | Train Loss: 0.0246 | Val Loss: 0.0165\n",
            "Epoch 45/100 | Train Loss: 0.0229 | Val Loss: 0.0159\n",
            "Epoch 46/100 | Train Loss: 0.0225 | Val Loss: 0.0147\n",
            "Epoch 47/100 | Train Loss: 0.0217 | Val Loss: 0.0142\n",
            "Epoch 48/100 | Train Loss: 0.0218 | Val Loss: 0.0148\n",
            "Epoch 49/100 | Train Loss: 0.0211 | Val Loss: 0.0143\n",
            "Epoch 50/100 | Train Loss: 0.0207 | Val Loss: 0.0144\n",
            "Epoch 51/100 | Train Loss: 0.0209 | Val Loss: 0.0150\n",
            "Epoch 52/100 | Train Loss: 0.0202 | Val Loss: 0.0150\n",
            "Epoch 53/100 | Train Loss: 0.0204 | Val Loss: 0.0151\n",
            "Epoch 54/100 | Train Loss: 0.0200 | Val Loss: 0.0155\n",
            "Epoch 55/100 | Train Loss: 0.0203 | Val Loss: 0.0146\n",
            "Epoch 56/100 | Train Loss: 0.0198 | Val Loss: 0.0167\n",
            "Epoch 57/100 | Train Loss: 0.0197 | Val Loss: 0.0145\n",
            "Epoch 58/100 | Train Loss: 0.0200 | Val Loss: 0.0145\n",
            "Epoch 59/100 | Train Loss: 0.0198 | Val Loss: 0.0146\n",
            "Epoch 60/100 | Train Loss: 0.0196 | Val Loss: 0.0137\n",
            "Epoch 61/100 | Train Loss: 0.0196 | Val Loss: 0.0144\n",
            "Epoch 62/100 | Train Loss: 0.0193 | Val Loss: 0.0136\n",
            "Epoch 63/100 | Train Loss: 0.0193 | Val Loss: 0.0143\n",
            "Epoch 64/100 | Train Loss: 0.0198 | Val Loss: 0.0145\n",
            "Epoch 65/100 | Train Loss: 0.0192 | Val Loss: 0.0136\n",
            "Epoch 66/100 | Train Loss: 0.0198 | Val Loss: 0.0136\n",
            "Epoch 67/100 | Train Loss: 0.0194 | Val Loss: 0.0132\n",
            "Epoch 68/100 | Train Loss: 0.0196 | Val Loss: 0.0130\n",
            "Epoch 69/100 | Train Loss: 0.0191 | Val Loss: 0.0133\n",
            "Epoch 70/100 | Train Loss: 0.0192 | Val Loss: 0.0137\n",
            "Epoch 71/100 | Train Loss: 0.0193 | Val Loss: 0.0130\n",
            "Epoch 72/100 | Train Loss: 0.0190 | Val Loss: 0.0130\n",
            "Epoch 73/100 | Train Loss: 0.0195 | Val Loss: 0.0125\n",
            "Epoch 74/100 | Train Loss: 0.0194 | Val Loss: 0.0153\n",
            "Epoch 75/100 | Train Loss: 0.0188 | Val Loss: 0.0125\n",
            "Epoch 76/100 | Train Loss: 0.0201 | Val Loss: 0.0130\n",
            "Epoch 77/100 | Train Loss: 0.0189 | Val Loss: 0.0129\n",
            "Epoch 78/100 | Train Loss: 0.0187 | Val Loss: 0.0128\n",
            "Epoch 79/100 | Train Loss: 0.0193 | Val Loss: 0.0131\n",
            "Epoch 80/100 | Train Loss: 0.0190 | Val Loss: 0.0130\n",
            "Epoch 81/100 | Train Loss: 0.0183 | Val Loss: 0.0125\n",
            "Epoch 82/100 | Train Loss: 0.0184 | Val Loss: 0.0124\n",
            "Epoch 83/100 | Train Loss: 0.0185 | Val Loss: 0.0127\n",
            "Epoch 84/100 | Train Loss: 0.0184 | Val Loss: 0.0123\n",
            "Epoch 85/100 | Train Loss: 0.0184 | Val Loss: 0.0127\n",
            "Epoch 86/100 | Train Loss: 0.0187 | Val Loss: 0.0129\n",
            "Epoch 87/100 | Train Loss: 0.0187 | Val Loss: 0.0131\n",
            "Epoch 88/100 | Train Loss: 0.0185 | Val Loss: 0.0131\n",
            "Epoch 89/100 | Train Loss: 0.0183 | Val Loss: 0.0123\n",
            "Epoch 90/100 | Train Loss: 0.0183 | Val Loss: 0.0123\n",
            "Epoch 91/100 | Train Loss: 0.0180 | Val Loss: 0.0124\n",
            "Epoch 92/100 | Train Loss: 0.0181 | Val Loss: 0.0121\n",
            "Epoch 93/100 | Train Loss: 0.0180 | Val Loss: 0.0129\n",
            "Epoch 94/100 | Train Loss: 0.0180 | Val Loss: 0.0128\n",
            "Epoch 95/100 | Train Loss: 0.0182 | Val Loss: 0.0125\n",
            "Epoch 96/100 | Train Loss: 0.0179 | Val Loss: 0.0140\n",
            "Epoch 97/100 | Train Loss: 0.0181 | Val Loss: 0.0130\n",
            "Epoch 98/100 | Train Loss: 0.0185 | Val Loss: 0.0128\n",
            "Epoch 99/100 | Train Loss: 0.0173 | Val Loss: 0.0124\n",
            "Epoch 100/100 | Train Loss: 0.0179 | Val Loss: 0.0126\n",
            "Saved best model as best_lstm_rtm_lstm2_weights.pt\n"
          ]
        }
      ],
      "source": [
        "rtm_model_2 = LSTMResidualModel(num_layers=2, dropout=0.3, target_len = 1)\n",
        "rtm_lstm2_weights = train_model(rtm_model_2, rtm_train_loader, rtm_val_loader, device = device, model_instance_name = \"rtm_lstm2_weights\", target_len = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.status.busy": "2025-05-07T03:27:24.422025Z",
          "iopub.status.idle": "2025-05-07T03:27:24.422399Z",
          "shell.execute_reply": "2025-05-07T03:27:24.422268Z"
        },
        "id": "A4nXhEO0RfbS",
        "outputId": "7c2cfd31-0f6e-4aed-b4a8-08a55cb88662",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100 | Train Loss: 0.0587 | Val Loss: 0.0248\n",
            "Epoch 2/100 | Train Loss: 0.0414 | Val Loss: 0.0251\n",
            "Epoch 3/100 | Train Loss: 0.0401 | Val Loss: 0.0245\n",
            "Epoch 4/100 | Train Loss: 0.0390 | Val Loss: 0.0241\n",
            "Epoch 5/100 | Train Loss: 0.0389 | Val Loss: 0.0277\n",
            "Epoch 6/100 | Train Loss: 0.0462 | Val Loss: 0.0249\n",
            "Epoch 7/100 | Train Loss: 0.0427 | Val Loss: 0.0242\n",
            "Epoch 8/100 | Train Loss: 0.0423 | Val Loss: 0.0238\n",
            "Epoch 9/100 | Train Loss: 0.0406 | Val Loss: 0.0235\n",
            "Epoch 10/100 | Train Loss: 0.0405 | Val Loss: 0.0235\n",
            "Epoch 11/100 | Train Loss: 0.0404 | Val Loss: 0.0235\n",
            "Epoch 12/100 | Train Loss: 0.0405 | Val Loss: 0.0234\n",
            "Epoch 13/100 | Train Loss: 0.0394 | Val Loss: 0.0225\n",
            "Epoch 14/100 | Train Loss: 0.0386 | Val Loss: 0.0224\n",
            "Epoch 15/100 | Train Loss: 0.0384 | Val Loss: 0.0225\n",
            "Epoch 16/100 | Train Loss: 0.0384 | Val Loss: 0.0225\n",
            "Epoch 17/100 | Train Loss: 0.0382 | Val Loss: 0.0226\n",
            "Epoch 18/100 | Train Loss: 0.0385 | Val Loss: 0.0226\n",
            "Epoch 19/100 | Train Loss: 0.0384 | Val Loss: 0.0225\n",
            "Epoch 20/100 | Train Loss: 0.0384 | Val Loss: 0.0225\n",
            "Epoch 21/100 | Train Loss: 0.0384 | Val Loss: 0.0225\n",
            "Epoch 22/100 | Train Loss: 0.0384 | Val Loss: 0.0225\n",
            "Epoch 23/100 | Train Loss: 0.0384 | Val Loss: 0.0225\n",
            "Epoch 24/100 | Train Loss: 0.0384 | Val Loss: 0.0226\n",
            "Epoch 25/100 | Train Loss: 0.0385 | Val Loss: 0.0225\n",
            "Epoch 26/100 | Train Loss: 0.0385 | Val Loss: 0.0225\n",
            "Epoch 27/100 | Train Loss: 0.0385 | Val Loss: 0.0226\n",
            "Epoch 28/100 | Train Loss: 0.0385 | Val Loss: 0.0225\n",
            "Epoch 29/100 | Train Loss: 0.0385 | Val Loss: 0.0225\n",
            "Epoch 30/100 | Train Loss: 0.0385 | Val Loss: 0.0225\n",
            "Epoch 31/100 | Train Loss: 0.0385 | Val Loss: 0.0225\n",
            "Epoch 32/100 | Train Loss: 0.0385 | Val Loss: 0.0225\n",
            "Epoch 33/100 | Train Loss: 0.0385 | Val Loss: 0.0225\n",
            "Epoch 34/100 | Train Loss: 0.0385 | Val Loss: 0.0225\n",
            "Epoch 35/100 | Train Loss: 0.0385 | Val Loss: 0.0225\n",
            "Epoch 36/100 | Train Loss: 0.0385 | Val Loss: 0.0225\n",
            "Epoch 37/100 | Train Loss: 0.0385 | Val Loss: 0.0225\n",
            "Epoch 38/100 | Train Loss: 0.0385 | Val Loss: 0.0225\n",
            "Epoch 39/100 | Train Loss: 0.0385 | Val Loss: 0.0225\n",
            "Epoch 40/100 | Train Loss: 0.0385 | Val Loss: 0.0225\n",
            "Epoch 41/100 | Train Loss: 0.0385 | Val Loss: 0.0225\n",
            "Epoch 42/100 | Train Loss: 0.0385 | Val Loss: 0.0225\n",
            "Epoch 43/100 | Train Loss: 0.0385 | Val Loss: 0.0225\n",
            "Epoch 44/100 | Train Loss: 0.0385 | Val Loss: 0.0225\n",
            "Epoch 45/100 | Train Loss: 0.0385 | Val Loss: 0.0225\n",
            "Epoch 46/100 | Train Loss: 0.0385 | Val Loss: 0.0225\n",
            "Epoch 47/100 | Train Loss: 0.0385 | Val Loss: 0.0225\n",
            "Epoch 48/100 | Train Loss: 0.0385 | Val Loss: 0.0225\n",
            "Epoch 49/100 | Train Loss: 0.0385 | Val Loss: 0.0225\n",
            "Epoch 50/100 | Train Loss: 0.0385 | Val Loss: 0.0225\n",
            "Epoch 51/100 | Train Loss: 0.0385 | Val Loss: 0.0226\n",
            "Epoch 52/100 | Train Loss: 0.0385 | Val Loss: 0.0225\n",
            "Epoch 53/100 | Train Loss: 0.0385 | Val Loss: 0.0225\n",
            "Epoch 54/100 | Train Loss: 0.0385 | Val Loss: 0.0225\n",
            "Epoch 55/100 | Train Loss: 0.0385 | Val Loss: 0.0225\n",
            "Epoch 56/100 | Train Loss: 0.0385 | Val Loss: 0.0225\n",
            "Epoch 57/100 | Train Loss: 0.0385 | Val Loss: 0.0226\n",
            "Epoch 58/100 | Train Loss: 0.0385 | Val Loss: 0.0226\n",
            "Epoch 59/100 | Train Loss: 0.0385 | Val Loss: 0.0225\n",
            "Epoch 60/100 | Train Loss: 0.0385 | Val Loss: 0.0225\n",
            "Epoch 61/100 | Train Loss: 0.0385 | Val Loss: 0.0226\n",
            "Epoch 62/100 | Train Loss: 0.0384 | Val Loss: 0.0226\n",
            "Epoch 63/100 | Train Loss: 0.0385 | Val Loss: 0.0226\n",
            "Epoch 64/100 | Train Loss: 0.0385 | Val Loss: 0.0226\n",
            "Epoch 65/100 | Train Loss: 0.0385 | Val Loss: 0.0225\n",
            "Epoch 66/100 | Train Loss: 0.0385 | Val Loss: 0.0225\n",
            "Epoch 67/100 | Train Loss: 0.0384 | Val Loss: 0.0225\n",
            "Epoch 68/100 | Train Loss: 0.0384 | Val Loss: 0.0225\n",
            "Epoch 69/100 | Train Loss: 0.0384 | Val Loss: 0.0226\n",
            "Epoch 70/100 | Train Loss: 0.0384 | Val Loss: 0.0225\n",
            "Epoch 71/100 | Train Loss: 0.0384 | Val Loss: 0.0226\n",
            "Epoch 72/100 | Train Loss: 0.0384 | Val Loss: 0.0225\n",
            "Epoch 73/100 | Train Loss: 0.0384 | Val Loss: 0.0226\n",
            "Epoch 74/100 | Train Loss: 0.0384 | Val Loss: 0.0226\n",
            "Epoch 75/100 | Train Loss: 0.0384 | Val Loss: 0.0226\n",
            "Epoch 76/100 | Train Loss: 0.0384 | Val Loss: 0.0225\n",
            "Epoch 77/100 | Train Loss: 0.0384 | Val Loss: 0.0226\n",
            "Epoch 78/100 | Train Loss: 0.0384 | Val Loss: 0.0226\n",
            "Epoch 79/100 | Train Loss: 0.0384 | Val Loss: 0.0226\n",
            "Epoch 80/100 | Train Loss: 0.0384 | Val Loss: 0.0226\n",
            "Epoch 81/100 | Train Loss: 0.0384 | Val Loss: 0.0226\n",
            "Epoch 82/100 | Train Loss: 0.0384 | Val Loss: 0.0226\n",
            "Epoch 83/100 | Train Loss: 0.0384 | Val Loss: 0.0226\n",
            "Epoch 84/100 | Train Loss: 0.0383 | Val Loss: 0.0225\n",
            "Epoch 85/100 | Train Loss: 0.0383 | Val Loss: 0.0225\n",
            "Epoch 86/100 | Train Loss: 0.0383 | Val Loss: 0.0226\n",
            "Epoch 87/100 | Train Loss: 0.0383 | Val Loss: 0.0225\n",
            "Epoch 88/100 | Train Loss: 0.0383 | Val Loss: 0.0225\n",
            "Epoch 89/100 | Train Loss: 0.0383 | Val Loss: 0.0226\n",
            "Epoch 90/100 | Train Loss: 0.0383 | Val Loss: 0.0227\n",
            "Epoch 91/100 | Train Loss: 0.0383 | Val Loss: 0.0227\n",
            "Epoch 92/100 | Train Loss: 0.0383 | Val Loss: 0.0227\n",
            "Epoch 93/100 | Train Loss: 0.0383 | Val Loss: 0.0226\n",
            "Epoch 94/100 | Train Loss: 0.0383 | Val Loss: 0.0226\n",
            "Epoch 95/100 | Train Loss: 0.0383 | Val Loss: 0.0226\n",
            "Epoch 96/100 | Train Loss: 0.0383 | Val Loss: 0.0226\n",
            "Epoch 97/100 | Train Loss: 0.0383 | Val Loss: 0.0227\n",
            "Epoch 98/100 | Train Loss: 0.0383 | Val Loss: 0.0226\n",
            "Epoch 99/100 | Train Loss: 0.0383 | Val Loss: 0.0226\n",
            "Epoch 100/100 | Train Loss: 0.0383 | Val Loss: 0.0226\n",
            "Saved best model as best_lstm_rtm_lstm3_weights.pt\n"
          ]
        }
      ],
      "source": [
        "rtm_model_3 = LSTMResidualModel(num_layers=3, dropout=0.3, target_len = 1, lr = 0.01)\n",
        "rtm_lstm3_weights = train_model(rtm_model_3, rtm_train_loader, rtm_val_loader, device = device, model_instance_name = \"rtm_lstm3_weights\", target_len = 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-L20TXc-mbj"
      },
      "source": [
        "2 layers gives the best generalization performance (lowest validation loss), while 3 layers lead to overfitting or optimization issues."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TLQJUQBs_82V"
      },
      "source": [
        "### RTM tommorow using yesterday's MCP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "4MRNKdseEt2r"
      },
      "outputs": [],
      "source": [
        "rtm_d_train_dataset = PriceDataset(rtm1_train, fit_scaler=True)  # MinMaxScaler is fitted\n",
        "\n",
        "rtm_d_scaler = rtm_d_train_dataset.get_scaler()\n",
        "\n",
        "rtm_d_val_dataset = PriceDataset(rtm1_valid, scaler=rtm_scaler)\n",
        "\n",
        "rtm_d_train_loader = DataLoader(rtm_d_train_dataset, batch_size=64, shuffle=False)\n",
        "rtm_d_val_loader = DataLoader(rtm_d_val_dataset, batch_size=64, shuffle = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Xac1-VnnsFH",
        "outputId": "5b49dabf-17fd-4dcc-dd52-cfb3f78eba13"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100 | Train Loss: 0.1122 | Val Loss: 0.0386\n",
            "Epoch 2/100 | Train Loss: 0.0679 | Val Loss: 0.0360\n",
            "Epoch 3/100 | Train Loss: 0.0649 | Val Loss: 0.0354\n",
            "Epoch 4/100 | Train Loss: 0.0629 | Val Loss: 0.0347\n",
            "Epoch 5/100 | Train Loss: 0.0610 | Val Loss: 0.0345\n",
            "Epoch 6/100 | Train Loss: 0.0591 | Val Loss: 0.0308\n",
            "Epoch 7/100 | Train Loss: 0.0535 | Val Loss: 0.0293\n",
            "Epoch 8/100 | Train Loss: 0.0506 | Val Loss: 0.0291\n",
            "Epoch 9/100 | Train Loss: 0.0489 | Val Loss: 0.0283\n",
            "Epoch 10/100 | Train Loss: 0.0476 | Val Loss: 0.0276\n",
            "Epoch 11/100 | Train Loss: 0.0464 | Val Loss: 0.0271\n",
            "Epoch 12/100 | Train Loss: 0.0451 | Val Loss: 0.0264\n",
            "Epoch 13/100 | Train Loss: 0.0437 | Val Loss: 0.0257\n",
            "Epoch 14/100 | Train Loss: 0.0419 | Val Loss: 0.0248\n",
            "Epoch 15/100 | Train Loss: 0.0405 | Val Loss: 0.0243\n",
            "Epoch 16/100 | Train Loss: 0.0398 | Val Loss: 0.0240\n",
            "Epoch 17/100 | Train Loss: 0.0391 | Val Loss: 0.0239\n",
            "Epoch 18/100 | Train Loss: 0.0386 | Val Loss: 0.0237\n",
            "Epoch 19/100 | Train Loss: 0.0381 | Val Loss: 0.0236\n",
            "Epoch 20/100 | Train Loss: 0.0377 | Val Loss: 0.0234\n",
            "Epoch 21/100 | Train Loss: 0.0374 | Val Loss: 0.0233\n",
            "Epoch 22/100 | Train Loss: 0.0371 | Val Loss: 0.0232\n",
            "Epoch 23/100 | Train Loss: 0.0369 | Val Loss: 0.0231\n",
            "Epoch 24/100 | Train Loss: 0.0367 | Val Loss: 0.0231\n",
            "Epoch 25/100 | Train Loss: 0.0365 | Val Loss: 0.0231\n",
            "Epoch 26/100 | Train Loss: 0.0364 | Val Loss: 0.0231\n",
            "Epoch 27/100 | Train Loss: 0.0362 | Val Loss: 0.0230\n",
            "Epoch 28/100 | Train Loss: 0.0361 | Val Loss: 0.0229\n",
            "Epoch 29/100 | Train Loss: 0.0360 | Val Loss: 0.0228\n",
            "Epoch 30/100 | Train Loss: 0.0358 | Val Loss: 0.0228\n",
            "Epoch 31/100 | Train Loss: 0.0357 | Val Loss: 0.0227\n",
            "Epoch 32/100 | Train Loss: 0.0356 | Val Loss: 0.0226\n",
            "Epoch 33/100 | Train Loss: 0.0355 | Val Loss: 0.0226\n",
            "Epoch 34/100 | Train Loss: 0.0355 | Val Loss: 0.0225\n",
            "Epoch 35/100 | Train Loss: 0.0354 | Val Loss: 0.0224\n",
            "Epoch 36/100 | Train Loss: 0.0353 | Val Loss: 0.0223\n",
            "Epoch 37/100 | Train Loss: 0.0353 | Val Loss: 0.0223\n",
            "Epoch 38/100 | Train Loss: 0.0352 | Val Loss: 0.0222\n",
            "Epoch 39/100 | Train Loss: 0.0352 | Val Loss: 0.0222\n",
            "Epoch 40/100 | Train Loss: 0.0351 | Val Loss: 0.0222\n",
            "Epoch 41/100 | Train Loss: 0.0351 | Val Loss: 0.0222\n",
            "Epoch 42/100 | Train Loss: 0.0351 | Val Loss: 0.0222\n",
            "Epoch 43/100 | Train Loss: 0.0350 | Val Loss: 0.0222\n",
            "Epoch 44/100 | Train Loss: 0.0350 | Val Loss: 0.0222\n",
            "Epoch 45/100 | Train Loss: 0.0350 | Val Loss: 0.0222\n",
            "Epoch 46/100 | Train Loss: 0.0350 | Val Loss: 0.0222\n",
            "Epoch 47/100 | Train Loss: 0.0349 | Val Loss: 0.0221\n",
            "Epoch 48/100 | Train Loss: 0.0349 | Val Loss: 0.0221\n",
            "Epoch 49/100 | Train Loss: 0.0348 | Val Loss: 0.0222\n",
            "Epoch 50/100 | Train Loss: 0.0349 | Val Loss: 0.0222\n",
            "Epoch 51/100 | Train Loss: 0.0348 | Val Loss: 0.0222\n",
            "Epoch 52/100 | Train Loss: 0.0348 | Val Loss: 0.0221\n",
            "Epoch 53/100 | Train Loss: 0.0347 | Val Loss: 0.0221\n",
            "Epoch 54/100 | Train Loss: 0.0346 | Val Loss: 0.0221\n",
            "Epoch 55/100 | Train Loss: 0.0344 | Val Loss: 0.0221\n",
            "Epoch 56/100 | Train Loss: 0.0343 | Val Loss: 0.0223\n",
            "Epoch 57/100 | Train Loss: 0.0344 | Val Loss: 0.0225\n",
            "Epoch 58/100 | Train Loss: 0.0343 | Val Loss: 0.0225\n",
            "Epoch 59/100 | Train Loss: 0.0342 | Val Loss: 0.0224\n",
            "Epoch 60/100 | Train Loss: 0.0342 | Val Loss: 0.0226\n",
            "Epoch 61/100 | Train Loss: 0.0343 | Val Loss: 0.0224\n",
            "Epoch 62/100 | Train Loss: 0.0339 | Val Loss: 0.0223\n",
            "Epoch 63/100 | Train Loss: 0.0339 | Val Loss: 0.0226\n",
            "Epoch 64/100 | Train Loss: 0.0343 | Val Loss: 0.0223\n",
            "Epoch 65/100 | Train Loss: 0.0339 | Val Loss: 0.0226\n",
            "Epoch 66/100 | Train Loss: 0.0340 | Val Loss: 0.0223\n",
            "Epoch 67/100 | Train Loss: 0.0338 | Val Loss: 0.0227\n",
            "Epoch 68/100 | Train Loss: 0.0336 | Val Loss: 0.0225\n",
            "Epoch 69/100 | Train Loss: 0.0336 | Val Loss: 0.0228\n",
            "Epoch 70/100 | Train Loss: 0.0334 | Val Loss: 0.0228\n",
            "Epoch 71/100 | Train Loss: 0.0333 | Val Loss: 0.0230\n",
            "Epoch 72/100 | Train Loss: 0.0333 | Val Loss: 0.0229\n",
            "Epoch 73/100 | Train Loss: 0.0332 | Val Loss: 0.0231\n",
            "Epoch 74/100 | Train Loss: 0.0333 | Val Loss: 0.0233\n",
            "Epoch 75/100 | Train Loss: 0.0332 | Val Loss: 0.0233\n",
            "Epoch 76/100 | Train Loss: 0.0330 | Val Loss: 0.0236\n",
            "Epoch 77/100 | Train Loss: 0.0330 | Val Loss: 0.0237\n",
            "Epoch 78/100 | Train Loss: 0.0330 | Val Loss: 0.0247\n",
            "Epoch 79/100 | Train Loss: 0.0328 | Val Loss: 0.0247\n",
            "Epoch 80/100 | Train Loss: 0.0327 | Val Loss: 0.0253\n",
            "Epoch 81/100 | Train Loss: 0.0324 | Val Loss: 0.0248\n",
            "Epoch 82/100 | Train Loss: 0.0326 | Val Loss: 0.0243\n",
            "Epoch 83/100 | Train Loss: 0.0323 | Val Loss: 0.0249\n",
            "Epoch 84/100 | Train Loss: 0.0322 | Val Loss: 0.0294\n",
            "Epoch 85/100 | Train Loss: 0.0323 | Val Loss: 0.0254\n",
            "Epoch 86/100 | Train Loss: 0.0319 | Val Loss: 0.0256\n",
            "Epoch 87/100 | Train Loss: 0.0320 | Val Loss: 0.0250\n",
            "Epoch 88/100 | Train Loss: 0.0318 | Val Loss: 0.0244\n",
            "Epoch 89/100 | Train Loss: 0.0317 | Val Loss: 0.0247\n",
            "Epoch 90/100 | Train Loss: 0.0315 | Val Loss: 0.0249\n",
            "Epoch 91/100 | Train Loss: 0.0317 | Val Loss: 0.0255\n",
            "Epoch 92/100 | Train Loss: 0.0320 | Val Loss: 0.0255\n",
            "Epoch 93/100 | Train Loss: 0.0315 | Val Loss: 0.0257\n",
            "Epoch 94/100 | Train Loss: 0.0314 | Val Loss: 0.0256\n",
            "Epoch 95/100 | Train Loss: 0.0316 | Val Loss: 0.0264\n",
            "Epoch 96/100 | Train Loss: 0.0312 | Val Loss: 0.0260\n",
            "Epoch 97/100 | Train Loss: 0.0313 | Val Loss: 0.0251\n",
            "Epoch 98/100 | Train Loss: 0.0315 | Val Loss: 0.0267\n",
            "Epoch 99/100 | Train Loss: 0.0315 | Val Loss: 0.0271\n",
            "Epoch 100/100 | Train Loss: 0.0314 | Val Loss: 0.0259\n",
            "Saved best model as best_lstm_rtm_d_lstm1_weights.pt\n"
          ]
        }
      ],
      "source": [
        "rtm_d_model_1 = LSTMResidualModel(num_layers=1, dropout=0.3, target_len = 96)\n",
        "rtm_d_lstm1_weights = train_model(rtm_d_model_1, rtm_d_train_loader, rtm_d_val_loader, device = device, model_instance_name = \"rtm_d_lstm1_weights\", target_len = 96)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P21d9zdBA3ap",
        "outputId": "b315536c-8367-40d7-ebb2-99bcd0be8de8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100 | Train Loss: 0.1051 | Val Loss: 0.0387\n",
            "Epoch 2/100 | Train Loss: 0.0682 | Val Loss: 0.0359\n",
            "Epoch 3/100 | Train Loss: 0.0646 | Val Loss: 0.0343\n",
            "Epoch 4/100 | Train Loss: 0.0626 | Val Loss: 0.0349\n",
            "Epoch 5/100 | Train Loss: 0.0601 | Val Loss: 0.0321\n",
            "Epoch 6/100 | Train Loss: 0.0572 | Val Loss: 0.0311\n",
            "Epoch 7/100 | Train Loss: 0.0535 | Val Loss: 0.0308\n",
            "Epoch 8/100 | Train Loss: 0.0511 | Val Loss: 0.0287\n",
            "Epoch 9/100 | Train Loss: 0.0494 | Val Loss: 0.0281\n",
            "Epoch 10/100 | Train Loss: 0.0483 | Val Loss: 0.0276\n",
            "Epoch 11/100 | Train Loss: 0.0473 | Val Loss: 0.0272\n",
            "Epoch 12/100 | Train Loss: 0.0460 | Val Loss: 0.0265\n",
            "Epoch 13/100 | Train Loss: 0.0437 | Val Loss: 0.0250\n",
            "Epoch 14/100 | Train Loss: 0.0417 | Val Loss: 0.0245\n",
            "Epoch 15/100 | Train Loss: 0.0406 | Val Loss: 0.0243\n",
            "Epoch 16/100 | Train Loss: 0.0397 | Val Loss: 0.0242\n",
            "Epoch 17/100 | Train Loss: 0.0387 | Val Loss: 0.0240\n",
            "Epoch 18/100 | Train Loss: 0.0381 | Val Loss: 0.0238\n",
            "Epoch 19/100 | Train Loss: 0.0376 | Val Loss: 0.0235\n",
            "Epoch 20/100 | Train Loss: 0.0374 | Val Loss: 0.0233\n",
            "Epoch 21/100 | Train Loss: 0.0371 | Val Loss: 0.0232\n",
            "Epoch 22/100 | Train Loss: 0.0369 | Val Loss: 0.0230\n",
            "Epoch 23/100 | Train Loss: 0.0367 | Val Loss: 0.0230\n",
            "Epoch 24/100 | Train Loss: 0.0365 | Val Loss: 0.0229\n",
            "Epoch 25/100 | Train Loss: 0.0363 | Val Loss: 0.0228\n",
            "Epoch 26/100 | Train Loss: 0.0362 | Val Loss: 0.0227\n",
            "Epoch 27/100 | Train Loss: 0.0361 | Val Loss: 0.0226\n",
            "Epoch 28/100 | Train Loss: 0.0360 | Val Loss: 0.0225\n",
            "Epoch 29/100 | Train Loss: 0.0359 | Val Loss: 0.0224\n",
            "Epoch 30/100 | Train Loss: 0.0358 | Val Loss: 0.0223\n",
            "Epoch 31/100 | Train Loss: 0.0357 | Val Loss: 0.0222\n",
            "Epoch 32/100 | Train Loss: 0.0356 | Val Loss: 0.0222\n",
            "Epoch 33/100 | Train Loss: 0.0355 | Val Loss: 0.0222\n",
            "Epoch 34/100 | Train Loss: 0.0354 | Val Loss: 0.0222\n",
            "Epoch 35/100 | Train Loss: 0.0353 | Val Loss: 0.0222\n",
            "Epoch 36/100 | Train Loss: 0.0352 | Val Loss: 0.0222\n",
            "Epoch 37/100 | Train Loss: 0.0354 | Val Loss: 0.0221\n",
            "Epoch 38/100 | Train Loss: 0.0353 | Val Loss: 0.0221\n",
            "Epoch 39/100 | Train Loss: 0.0353 | Val Loss: 0.0220\n",
            "Epoch 40/100 | Train Loss: 0.0355 | Val Loss: 0.0223\n",
            "Epoch 41/100 | Train Loss: 0.0351 | Val Loss: 0.0223\n",
            "Epoch 42/100 | Train Loss: 0.0352 | Val Loss: 0.0224\n",
            "Epoch 43/100 | Train Loss: 0.0347 | Val Loss: 0.0221\n",
            "Epoch 44/100 | Train Loss: 0.0348 | Val Loss: 0.0221\n",
            "Epoch 45/100 | Train Loss: 0.0347 | Val Loss: 0.0222\n",
            "Epoch 46/100 | Train Loss: 0.0349 | Val Loss: 0.0224\n",
            "Epoch 47/100 | Train Loss: 0.0347 | Val Loss: 0.0228\n",
            "Epoch 48/100 | Train Loss: 0.0348 | Val Loss: 0.0231\n",
            "Epoch 49/100 | Train Loss: 0.0346 | Val Loss: 0.0228\n",
            "Epoch 50/100 | Train Loss: 0.0349 | Val Loss: 0.0230\n",
            "Epoch 51/100 | Train Loss: 0.0346 | Val Loss: 0.0227\n",
            "Epoch 52/100 | Train Loss: 0.0346 | Val Loss: 0.0229\n",
            "Epoch 53/100 | Train Loss: 0.0349 | Val Loss: 0.0231\n",
            "Epoch 54/100 | Train Loss: 0.0344 | Val Loss: 0.0233\n",
            "Epoch 55/100 | Train Loss: 0.0347 | Val Loss: 0.0235\n",
            "Epoch 56/100 | Train Loss: 0.0344 | Val Loss: 0.0233\n",
            "Epoch 57/100 | Train Loss: 0.0344 | Val Loss: 0.0234\n",
            "Epoch 58/100 | Train Loss: 0.0347 | Val Loss: 0.0234\n",
            "Epoch 59/100 | Train Loss: 0.0344 | Val Loss: 0.0232\n",
            "Epoch 60/100 | Train Loss: 0.0346 | Val Loss: 0.0239\n",
            "Epoch 61/100 | Train Loss: 0.0342 | Val Loss: 0.0234\n",
            "Epoch 62/100 | Train Loss: 0.0340 | Val Loss: 0.0234\n",
            "Epoch 63/100 | Train Loss: 0.0339 | Val Loss: 0.0234\n",
            "Epoch 64/100 | Train Loss: 0.0338 | Val Loss: 0.0238\n",
            "Epoch 65/100 | Train Loss: 0.0337 | Val Loss: 0.0243\n",
            "Epoch 66/100 | Train Loss: 0.0336 | Val Loss: 0.0237\n",
            "Epoch 67/100 | Train Loss: 0.0335 | Val Loss: 0.0243\n",
            "Epoch 68/100 | Train Loss: 0.0334 | Val Loss: 0.0243\n",
            "Epoch 69/100 | Train Loss: 0.0334 | Val Loss: 0.0240\n",
            "Epoch 70/100 | Train Loss: 0.0334 | Val Loss: 0.0250\n",
            "Epoch 71/100 | Train Loss: 0.0331 | Val Loss: 0.0241\n",
            "Epoch 72/100 | Train Loss: 0.0331 | Val Loss: 0.0240\n",
            "Epoch 73/100 | Train Loss: 0.0328 | Val Loss: 0.0243\n",
            "Epoch 74/100 | Train Loss: 0.0329 | Val Loss: 0.0249\n",
            "Epoch 75/100 | Train Loss: 0.0328 | Val Loss: 0.0254\n",
            "Epoch 76/100 | Train Loss: 0.0327 | Val Loss: 0.0257\n",
            "Epoch 77/100 | Train Loss: 0.0327 | Val Loss: 0.0257\n",
            "Epoch 78/100 | Train Loss: 0.0325 | Val Loss: 0.0244\n",
            "Epoch 79/100 | Train Loss: 0.0324 | Val Loss: 0.0253\n",
            "Epoch 80/100 | Train Loss: 0.0322 | Val Loss: 0.0248\n",
            "Epoch 81/100 | Train Loss: 0.0322 | Val Loss: 0.0247\n",
            "Epoch 82/100 | Train Loss: 0.0321 | Val Loss: 0.0249\n",
            "Epoch 83/100 | Train Loss: 0.0321 | Val Loss: 0.0250\n",
            "Epoch 84/100 | Train Loss: 0.0322 | Val Loss: 0.0254\n",
            "Epoch 85/100 | Train Loss: 0.0325 | Val Loss: 0.0249\n",
            "Epoch 86/100 | Train Loss: 0.0323 | Val Loss: 0.0246\n",
            "Epoch 87/100 | Train Loss: 0.0320 | Val Loss: 0.0249\n",
            "Epoch 88/100 | Train Loss: 0.0319 | Val Loss: 0.0251\n",
            "Epoch 89/100 | Train Loss: 0.0318 | Val Loss: 0.0255\n",
            "Epoch 90/100 | Train Loss: 0.0316 | Val Loss: 0.0257\n",
            "Epoch 91/100 | Train Loss: 0.0316 | Val Loss: 0.0251\n",
            "Epoch 92/100 | Train Loss: 0.0313 | Val Loss: 0.0255\n",
            "Epoch 93/100 | Train Loss: 0.0312 | Val Loss: 0.0257\n",
            "Epoch 94/100 | Train Loss: 0.0314 | Val Loss: 0.0254\n",
            "Epoch 95/100 | Train Loss: 0.0309 | Val Loss: 0.0261\n",
            "Epoch 96/100 | Train Loss: 0.0312 | Val Loss: 0.0259\n",
            "Epoch 97/100 | Train Loss: 0.0313 | Val Loss: 0.0252\n",
            "Epoch 98/100 | Train Loss: 0.0309 | Val Loss: 0.0265\n",
            "Epoch 99/100 | Train Loss: 0.0311 | Val Loss: 0.0257\n",
            "Epoch 100/100 | Train Loss: 0.0313 | Val Loss: 0.0254\n",
            "Saved best model as best_lstm_rtm_d_lstm1_1_weights.pt\n"
          ]
        }
      ],
      "source": [
        "rtm_d_model_1_1 = LSTMResidualModel(num_layers=1, dropout=0.1, target_len = 96)\n",
        "rtm_d_lstm1_1_weights = train_model(rtm_d_model_1_1, rtm_d_train_loader, rtm_d_val_loader, device = device, model_instance_name = \"rtm_d_lstm1_1_weights\", target_len = 96)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EMQ5FgTMEnyA",
        "outputId": "b85e7798-623b-4f0e-b329-fd9a3489fe7a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100 | Train Loss: 0.1127 | Val Loss: 0.0398\n",
            "Epoch 2/100 | Train Loss: 0.0678 | Val Loss: 0.0365\n",
            "Epoch 3/100 | Train Loss: 0.0648 | Val Loss: 0.0350\n",
            "Epoch 4/100 | Train Loss: 0.0623 | Val Loss: 0.0338\n",
            "Epoch 5/100 | Train Loss: 0.0595 | Val Loss: 0.0324\n",
            "Epoch 6/100 | Train Loss: 0.0545 | Val Loss: 0.0308\n",
            "Epoch 7/100 | Train Loss: 0.0519 | Val Loss: 0.0298\n",
            "Epoch 8/100 | Train Loss: 0.0489 | Val Loss: 0.0283\n",
            "Epoch 9/100 | Train Loss: 0.0461 | Val Loss: 0.0276\n",
            "Epoch 10/100 | Train Loss: 0.0432 | Val Loss: 0.0263\n",
            "Epoch 11/100 | Train Loss: 0.0414 | Val Loss: 0.0254\n",
            "Epoch 12/100 | Train Loss: 0.0402 | Val Loss: 0.0253\n",
            "Epoch 13/100 | Train Loss: 0.0394 | Val Loss: 0.0249\n",
            "Epoch 14/100 | Train Loss: 0.0388 | Val Loss: 0.0250\n",
            "Epoch 15/100 | Train Loss: 0.0382 | Val Loss: 0.0253\n",
            "Epoch 16/100 | Train Loss: 0.0377 | Val Loss: 0.0252\n",
            "Epoch 17/100 | Train Loss: 0.0375 | Val Loss: 0.0254\n",
            "Epoch 18/100 | Train Loss: 0.0372 | Val Loss: 0.0252\n",
            "Epoch 19/100 | Train Loss: 0.0370 | Val Loss: 0.0252\n",
            "Epoch 20/100 | Train Loss: 0.0368 | Val Loss: 0.0252\n",
            "Epoch 21/100 | Train Loss: 0.0368 | Val Loss: 0.0256\n",
            "Epoch 22/100 | Train Loss: 0.0368 | Val Loss: 0.0261\n",
            "Epoch 23/100 | Train Loss: 0.0371 | Val Loss: 0.0250\n",
            "Epoch 24/100 | Train Loss: 0.0366 | Val Loss: 0.0247\n",
            "Epoch 25/100 | Train Loss: 0.0364 | Val Loss: 0.0241\n",
            "Epoch 26/100 | Train Loss: 0.0362 | Val Loss: 0.0236\n",
            "Epoch 27/100 | Train Loss: 0.0360 | Val Loss: 0.0235\n",
            "Epoch 28/100 | Train Loss: 0.0359 | Val Loss: 0.0240\n",
            "Epoch 29/100 | Train Loss: 0.0357 | Val Loss: 0.0241\n",
            "Epoch 30/100 | Train Loss: 0.0359 | Val Loss: 0.0245\n",
            "Epoch 31/100 | Train Loss: 0.0357 | Val Loss: 0.0238\n",
            "Epoch 32/100 | Train Loss: 0.0354 | Val Loss: 0.0235\n",
            "Epoch 33/100 | Train Loss: 0.0354 | Val Loss: 0.0234\n",
            "Epoch 34/100 | Train Loss: 0.0352 | Val Loss: 0.0235\n",
            "Epoch 35/100 | Train Loss: 0.0352 | Val Loss: 0.0236\n",
            "Epoch 36/100 | Train Loss: 0.0350 | Val Loss: 0.0234\n",
            "Epoch 37/100 | Train Loss: 0.0348 | Val Loss: 0.0234\n",
            "Epoch 38/100 | Train Loss: 0.0348 | Val Loss: 0.0233\n",
            "Epoch 39/100 | Train Loss: 0.0348 | Val Loss: 0.0230\n",
            "Epoch 40/100 | Train Loss: 0.0346 | Val Loss: 0.0232\n",
            "Epoch 41/100 | Train Loss: 0.0347 | Val Loss: 0.0234\n",
            "Epoch 42/100 | Train Loss: 0.0346 | Val Loss: 0.0234\n",
            "Epoch 43/100 | Train Loss: 0.0344 | Val Loss: 0.0236\n",
            "Epoch 44/100 | Train Loss: 0.0344 | Val Loss: 0.0242\n",
            "Epoch 45/100 | Train Loss: 0.0340 | Val Loss: 0.0239\n",
            "Epoch 46/100 | Train Loss: 0.0345 | Val Loss: 0.0238\n",
            "Epoch 47/100 | Train Loss: 0.0348 | Val Loss: 0.0241\n",
            "Epoch 48/100 | Train Loss: 0.0341 | Val Loss: 0.0241\n",
            "Epoch 49/100 | Train Loss: 0.0340 | Val Loss: 0.0239\n",
            "Epoch 50/100 | Train Loss: 0.0339 | Val Loss: 0.0236\n",
            "Epoch 51/100 | Train Loss: 0.0338 | Val Loss: 0.0237\n",
            "Epoch 52/100 | Train Loss: 0.0338 | Val Loss: 0.0235\n",
            "Epoch 53/100 | Train Loss: 0.0338 | Val Loss: 0.0235\n",
            "Epoch 54/100 | Train Loss: 0.0335 | Val Loss: 0.0235\n",
            "Epoch 55/100 | Train Loss: 0.0331 | Val Loss: 0.0242\n",
            "Epoch 56/100 | Train Loss: 0.0330 | Val Loss: 0.0242\n",
            "Epoch 57/100 | Train Loss: 0.0330 | Val Loss: 0.0250\n",
            "Epoch 58/100 | Train Loss: 0.0330 | Val Loss: 0.0241\n",
            "Epoch 59/100 | Train Loss: 0.0327 | Val Loss: 0.0246\n",
            "Epoch 60/100 | Train Loss: 0.0326 | Val Loss: 0.0244\n",
            "Epoch 61/100 | Train Loss: 0.0324 | Val Loss: 0.0248\n",
            "Epoch 62/100 | Train Loss: 0.0325 | Val Loss: 0.0252\n",
            "Epoch 63/100 | Train Loss: 0.0324 | Val Loss: 0.0256\n",
            "Epoch 64/100 | Train Loss: 0.0325 | Val Loss: 0.0244\n",
            "Epoch 65/100 | Train Loss: 0.0322 | Val Loss: 0.0248\n",
            "Epoch 66/100 | Train Loss: 0.0318 | Val Loss: 0.0250\n",
            "Epoch 67/100 | Train Loss: 0.0318 | Val Loss: 0.0251\n",
            "Epoch 68/100 | Train Loss: 0.0317 | Val Loss: 0.0254\n",
            "Epoch 69/100 | Train Loss: 0.0315 | Val Loss: 0.0257\n",
            "Epoch 70/100 | Train Loss: 0.0315 | Val Loss: 0.0251\n",
            "Epoch 71/100 | Train Loss: 0.0313 | Val Loss: 0.0266\n",
            "Epoch 72/100 | Train Loss: 0.0312 | Val Loss: 0.0257\n",
            "Epoch 73/100 | Train Loss: 0.0311 | Val Loss: 0.0280\n",
            "Epoch 74/100 | Train Loss: 0.0321 | Val Loss: 0.0262\n",
            "Epoch 75/100 | Train Loss: 0.0310 | Val Loss: 0.0262\n",
            "Epoch 76/100 | Train Loss: 0.0305 | Val Loss: 0.0271\n",
            "Epoch 77/100 | Train Loss: 0.0305 | Val Loss: 0.0248\n",
            "Epoch 78/100 | Train Loss: 0.0318 | Val Loss: 0.0265\n",
            "Epoch 79/100 | Train Loss: 0.0300 | Val Loss: 0.0259\n",
            "Epoch 80/100 | Train Loss: 0.0303 | Val Loss: 0.0273\n",
            "Epoch 81/100 | Train Loss: 0.0301 | Val Loss: 0.0264\n",
            "Epoch 82/100 | Train Loss: 0.0297 | Val Loss: 0.0263\n",
            "Epoch 83/100 | Train Loss: 0.0298 | Val Loss: 0.0276\n",
            "Epoch 84/100 | Train Loss: 0.0297 | Val Loss: 0.0264\n",
            "Epoch 85/100 | Train Loss: 0.0294 | Val Loss: 0.0260\n",
            "Epoch 86/100 | Train Loss: 0.0298 | Val Loss: 0.0260\n",
            "Epoch 87/100 | Train Loss: 0.0286 | Val Loss: 0.0265\n",
            "Epoch 88/100 | Train Loss: 0.0285 | Val Loss: 0.0275\n",
            "Epoch 89/100 | Train Loss: 0.0285 | Val Loss: 0.0262\n",
            "Epoch 90/100 | Train Loss: 0.0283 | Val Loss: 0.0273\n",
            "Epoch 91/100 | Train Loss: 0.0280 | Val Loss: 0.0272\n",
            "Epoch 92/100 | Train Loss: 0.0276 | Val Loss: 0.0270\n",
            "Epoch 93/100 | Train Loss: 0.0274 | Val Loss: 0.0290\n",
            "Epoch 94/100 | Train Loss: 0.0270 | Val Loss: 0.0282\n",
            "Epoch 95/100 | Train Loss: 0.0267 | Val Loss: 0.0289\n",
            "Epoch 96/100 | Train Loss: 0.0263 | Val Loss: 0.0293\n",
            "Epoch 97/100 | Train Loss: 0.0265 | Val Loss: 0.0289\n",
            "Epoch 98/100 | Train Loss: 0.0263 | Val Loss: 0.0298\n",
            "Epoch 99/100 | Train Loss: 0.0261 | Val Loss: 0.0291\n",
            "Epoch 100/100 | Train Loss: 0.0256 | Val Loss: 0.0303\n",
            "Saved best model as best_lstm_rtm_d_lstm2_weights.pt\n"
          ]
        }
      ],
      "source": [
        "rtm_d_model_2 = LSTMResidualModel(num_layers=2, dropout=0.3, target_len = 96)\n",
        "rtm_d_lstm2_weights = train_model(rtm_d_model_2, rtm_d_train_loader, rtm_d_val_loader, device = device, model_instance_name = \"rtm_d_lstm2_weights\", target_len = 96)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-oqoyFK1NJrR",
        "outputId": "1c6eab76-0298-42c2-ffcf-b513bd852403"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100 | Train Loss: 0.1049 | Val Loss: 0.0462\n",
            "Epoch 2/100 | Train Loss: 0.0789 | Val Loss: 0.0386\n",
            "Epoch 3/100 | Train Loss: 0.0670 | Val Loss: 0.0381\n",
            "Epoch 4/100 | Train Loss: 0.0607 | Val Loss: 0.0338\n",
            "Epoch 5/100 | Train Loss: 0.0592 | Val Loss: 0.0322\n",
            "Epoch 6/100 | Train Loss: 0.0529 | Val Loss: 0.0341\n",
            "Epoch 7/100 | Train Loss: 0.0507 | Val Loss: 0.0295\n",
            "Epoch 8/100 | Train Loss: 0.0480 | Val Loss: 0.0303\n",
            "Epoch 9/100 | Train Loss: 0.0480 | Val Loss: 0.0289\n",
            "Epoch 10/100 | Train Loss: 0.0447 | Val Loss: 0.0294\n",
            "Epoch 11/100 | Train Loss: 0.0436 | Val Loss: 0.0273\n",
            "Epoch 12/100 | Train Loss: 0.0420 | Val Loss: 0.0269\n",
            "Epoch 13/100 | Train Loss: 0.0406 | Val Loss: 0.0262\n",
            "Epoch 14/100 | Train Loss: 0.0405 | Val Loss: 0.0260\n",
            "Epoch 15/100 | Train Loss: 0.0394 | Val Loss: 0.0257\n",
            "Epoch 16/100 | Train Loss: 0.0389 | Val Loss: 0.0258\n",
            "Epoch 17/100 | Train Loss: 0.0391 | Val Loss: 0.0269\n",
            "Epoch 18/100 | Train Loss: 0.0384 | Val Loss: 0.0259\n",
            "Epoch 19/100 | Train Loss: 0.0382 | Val Loss: 0.0251\n",
            "Epoch 20/100 | Train Loss: 0.0378 | Val Loss: 0.0253\n",
            "Epoch 21/100 | Train Loss: 0.0376 | Val Loss: 0.0246\n",
            "Epoch 22/100 | Train Loss: 0.0371 | Val Loss: 0.0244\n",
            "Epoch 23/100 | Train Loss: 0.0369 | Val Loss: 0.0250\n",
            "Epoch 24/100 | Train Loss: 0.0366 | Val Loss: 0.0246\n",
            "Epoch 25/100 | Train Loss: 0.0371 | Val Loss: 0.0252\n",
            "Epoch 26/100 | Train Loss: 0.0362 | Val Loss: 0.0245\n",
            "Epoch 27/100 | Train Loss: 0.0362 | Val Loss: 0.0246\n",
            "Epoch 28/100 | Train Loss: 0.0361 | Val Loss: 0.0233\n",
            "Epoch 29/100 | Train Loss: 0.0360 | Val Loss: 0.0235\n",
            "Epoch 30/100 | Train Loss: 0.0361 | Val Loss: 0.0236\n",
            "Epoch 31/100 | Train Loss: 0.0354 | Val Loss: 0.0235\n",
            "Epoch 32/100 | Train Loss: 0.0353 | Val Loss: 0.0235\n",
            "Epoch 33/100 | Train Loss: 0.0348 | Val Loss: 0.0243\n",
            "Epoch 34/100 | Train Loss: 0.0350 | Val Loss: 0.0239\n",
            "Epoch 35/100 | Train Loss: 0.0352 | Val Loss: 0.0242\n",
            "Epoch 36/100 | Train Loss: 0.0352 | Val Loss: 0.0236\n",
            "Epoch 37/100 | Train Loss: 0.0345 | Val Loss: 0.0241\n",
            "Epoch 38/100 | Train Loss: 0.0342 | Val Loss: 0.0246\n",
            "Epoch 39/100 | Train Loss: 0.0341 | Val Loss: 0.0242\n",
            "Epoch 40/100 | Train Loss: 0.0339 | Val Loss: 0.0239\n",
            "Epoch 41/100 | Train Loss: 0.0351 | Val Loss: 0.0236\n",
            "Epoch 42/100 | Train Loss: 0.0342 | Val Loss: 0.0237\n",
            "Epoch 43/100 | Train Loss: 0.0338 | Val Loss: 0.0251\n",
            "Epoch 44/100 | Train Loss: 0.0341 | Val Loss: 0.0235\n",
            "Epoch 45/100 | Train Loss: 0.0335 | Val Loss: 0.0250\n",
            "Epoch 46/100 | Train Loss: 0.0335 | Val Loss: 0.0251\n",
            "Epoch 47/100 | Train Loss: 0.0336 | Val Loss: 0.0237\n",
            "Epoch 48/100 | Train Loss: 0.0333 | Val Loss: 0.0242\n",
            "Epoch 49/100 | Train Loss: 0.0333 | Val Loss: 0.0240\n",
            "Epoch 50/100 | Train Loss: 0.0331 | Val Loss: 0.0263\n",
            "Epoch 51/100 | Train Loss: 0.0328 | Val Loss: 0.0238\n",
            "Epoch 52/100 | Train Loss: 0.0329 | Val Loss: 0.0249\n",
            "Epoch 53/100 | Train Loss: 0.0328 | Val Loss: 0.0250\n",
            "Epoch 54/100 | Train Loss: 0.0324 | Val Loss: 0.0262\n",
            "Epoch 55/100 | Train Loss: 0.0327 | Val Loss: 0.0267\n",
            "Epoch 56/100 | Train Loss: 0.0323 | Val Loss: 0.0268\n",
            "Epoch 57/100 | Train Loss: 0.0325 | Val Loss: 0.0274\n",
            "Epoch 58/100 | Train Loss: 0.0324 | Val Loss: 0.0274\n",
            "Epoch 59/100 | Train Loss: 0.0320 | Val Loss: 0.0295\n",
            "Epoch 60/100 | Train Loss: 0.0318 | Val Loss: 0.0270\n",
            "Epoch 61/100 | Train Loss: 0.0311 | Val Loss: 0.0280\n",
            "Epoch 62/100 | Train Loss: 0.0311 | Val Loss: 0.0271\n",
            "Epoch 63/100 | Train Loss: 0.0311 | Val Loss: 0.0270\n",
            "Epoch 64/100 | Train Loss: 0.0312 | Val Loss: 0.0270\n",
            "Epoch 65/100 | Train Loss: 0.0305 | Val Loss: 0.0274\n",
            "Epoch 66/100 | Train Loss: 0.0302 | Val Loss: 0.0280\n",
            "Epoch 67/100 | Train Loss: 0.0311 | Val Loss: 0.0268\n",
            "Epoch 68/100 | Train Loss: 0.0305 | Val Loss: 0.0268\n",
            "Epoch 69/100 | Train Loss: 0.0306 | Val Loss: 0.0270\n",
            "Epoch 70/100 | Train Loss: 0.0303 | Val Loss: 0.0283\n",
            "Epoch 71/100 | Train Loss: 0.0294 | Val Loss: 0.0273\n",
            "Epoch 72/100 | Train Loss: 0.0296 | Val Loss: 0.0284\n",
            "Epoch 73/100 | Train Loss: 0.0289 | Val Loss: 0.0284\n",
            "Epoch 74/100 | Train Loss: 0.0285 | Val Loss: 0.0290\n",
            "Epoch 75/100 | Train Loss: 0.0281 | Val Loss: 0.0293\n",
            "Epoch 76/100 | Train Loss: 0.0278 | Val Loss: 0.0291\n",
            "Epoch 77/100 | Train Loss: 0.0278 | Val Loss: 0.0282\n",
            "Epoch 78/100 | Train Loss: 0.0278 | Val Loss: 0.0279\n",
            "Epoch 79/100 | Train Loss: 0.0282 | Val Loss: 0.0297\n",
            "Epoch 80/100 | Train Loss: 0.0279 | Val Loss: 0.0272\n",
            "Epoch 81/100 | Train Loss: 0.0275 | Val Loss: 0.0273\n",
            "Epoch 82/100 | Train Loss: 0.0263 | Val Loss: 0.0282\n",
            "Epoch 83/100 | Train Loss: 0.0264 | Val Loss: 0.0276\n",
            "Epoch 84/100 | Train Loss: 0.0269 | Val Loss: 0.0286\n",
            "Epoch 85/100 | Train Loss: 0.0257 | Val Loss: 0.0286\n",
            "Epoch 86/100 | Train Loss: 0.0264 | Val Loss: 0.0287\n",
            "Epoch 87/100 | Train Loss: 0.0254 | Val Loss: 0.0292\n",
            "Epoch 88/100 | Train Loss: 0.0252 | Val Loss: 0.0290\n",
            "Epoch 89/100 | Train Loss: 0.0249 | Val Loss: 0.0290\n",
            "Epoch 90/100 | Train Loss: 0.0250 | Val Loss: 0.0294\n",
            "Epoch 91/100 | Train Loss: 0.0248 | Val Loss: 0.0303\n",
            "Epoch 92/100 | Train Loss: 0.0243 | Val Loss: 0.0306\n",
            "Epoch 93/100 | Train Loss: 0.0249 | Val Loss: 0.0293\n",
            "Epoch 94/100 | Train Loss: 0.0237 | Val Loss: 0.0310\n",
            "Epoch 95/100 | Train Loss: 0.0236 | Val Loss: 0.0305\n",
            "Epoch 96/100 | Train Loss: 0.0232 | Val Loss: 0.0296\n",
            "Epoch 97/100 | Train Loss: 0.0230 | Val Loss: 0.0303\n",
            "Epoch 98/100 | Train Loss: 0.0223 | Val Loss: 0.0303\n",
            "Epoch 99/100 | Train Loss: 0.0223 | Val Loss: 0.0320\n",
            "Epoch 100/100 | Train Loss: 0.0235 | Val Loss: 0.0324\n",
            "Saved best model as best_lstm_rtm_d_lstm3_weights.pt\n"
          ]
        }
      ],
      "source": [
        "rtm_d_model_3 = LSTMResidualModel(num_layers=3, dropout=0.3, target_len = 96)\n",
        "rtm_d_lstm3_weights = train_model(rtm_d_model_3, rtm_d_train_loader, rtm_d_val_loader, device = device, model_instance_name = \"rtm_d_lstm3_weights\", target_len = 96)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQ0HBD8GNy9b"
      },
      "source": [
        "Trained rtm_d_model1_1 (dropout = 0.1, num_layer = 1) has the least val loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDYjsaQjOjLU"
      },
      "source": [
        "## Store Parameters of Model and Scaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CdNVALQ1USlO",
        "outputId": "6c4b3004-5088-41a9-ed24-41f690674247"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['rtm_d_scaler.save']"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import joblib\n",
        "joblib.dump(dam_scaler, 'dam_scaler.save')\n",
        "\n",
        "joblib.dump(rtm_scaler, 'rtm_scaler.save')\n",
        "\n",
        "joblib.dump(rtm_d_scaler, 'rtm_d_scaler.save')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "r2gJTP3xHt44",
        "outputId": "8c75a4d7-d5aa-4985-8cd9-9bcd474df50a"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"dam1_test\",\n  \"rows\": 7575,\n  \"fields\": [\n    {\n      \"column\": \"datetime\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"2024-11-21 02:15:00\",\n        \"max\": \"2025-02-07 23:45:00\",\n        \"num_unique_values\": 7575,\n        \"samples\": [\n          \"2025-02-02 06:30:00\",\n          \"2024-12-29 20:30:00\",\n          \"2025-01-02 02:45:00\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"MCP (Rs/MWh)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2427.779502378352,\n        \"min\": 1610.59,\n        \"max\": 10000.0,\n        \"num_unique_values\": 5973,\n        \"samples\": [\n          7999.81,\n          3003.13,\n          3169.2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "dam1_test"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-9e3577df-a86e-4a82-bd32-7423d323a2b6\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>datetime</th>\n",
              "      <th>MCP (Rs/MWh)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>42921</th>\n",
              "      <td>2024-11-21 02:15:00</td>\n",
              "      <td>2400.21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42922</th>\n",
              "      <td>2024-11-21 02:30:00</td>\n",
              "      <td>2393.83</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42923</th>\n",
              "      <td>2024-11-21 02:45:00</td>\n",
              "      <td>2400.39</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42924</th>\n",
              "      <td>2024-11-21 03:00:00</td>\n",
              "      <td>2393.61</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42925</th>\n",
              "      <td>2024-11-21 03:15:00</td>\n",
              "      <td>2393.35</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9e3577df-a86e-4a82-bd32-7423d323a2b6')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-9e3577df-a86e-4a82-bd32-7423d323a2b6 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-9e3577df-a86e-4a82-bd32-7423d323a2b6');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-f4a090a5-c40e-4431-be9a-b6b6750b2153\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f4a090a5-c40e-4431-be9a-b6b6750b2153')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-f4a090a5-c40e-4431-be9a-b6b6750b2153 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                 datetime  MCP (Rs/MWh)\n",
              "42921 2024-11-21 02:15:00       2400.21\n",
              "42922 2024-11-21 02:30:00       2393.83\n",
              "42923 2024-11-21 02:45:00       2400.39\n",
              "42924 2024-11-21 03:00:00       2393.61\n",
              "42925 2024-11-21 03:15:00       2393.35"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dam1_test.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZE5WmpS3Fmjc",
        "outputId": "b896a596-2955-4361-882d-d19c0dc4c7c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-6MWxobUg_3",
        "outputId": "6acc406e-656a-48cc-e671-83f24b32ccdf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import joblib\n",
        "from model_definition import LSTMResidualModel\n",
        "\n",
        "\"\"\"\n",
        "Change the num_layers and target_len as per the .pt file\n",
        "best_lstm_dam_lstm1_weights.pt: num_layers = 1, target_len = 96\n",
        "best_lstm_dam_lstm1_200_weights.pt: num_layers = 1, target_len = 96. 200 epochs\n",
        "best_lstm_dam_lstm2_weights.pt: num_layers = 2, target_len = 96\n",
        "best_lstm_dam_lstm2_150_weights.pt: num_layers = 2, target_len = 96. 150 epochs\n",
        "best_lstm_dam_lstm3_weights.pt: num_layers = 3, target_len = 96\n",
        "best_lstm_rtm_lstm2_weights.pt: num_layers = 2, target_len = 1\n",
        "best_lstm_rtm_lstm3_weights.pt: num_layers = 3, target_len = 1\n",
        "best_lstm_rtm_d_lstm1_1_weights.pt: num_layers = 1, target_len = 96, droput = 0.1\n",
        "best_lstm_rtm_d_lstm2_weights.pt: num_layers = 2, target_len = 96\n",
        "best_lstm_rtm_d_lstm3_weights.pt: num_layers = 3, target_len = 96\n",
        "\"\"\"\n",
        "model = LSTMResidualModel(num_layers=1, dropout=0.1, target_len = 96)\n",
        "model.load_state_dict(torch.load('/content/best_lstm_dam_lstm1_weights.pt', map_location=device))\n",
        "\n",
        "model.to(device)\n",
        "\"\"\"\n",
        "Load the scaler used while training the model\n",
        "best_lstm_dam_...: 'dam_scaler.save'\n",
        "best_lstm_rtm_...: 'rtm_scaler.save'\n",
        "best_lstm_rtm_d_...: 'rtm_d_scaler.save'\n",
        "\"\"\"\n",
        "dam_scaler = joblib.load('/content/dam_scaler.save')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3ydxxKoRZrv"
      },
      "source": [
        "## How to use model parameters and scaler.save\n",
        "\n",
        "### Yesterday's MCP to get Tommorow's MCP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "JmccGVbwcGGD"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\"\"\"\n",
        "Predict next day's 96 prices using the previous day's prices\n",
        "Using df containing datetime and MCP columns, trained model and their\n",
        "respective scalers\n",
        "\"\"\"\n",
        "\n",
        "def rolling_day_predictions(df, model, scaler, device):\n",
        "\n",
        "    df = df.copy()\n",
        "    df['date'] = df['datetime'].dt.date\n",
        "\n",
        "    result = []\n",
        "\n",
        "    unique_dates = sorted(df['date'].unique())\n",
        "\n",
        "    for i in range(1, len(unique_dates) - 1):\n",
        "        prev_day = unique_dates[i - 1]\n",
        "        next_day = unique_dates[i + 1]\n",
        "\n",
        "        prev_day_data = df[df['date'] == prev_day]['MCP (Rs/MWh)'].values\n",
        "        next_day_times = df[df['date'] == next_day]['datetime'].values\n",
        "        true_next_day_data = df[df['date'] == next_day]['MCP (Rs/MWh)'].values\n",
        "\n",
        "        if len(prev_day_data) != 96 or len(next_day_times) != 96:\n",
        "            continue  # skip incomplete days\n",
        "\n",
        "        # Scale the input\n",
        "        x = scaler.transform(prev_day_data.reshape(-1, 1)).astype(np.float32)  # (96, 1)\n",
        "        x_tensor = torch.tensor(x).unsqueeze(0).to(device)  # (1, 96, 1)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            pred_scaled = model(x_tensor).cpu().numpy().flatten()\n",
        "\n",
        "        # unscale it back\n",
        "        pred = scaler.inverse_transform(pred_scaled.reshape(-1, 1)).flatten()\n",
        "\n",
        "        # Store predictions with corresponding datetime\n",
        "        for dt, p, t in zip(next_day_times, pred, true_next_day_data):\n",
        "            result.append({'datetime': dt, 'predicted': p, 'target': t})\n",
        "\n",
        "    return pd.DataFrame(result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IzTm24XnGHwP",
        "outputId": "4a31560c-f26a-4a0e-b418-7830e233af3f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "test_dam = rolling_day_predictions(dam1_test, model, dam_scaler, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "jWQdS9fNHSJ7",
        "outputId": "5562547f-303c-4454-fac4-8a5d6f8f66e6"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"test_dam\",\n  \"rows\": 7296,\n  \"fields\": [\n    {\n      \"column\": \"datetime\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"2024-11-24 00:00:00\",\n        \"max\": \"2025-02-07 23:45:00\",\n        \"num_unique_values\": 7296,\n        \"samples\": [\n          \"2025-01-03 17:15:00\",\n          \"2025-01-25 13:00:00\",\n          \"2024-12-12 10:00:00\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"predicted\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 7295,\n        \"samples\": [\n          4592.7470703125,\n          3599.477783203125,\n          4256.44140625\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"target\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2446.110512246966,\n        \"min\": 1610.59,\n        \"max\": 10000.0,\n        \"num_unique_values\": 5762,\n        \"samples\": [\n          2927.1,\n          2269.54,\n          3698.49\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "test_dam"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-1354231b-f723-4683-bcb9-c76070cbcc2d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>datetime</th>\n",
              "      <th>predicted</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2024-11-24 00:00:00</td>\n",
              "      <td>2571.542236</td>\n",
              "      <td>2730.12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2024-11-24 00:15:00</td>\n",
              "      <td>2550.286133</td>\n",
              "      <td>2649.94</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2024-11-24 00:30:00</td>\n",
              "      <td>2479.662109</td>\n",
              "      <td>2649.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2024-11-24 00:45:00</td>\n",
              "      <td>2483.994141</td>\n",
              "      <td>2649.06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2024-11-24 01:00:00</td>\n",
              "      <td>2461.412109</td>\n",
              "      <td>2590.22</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7291</th>\n",
              "      <td>2025-02-07 22:45:00</td>\n",
              "      <td>2907.379639</td>\n",
              "      <td>3264.65</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7292</th>\n",
              "      <td>2025-02-07 23:00:00</td>\n",
              "      <td>2890.103760</td>\n",
              "      <td>3120.54</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7293</th>\n",
              "      <td>2025-02-07 23:15:00</td>\n",
              "      <td>2933.112549</td>\n",
              "      <td>3119.11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7294</th>\n",
              "      <td>2025-02-07 23:30:00</td>\n",
              "      <td>2976.169922</td>\n",
              "      <td>3119.74</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7295</th>\n",
              "      <td>2025-02-07 23:45:00</td>\n",
              "      <td>2998.153320</td>\n",
              "      <td>3120.16</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>7296 rows Ã— 3 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1354231b-f723-4683-bcb9-c76070cbcc2d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-1354231b-f723-4683-bcb9-c76070cbcc2d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-1354231b-f723-4683-bcb9-c76070cbcc2d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-11ecb82a-f317-4d79-bf57-e1f23e249f0d\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-11ecb82a-f317-4d79-bf57-e1f23e249f0d')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-11ecb82a-f317-4d79-bf57-e1f23e249f0d button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_8f4735d6-08cd-4d17-b8ca-6fd72d92ce27\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('test_dam')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_8f4735d6-08cd-4d17-b8ca-6fd72d92ce27 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('test_dam');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                datetime    predicted   target\n",
              "0    2024-11-24 00:00:00  2571.542236  2730.12\n",
              "1    2024-11-24 00:15:00  2550.286133  2649.94\n",
              "2    2024-11-24 00:30:00  2479.662109  2649.25\n",
              "3    2024-11-24 00:45:00  2483.994141  2649.06\n",
              "4    2024-11-24 01:00:00  2461.412109  2590.22\n",
              "...                  ...          ...      ...\n",
              "7291 2025-02-07 22:45:00  2907.379639  3264.65\n",
              "7292 2025-02-07 23:00:00  2890.103760  3120.54\n",
              "7293 2025-02-07 23:15:00  2933.112549  3119.11\n",
              "7294 2025-02-07 23:30:00  2976.169922  3119.74\n",
              "7295 2025-02-07 23:45:00  2998.153320  3120.16\n",
              "\n",
              "[7296 rows x 3 columns]"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_dam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r7JDFZQDP9uk",
        "outputId": "a49aeca8-5cf6-4b46-fe27-e66ea7db4594"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "rtm_d_model  = LSTMResidualModel(num_layers=1, dropout=0.1, target_len = 96)\n",
        "rtm_d_model.load_state_dict(torch.load('/content/best_lstm_rtm_d_lstm1_1_weights.pt', map_location=device))\n",
        "\n",
        "rtm_d_model.to(device)\n",
        "rtm_d_scaler = joblib.load('/content/rtm_d_scaler.save')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "umqT9rflPypY",
        "outputId": "2c240a6e-6a24-4743-dc58-49259f548f10"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "test_rtm_d = rolling_day_predictions(rtm1_test, rtm_d_model, rtm_d_scaler, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "ceTriVDAQ7__",
        "outputId": "226b3b95-30f7-4a53-ef1d-6c1c04fd8bd5"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"test_rtm_d\",\n  \"rows\": 7200,\n  \"fields\": [\n    {\n      \"column\": \"datetime\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"2024-11-24 00:00:00\",\n        \"max\": \"2025-02-06 23:45:00\",\n        \"num_unique_values\": 7200,\n        \"samples\": [\n          \"2024-12-26 06:30:00\",\n          \"2024-12-20 08:45:00\",\n          \"2025-01-05 09:45:00\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"predicted\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 7194,\n        \"samples\": [\n          3586.411865234375,\n          5604.35302734375,\n          4978.0830078125\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"target\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2373.7484265943435,\n        \"min\": 0.0,\n        \"max\": 10000.0,\n        \"num_unique_values\": 5838,\n        \"samples\": [\n          2118.05,\n          3176.34,\n          3750.7\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "test_rtm_d"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-f1352db6-852e-49c6-a60d-0150d47e8d95\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>datetime</th>\n",
              "      <th>predicted</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2024-11-24 00:00:00</td>\n",
              "      <td>1824.916626</td>\n",
              "      <td>2778.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2024-11-24 00:15:00</td>\n",
              "      <td>2000.451050</td>\n",
              "      <td>2900.14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2024-11-24 00:30:00</td>\n",
              "      <td>1926.638062</td>\n",
              "      <td>2900.41</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2024-11-24 00:45:00</td>\n",
              "      <td>1977.558716</td>\n",
              "      <td>2843.49</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2024-11-24 01:00:00</td>\n",
              "      <td>1988.781128</td>\n",
              "      <td>2765.59</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7195</th>\n",
              "      <td>2025-02-06 22:45:00</td>\n",
              "      <td>2772.193848</td>\n",
              "      <td>3067.61</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7196</th>\n",
              "      <td>2025-02-06 23:00:00</td>\n",
              "      <td>2660.985840</td>\n",
              "      <td>3067.47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7197</th>\n",
              "      <td>2025-02-06 23:15:00</td>\n",
              "      <td>2605.918701</td>\n",
              "      <td>3067.31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7198</th>\n",
              "      <td>2025-02-06 23:30:00</td>\n",
              "      <td>2620.921143</td>\n",
              "      <td>3067.49</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7199</th>\n",
              "      <td>2025-02-06 23:45:00</td>\n",
              "      <td>2738.758301</td>\n",
              "      <td>3040.70</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>7200 rows Ã— 3 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f1352db6-852e-49c6-a60d-0150d47e8d95')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f1352db6-852e-49c6-a60d-0150d47e8d95 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f1352db6-852e-49c6-a60d-0150d47e8d95');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-e0264953-5f2a-4d91-89e0-eed9f6938683\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e0264953-5f2a-4d91-89e0-eed9f6938683')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-e0264953-5f2a-4d91-89e0-eed9f6938683 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_d7360f14-b957-4587-82ea-e862a7b58628\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('test_rtm_d')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_d7360f14-b957-4587-82ea-e862a7b58628 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('test_rtm_d');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                datetime    predicted   target\n",
              "0    2024-11-24 00:00:00  1824.916626  2778.00\n",
              "1    2024-11-24 00:15:00  2000.451050  2900.14\n",
              "2    2024-11-24 00:30:00  1926.638062  2900.41\n",
              "3    2024-11-24 00:45:00  1977.558716  2843.49\n",
              "4    2024-11-24 01:00:00  1988.781128  2765.59\n",
              "...                  ...          ...      ...\n",
              "7195 2025-02-06 22:45:00  2772.193848  3067.61\n",
              "7196 2025-02-06 23:00:00  2660.985840  3067.47\n",
              "7197 2025-02-06 23:15:00  2605.918701  3067.31\n",
              "7198 2025-02-06 23:30:00  2620.921143  3067.49\n",
              "7199 2025-02-06 23:45:00  2738.758301  3040.70\n",
              "\n",
              "[7200 rows x 3 columns]"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_rtm_d"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### RTM\n",
        "Get RTM MCP using 24 hours MCP data before RTM auction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "-O9OQkmaRIbt"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\"\"\"\n",
        "Predict rtm after 7 prices using the last 96 prices\n",
        "Using df containing datetime and MCP columns, trained model and their\n",
        "respective scalers\n",
        "\"\"\"\n",
        "\n",
        "def rtm_rolling_day_predictions(df, model, scaler, device):\n",
        "  ndf = df.copy()\n",
        "  result = []\n",
        "  for i in range(96, len(df) - 7):\n",
        "    last_96_data = ndf[['MCP (Rs/MWh)']].iloc[i - 96:i]\n",
        "    target = df['MCP (Rs/MWh)'].iloc[i + 7]\n",
        "    datetime = df['datetime'].iloc[i + 7]\n",
        "\n",
        "    # Scale the input\n",
        "    x = scaler.transform(last_96_data).astype(np.float32)  # (96, 1)\n",
        "    x_tensor = torch.tensor(x).unsqueeze(0).to(device)  # (1, 96, 1)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      pred_scaled = model(x_tensor).cpu().numpy().flatten()\n",
        "\n",
        "      # unscale it back\n",
        "      pred = scaler.inverse_transform(pred_scaled.reshape(-1, 1)).flatten()\n",
        "      pred = pred.squeeze()\n",
        "\n",
        "      # Store predictions with corresponding datetime\n",
        "      result.append({\n",
        "            'datetime': datetime,\n",
        "            'predicted': pred,\n",
        "            'target': target\n",
        "        })\n",
        "  return pd.DataFrame(result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "jcgz6AvkVMoh"
      },
      "outputs": [],
      "source": [
        "rtm_model  = LSTMResidualModel(num_layers=2, dropout=0.3, target_len = 1)\n",
        "rtm_model.load_state_dict(torch.load('/content/best_lstm_rtm_lstm2_weights.pt', map_location=device))\n",
        "\n",
        "rtm_model.to(device)\n",
        "rtm_scaler = joblib.load('/content/rtm_scaler.save')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "BIGn6qbmUrPF"
      },
      "outputs": [],
      "source": [
        "test_rtm = rtm_rolling_day_predictions(rtm1_test, rtm_model, rtm_scaler, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ynMLd5i5bQzh",
        "outputId": "1086b25c-8dae-4794-cbae-789eabb8b4bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MSE: 3053535.4118\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "mse = mean_squared_error(test_rtm['target'], test_rtm['predicted'])\n",
        "print(f\"MSE: {mse:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "YPiUh5noYWbo",
        "outputId": "ea8652e6-409a-4275-c3fd-754887982659"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"test_rtm\",\n  \"rows\": 7376,\n  \"fields\": [\n    {\n      \"column\": \"datetime\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"2024-11-22 04:00:00\",\n        \"max\": \"2025-02-06 23:45:00\",\n        \"num_unique_values\": 7376,\n        \"samples\": [\n          \"2024-12-03 15:45:00\",\n          \"2024-12-16 12:30:00\",\n          \"2025-02-05 03:00:00\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"predicted\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"target\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2368.248487437474,\n        \"min\": 0.0,\n        \"max\": 10000.0,\n        \"num_unique_values\": 5972,\n        \"samples\": [\n          3573.25,\n          3699.72,\n          5400.76\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "test_rtm"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-6e1c57f0-2c8a-4ba5-9682-8e031fb1a061\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>datetime</th>\n",
              "      <th>predicted</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2024-11-22 04:00:00</td>\n",
              "      <td>2575.7446</td>\n",
              "      <td>2751.99</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2024-11-22 04:15:00</td>\n",
              "      <td>2417.4275</td>\n",
              "      <td>2954.06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2024-11-22 04:30:00</td>\n",
              "      <td>2602.4854</td>\n",
              "      <td>3082.16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2024-11-22 04:45:00</td>\n",
              "      <td>2594.5679</td>\n",
              "      <td>3097.28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2024-11-22 05:00:00</td>\n",
              "      <td>2510.7915</td>\n",
              "      <td>3176.62</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7371</th>\n",
              "      <td>2025-02-06 22:45:00</td>\n",
              "      <td>3303.0327</td>\n",
              "      <td>3067.61</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7372</th>\n",
              "      <td>2025-02-06 23:00:00</td>\n",
              "      <td>3208.7878</td>\n",
              "      <td>3067.47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7373</th>\n",
              "      <td>2025-02-06 23:15:00</td>\n",
              "      <td>2935.1433</td>\n",
              "      <td>3067.31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7374</th>\n",
              "      <td>2025-02-06 23:30:00</td>\n",
              "      <td>2991.9402</td>\n",
              "      <td>3067.49</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7375</th>\n",
              "      <td>2025-02-06 23:45:00</td>\n",
              "      <td>2896.4473</td>\n",
              "      <td>3040.70</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>7376 rows Ã— 3 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6e1c57f0-2c8a-4ba5-9682-8e031fb1a061')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-6e1c57f0-2c8a-4ba5-9682-8e031fb1a061 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-6e1c57f0-2c8a-4ba5-9682-8e031fb1a061');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-cc21e4fb-06c6-4992-9b4e-79e5451819b2\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-cc21e4fb-06c6-4992-9b4e-79e5451819b2')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-cc21e4fb-06c6-4992-9b4e-79e5451819b2 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_23f7c4f6-b43e-43fc-97fd-4d6aa6c93e9d\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('test_rtm')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_23f7c4f6-b43e-43fc-97fd-4d6aa6c93e9d button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('test_rtm');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                datetime  predicted   target\n",
              "0    2024-11-22 04:00:00  2575.7446  2751.99\n",
              "1    2024-11-22 04:15:00  2417.4275  2954.06\n",
              "2    2024-11-22 04:30:00  2602.4854  3082.16\n",
              "3    2024-11-22 04:45:00  2594.5679  3097.28\n",
              "4    2024-11-22 05:00:00  2510.7915  3176.62\n",
              "...                  ...        ...      ...\n",
              "7371 2025-02-06 22:45:00  3303.0327  3067.61\n",
              "7372 2025-02-06 23:00:00  3208.7878  3067.47\n",
              "7373 2025-02-06 23:15:00  2935.1433  3067.31\n",
              "7374 2025-02-06 23:30:00  2991.9402  3067.49\n",
              "7375 2025-02-06 23:45:00  2896.4473  3040.70\n",
              "\n",
              "[7376 rows x 3 columns]"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_rtm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7z4A8BWZDD5"
      },
      "source": [
        "# More Possible Works\n",
        "Temporal Fusion Transformer supported by pytorch_forecasting. Tutorial: [forecasting with the Temporal Fusion Transformer](https://pytorch-forecasting.readthedocs.io/en/stable/tutorials/stallion.html)\n",
        "\n",
        "*   They are more advanced, can notice both short trends and long trends\n",
        "*   Uses attension to get even the small trends.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "datasetId": 7318942,
          "sourceId": 11662284,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30918,
      "isGpuEnabled": false,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
